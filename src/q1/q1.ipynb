{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Top 10 Fechas con M√°s Tweets\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Encontrar las **top 10 fechas** (por conteo de tweets) y para cada una, el **usuario con m√°s tweets ese d√≠a**.\n",
    "\n",
    "**Output esperado:** `List[Tuple[datetime.date, str]]`\n",
    "\n",
    "## Enfoque Experimental: Comparaci√≥n TIME vs MEMORY\n",
    "\n",
    "Este notebook eval√∫a **cuatro enfoques diferentes** para resolver Q1, divididos en dos categor√≠as:\n",
    "\n",
    "### üöÄ TIME-OPTIMIZED (In-Memory)\n",
    "Prioridad: **m√°xima velocidad de ejecuci√≥n**\n",
    "\n",
    "#### üîµ Approach 1: Polars In-Memory\n",
    "- Biblioteca moderna escrita en Rust\n",
    "- Columnar storage (Apache Arrow)\n",
    "- **Carga completa en memoria con `scan_ndjson().collect()`**\n",
    "- Lazy evaluation + eager collection\n",
    "- Operaciones vectorizadas y paralelizadas\n",
    "\n",
    "#### üü† Approach 2: Pandas In-Memory  \n",
    "- Biblioteca tradicional de Python\n",
    "- Basada en NumPy\n",
    "- **Carga completa en memoria con `read_json(lines=True)`**\n",
    "- Eager evaluation\n",
    "- Ampliamente usada en la industria\n",
    "\n",
    "### üíæ MEMORY-OPTIMIZED (Streaming)\n",
    "Prioridad: **m√≠nimo consumo de memoria**\n",
    "\n",
    "#### üîµ Approach 3: Polars Streaming\n",
    "- Lazy evaluation sin materializaci√≥n temprana\n",
    "- Streaming aggregations\n",
    "- Solo materializa resultados finales\n",
    "- Procesa datos sin cargar todo en RAM\n",
    "\n",
    "#### üü† Approach 4: Pandas Chunked Processing\n",
    "- Procesamiento por chunks con `chunksize`\n",
    "- Contadores incrementales\n",
    "- Evita DataFrames intermedios grandes\n",
    "- Trade-off memoria por tiempo\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de la Comparaci√≥n\n",
    "\n",
    "1. **Performance**: Medir tiempo de ejecuci√≥n de cada enfoque\n",
    "2. **Memory**: Medir consumo de memoria (RSS delta)\n",
    "3. **Profiling**: Identificar bottlenecks con cProfile\n",
    "4. **Trade-offs**: Evaluar cu√°ndo usar cada estrategia\n",
    "5. **Correctitud**: Verificar que todos producen resultados id√©nticos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Imports y configuraci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found: 388.83 MB\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"../../data/raw/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Run: python src/dataset/download_dataset.py\")\n",
    "else:\n",
    "    file_size_mb = dataset_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Dataset found: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n 1: Polars (TIME-optimized, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_polars(file_path: str) -> List[Tuple[date, str]]:\n",
    "    # Leer el archivo JSON en modo lazy (no se carga en memoria a√∫n).\n",
    "    # Solo se seleccionan las columnas necesarias para Q1:\n",
    "    # - date_only: fecha truncada a nivel d√≠a (YYYY-MM-DD)\n",
    "    # - username: nombre de usuario del autor del tweet\n",
    "    df = (\n",
    "        pl.scan_ndjson(file_path)\n",
    "        .select([\n",
    "            pl.col(\"date\").str.slice(0, 10).alias(\"date_only\"),\n",
    "            pl.col(\"user\").struct.field(\"username\").alias(\"username\")\n",
    "        ])\n",
    "        # Filtrar registros inv√°lidos de forma expl√≠cita\n",
    "        .filter(\n",
    "            pl.col(\"username\").is_not_null() &\n",
    "            pl.col(\"date_only\").is_not_null()\n",
    "        )\n",
    "        # Materializar el DataFrame completo en memoria\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Calcular el top 10 de fechas con mayor n√∫mero de tweets\n",
    "    # Se agrupa por fecha, se cuentan tweets y se ordena:\n",
    "    # - Primero por n√∫mero de tweets (descendente)\n",
    "    # - Luego por fecha (ascendente) para desempates determin√≠sticos\n",
    "    top_dates = (\n",
    "        df\n",
    "        .group_by(\"date_only\")\n",
    "        .agg(pl.len().alias(\"tweet_count\"))\n",
    "        .sort([\"tweet_count\", \"date_only\"], descending=[True, False])\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Para cada una de las fechas top, se identifica el usuario m√°s activo\n",
    "    for row in top_dates.iter_rows(named=True):\n",
    "        date_str = row[\"date_only\"]\n",
    "\n",
    "        # Filtrar los tweets correspondientes a la fecha actual\n",
    "        date_df = df.filter(pl.col(\"date_only\") == date_str)\n",
    "\n",
    "        # Agrupar por usuario y contar tweets por usuario en ese d√≠a\n",
    "        # Se ordena por:\n",
    "        # - n√∫mero de tweets del usuario (descendente)\n",
    "        # - username (ascendente) para desempates determin√≠sticos\n",
    "        top_user = (\n",
    "            date_df\n",
    "            .group_by(\"username\")\n",
    "            .agg(pl.len().alias(\"user_tweet_count\"))\n",
    "            .sort([\"user_tweet_count\", \"username\"], descending=[True, False])\n",
    "            .head(1)\n",
    "        )\n",
    "\n",
    "        # Extraer el username ganador\n",
    "        username = top_user[\"username\"][0]\n",
    "\n",
    "        # Convertir la fecha de string a datetime.date\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "        # Agregar la tupla (fecha, username) al resultado final\n",
    "        results.append((date_obj, username))\n",
    "\n",
    "    # Retornar la lista de resultados en el formato solicitado\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars - Top 10 Dates:\n",
      "============================================================\n",
      " 1. 2021-02-12 -> @RanbirS00614606\n",
      " 2. 2021-02-13 -> @MaanDee08215437\n",
      " 3. 2021-02-17 -> @RaaJVinderkaur\n",
      " 4. 2021-02-16 -> @jot__b\n",
      " 5. 2021-02-14 -> @rebelpacifist\n",
      " 6. 2021-02-18 -> @neetuanjle_nitu\n",
      " 7. 2021-02-15 -> @jot__b\n",
      " 8. 2021-02-20 -> @MangalJ23056160\n",
      " 9. 2021-02-23 -> @Surrypuria\n",
      "10. 2021-02-19 -> @Preetm91\n"
     ]
    }
   ],
   "source": [
    "result_polars = q1_time_polars(str(dataset_path))\n",
    "\n",
    "print(\"Polars - Top 10 Dates:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (date_obj, username) in enumerate(result_polars, 1):\n",
    "    print(f\"{i:2d}. {date_obj} -> @{username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados muestran las top 10 fechas ordenadas por n√∫mero de tweets. El 2021-02-12 tiene el mayor volumen con 12,347 tweets, y el usuario m√°s activo ese d√≠a fue @RanbirS00614606 con 176 tweets (1.4% del total del d√≠a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polars - Verification (Tweet Counts):\n",
      "================================================================================\n",
      "#   Date         Top User                Total Tweets     User Tweets\n",
      "--------------------------------------------------------------------------------\n",
      "1   2021-02-12   @RanbirS00614606              12,347             176\n",
      "2   2021-02-13   @MaanDee08215437              11,296             178\n",
      "3   2021-02-17   @RaaJVinderkaur               11,087             185\n",
      "4   2021-02-16   @jot__b                       10,443             133\n",
      "5   2021-02-14   @rebelpacifist                10,249             119\n",
      "6   2021-02-18   @neetuanjle_nitu               9,625             195\n",
      "7   2021-02-15   @jot__b                        9,197             134\n",
      "8   2021-02-20   @MangalJ23056160               8,502             108\n",
      "9   2021-02-23   @Surrypuria                    8,417             135\n",
      "10  2021-02-19   @Preetm91                      8,204             267\n"
     ]
    }
   ],
   "source": [
    "df_verify_polars = pl.scan_ndjson(str(dataset_path)).select([\n",
    "    pl.col(\"date\").str.slice(0, 10).alias(\"date_only\"),\n",
    "    pl.col(\"user\").struct.field(\"username\").alias(\"username\")\n",
    "]).filter(\n",
    "    pl.col(\"username\").is_not_null() & \n",
    "    pl.col(\"date_only\").is_not_null()\n",
    ").collect()\n",
    "\n",
    "print(\"\\nPolars - Verification (Tweet Counts):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'#':<3} {'Date':<12} {'Top User':<20} {'Total Tweets':>15} {'User Tweets':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (date_obj, username) in enumerate(result_polars, 1):\n",
    "    date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    total_tweets = df_verify_polars.filter(pl.col(\"date_only\") == date_str).height\n",
    "    user_tweets = df_verify_polars.filter(\n",
    "        (pl.col(\"date_only\") == date_str) & (pl.col(\"username\") == username)\n",
    "    ).height\n",
    "    print(f\"{i:<3} {date_str:<12} @{username:<19} {total_tweets:>15,} {user_tweets:>15,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n 2: Pandas (TIME-optimized, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time_pandas(file_path: str) -> List[Tuple[date, str]]:\n",
    "    # Leer el archivo JSON Lines completo en memoria usando Pandas\n",
    "    # Cada l√≠nea se convierte en una fila del DataFrame\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Extraer la fecha a nivel d√≠a (YYYY-MM-DD) a partir del campo 'date'\n",
    "    # Se fuerza a string para asegurar slicing consistente\n",
    "    df['date_only'] = df['date'].astype(str).str[:10]\n",
    "\n",
    "    # Extraer el username desde el campo anidado 'user'\n",
    "    # Se usa apply con lambda porque 'user' es un diccionario por fila\n",
    "    df['username'] = df['user'].apply(\n",
    "        lambda x: x.get('username') if isinstance(x, dict) else None\n",
    "    )\n",
    "\n",
    "    # Conservar solo las columnas necesarias y eliminar registros inv√°lidos\n",
    "    df = df[['date_only', 'username']].dropna()\n",
    "\n",
    "    # Calcular el top 10 de fechas con m√°s tweets\n",
    "    # Agrupar por fecha, contar filas y ordenar:\n",
    "    # - tweet_count descendente\n",
    "    # - date_only ascendente para desempates determin√≠sticos\n",
    "    top_dates = (\n",
    "        df.groupby('date_only')\n",
    "        .size()\n",
    "        .reset_index(name='tweet_count')\n",
    "        .sort_values(['tweet_count', 'date_only'], ascending=[False, True])\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Para cada fecha top, encontrar el usuario con m√°s tweets ese d√≠a\n",
    "    for _, row in top_dates.iterrows():\n",
    "        date_str = row['date_only']\n",
    "\n",
    "        # Filtrar tweets correspondientes a la fecha actual\n",
    "        date_df = df[df['date_only'] == date_str]\n",
    "\n",
    "        # Agrupar por username y contar tweets por usuario en esa fecha\n",
    "        # Ordenar por:\n",
    "        # - n√∫mero de tweets (descendente)\n",
    "        # - username (ascendente) para desempates determin√≠sticos\n",
    "        top_user = (\n",
    "            date_df.groupby('username')\n",
    "            .size()\n",
    "            .reset_index(name='user_tweet_count')\n",
    "            .sort_values(['user_tweet_count', 'username'], ascending=[False, True])\n",
    "            .head(1)\n",
    "        )\n",
    "\n",
    "        # Extraer el username ganador\n",
    "        username = top_user['username'].iloc[0]\n",
    "\n",
    "        # Convertir la fecha a objeto datetime.date\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "        # Agregar la tupla (fecha, username) al resultado final\n",
    "        results.append((date_obj, username))\n",
    "\n",
    "    # Retornar la lista de resultados en el formato solicitado\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Top 10 Dates:\n",
      "============================================================\n",
      " 1. 2021-02-12 -> @RanbirS00614606\n",
      " 2. 2021-02-13 -> @MaanDee08215437\n",
      " 3. 2021-02-17 -> @RaaJVinderkaur\n",
      " 4. 2021-02-16 -> @jot__b\n",
      " 5. 2021-02-14 -> @rebelpacifist\n",
      " 6. 2021-02-18 -> @neetuanjle_nitu\n",
      " 7. 2021-02-15 -> @jot__b\n",
      " 8. 2021-02-20 -> @MangalJ23056160\n",
      " 9. 2021-02-23 -> @Surrypuria\n",
      "10. 2021-02-19 -> @Preetm91\n"
     ]
    }
   ],
   "source": [
    "result_pandas = q1_time_pandas(str(dataset_path))\n",
    "\n",
    "print(\"Pandas - Top 10 Dates:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (date_obj, username) in enumerate(result_pandas, 1):\n",
    "    print(f\"{i:2d}. {date_obj} -> @{username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas produce exactamente los mismos resultados que Polars: mismas 10 fechas, mismos usuarios top, mismo orden. La verificaci√≥n de counts confirma que ambas implementaciones procesan el dataset de forma id√©ntica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verificaci√≥n: Resultados Id√©nticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pandas - Verification (Tweet Counts):\n",
      "================================================================================\n",
      "#   Date         Top User                Total Tweets     User Tweets\n",
      "--------------------------------------------------------------------------------\n",
      "1   2021-02-12   @RanbirS00614606              12,347             176\n",
      "2   2021-02-13   @MaanDee08215437              11,296             178\n",
      "3   2021-02-17   @RaaJVinderkaur               11,087             185\n",
      "4   2021-02-16   @jot__b                       10,443             133\n",
      "5   2021-02-14   @rebelpacifist                10,249             119\n",
      "6   2021-02-18   @neetuanjle_nitu               9,625             195\n",
      "7   2021-02-15   @jot__b                        9,197             134\n",
      "8   2021-02-20   @MangalJ23056160               8,502             108\n",
      "9   2021-02-23   @Surrypuria                    8,417             135\n",
      "10  2021-02-19   @Preetm91                      8,204             267\n"
     ]
    }
   ],
   "source": [
    "df_verify_pandas = pd.read_json(str(dataset_path), lines=True)\n",
    "\n",
    "df_verify_pandas['date_only'] = df_verify_pandas['date'].astype(str).str[:10]\n",
    "df_verify_pandas['username'] = df_verify_pandas['user'].apply(\n",
    "    lambda x: x.get('username') if isinstance(x, dict) else None\n",
    ")\n",
    "\n",
    "df_verify_pandas = df_verify_pandas[['date_only', 'username']].dropna()\n",
    "\n",
    "print(\"\\nPandas - Verification (Tweet Counts):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'#':<3} {'Date':<12} {'Top User':<20} {'Total Tweets':>15} {'User Tweets':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (date_obj, username) in enumerate(result_pandas, 1):\n",
    "    date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    total_tweets = len(df_verify_pandas[df_verify_pandas['date_only'] == date_str])\n",
    "    user_tweets = len(df_verify_pandas[\n",
    "        (df_verify_pandas['date_only'] == date_str) & \n",
    "        (df_verify_pandas['username'] == username)\n",
    "    ])\n",
    "    print(f\"{i:<3} {date_str:<12} @{username:<19} {total_tweets:>15,} {user_tweets:>15,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Polars and Pandas produce IDENTICAL results\n",
      "   10 tuples match perfectly\n"
     ]
    }
   ],
   "source": [
    "if result_polars == result_pandas:\n",
    "    print(\"‚úÖ Polars and Pandas produce IDENTICAL results\")\n",
    "    print(f\"   {len(result_polars)} tuples match perfectly\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: Results differ!\")\n",
    "    for i, (pol, pan) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "        if pol != pan:\n",
    "            print(f\"   Position {i}: Polars={pol}, Pandas={pan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Comparing Results\n",
      "================================================================================\n",
      "‚úÖ Results are IDENTICAL\n",
      "   10 tuples match perfectly\n",
      "\n",
      "‚úÖ Verifying tweet counts match...\n",
      "‚úÖ All tweet counts match between Polars and Pandas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification: Comparing Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if result_polars == result_pandas:\n",
    "    print(\"‚úÖ Results are IDENTICAL\")\n",
    "    print(f\"   {len(result_polars)} tuples match perfectly\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: Results differ!\")\n",
    "    for i, (pol, pan) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "        if pol != pan:\n",
    "            print(f\"   Position {i}: Polars={pol}, Pandas={pan}\")\n",
    "\n",
    "print(\"\\n‚úÖ Verifying tweet counts match...\")\n",
    "counts_match = True\n",
    "\n",
    "for i, (date_obj, username) in enumerate(result_polars, 1):\n",
    "    date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    polars_total = df_verify_polars.filter(pl.col(\"date_only\") == date_str).height\n",
    "    polars_user = df_verify_polars.filter(\n",
    "        (pl.col(\"date_only\") == date_str) & (pl.col(\"username\") == username)\n",
    "    ).height\n",
    "    \n",
    "    pandas_total = len(df_verify_pandas[df_verify_pandas['date_only'] == date_str])\n",
    "    pandas_user = len(df_verify_pandas[\n",
    "        (df_verify_pandas['date_only'] == date_str) & \n",
    "        (df_verify_pandas['username'] == username)\n",
    "    ])\n",
    "    \n",
    "    if polars_total != pandas_total or polars_user != pandas_user:\n",
    "        counts_match = False\n",
    "        print(f\"‚ùå Counts mismatch at position {i}:\")\n",
    "        print(f\"   Polars: total={polars_total}, user={polars_user}\")\n",
    "        print(f\"   Pandas: total={pandas_total}, user={pandas_user}\")\n",
    "\n",
    "if counts_match:\n",
    "    print(\"‚úÖ All tweet counts match between Polars and Pandas\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Tiempo de Ejecuci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecutan 3 runs de cada implementaci√≥n para obtener m√©tricas confiables. Se reportan min, avg y max para capturar variabilidad por estado del sistema (cach√©, GC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Comparison: Polars vs Pandas\n",
      "================================================================================\n",
      "\n",
      "Running Polars implementation 3 times...\n",
      "  Run 1: 0.348s\n",
      "  Run 2: 0.303s\n",
      "  Run 3: 0.325s\n",
      "\n",
      "Running Pandas implementation 3 times...\n",
      "  Run 1: 2.796s\n",
      "  Run 2: 2.765s\n",
      "  Run 3: 2.749s\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "\n",
      "Library                Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Polars              0.303s     0.325s     0.348s\n",
      "Pandas              2.749s     2.770s     2.796s\n",
      "\n",
      "Speedup:        8.51x (Polars is 8.51x faster)\n",
      "Difference:     2.445s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "print(\"Time Comparison: Polars vs Pandas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nRunning Polars implementation {n_runs} times...\")\n",
    "polars_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q1_time_polars(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    polars_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "polars_avg = sum(polars_times) / len(polars_times)\n",
    "polars_min = min(polars_times)\n",
    "polars_max = max(polars_times)\n",
    "\n",
    "print(f\"\\nRunning Pandas implementation {n_runs} times...\")\n",
    "pandas_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q1_time_pandas(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    pandas_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "pandas_avg = sum(pandas_times) / len(pandas_times)\n",
    "pandas_min = min(pandas_times)\n",
    "pandas_max = max(pandas_times)\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Library':<15} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Polars':<15} {polars_min:>9.3f}s {polars_avg:>9.3f}s {polars_max:>9.3f}s\")\n",
    "print(f\"{'Pandas':<15} {pandas_min:>9.3f}s {pandas_avg:>9.3f}s {pandas_max:>9.3f}s\")\n",
    "\n",
    "speedup = pandas_avg / polars_avg if polars_avg > 0 else float('inf')\n",
    "diff = abs(pandas_avg - polars_avg)\n",
    "\n",
    "print(f\"\\n{'Speedup:':<15} {speedup:.2f}x (Polars is {speedup:.2f}x faster)\")\n",
    "print(f\"{'Difference:':<15} {diff:.3f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars es **8.51x m√°s r√°pido** que Pandas (0.325s vs 2.770s en promedio). La diferencia es significativa: 2.445 segundos absolutos. El primer run de Polars (0.348s) muestra un warm-up m√≠nimo, estabiliz√°ndose r√°pidamente en ~0.32s. Pandas mantiene consistencia entre runs (~2.77s) pero con overhead significativo en parsing JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Profiling Detallado: cProfile\n",
    "\n",
    "An√°lisis de latencia funci√≥n por funci√≥n usando cProfile para identificar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling POLARS implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         5046 function calls (5003 primitive calls) in 0.700 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 329 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       33    0.000    0.000    0.494    0.015 opt_flags.py:312(wrapper)\n",
      "       33    0.000    0.000    0.494    0.015 frame.py:2198(collect)\n",
      "       33    0.494    0.015    0.494    0.015 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        4    0.000    0.000    0.200    0.050 base_events.py:1962(_run_once)\n",
      "        4    0.000    0.000    0.198    0.050 selectors.py:540(select)\n",
      "        4    0.198    0.050    0.198    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "       11    0.000    0.000    0.007    0.001 group_by.py:190(agg)\n",
      "       11    0.000    0.000    0.005    0.000 frame.py:5840(sort)\n",
      "       10    0.000    0.000    0.003    0.000 frame.py:5156(filter)\n",
      "        1    0.000    0.000    0.002    0.002 history.py:1025(writeout_cache)\n",
      "        1    0.000    0.000    0.002    0.002 history.py:1009(_writeout_input_cache)\n",
      "        1    0.001    0.001    0.001    0.001 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        4    0.000    0.000    0.001    0.000 events.py:87(_run)\n",
      "        4    0.000    0.000    0.001    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        3    0.000    0.000    0.001    0.000 ioloop.py:750(_run_callback)\n",
      "        2    0.001    0.000    0.001    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        1    0.000    0.000    0.001    0.001 iostream.py:611(_flush)\n",
      "        3    0.000    0.000    0.001    0.000 zmqstream.py:573(_handle_events)\n",
      "      2/1    0.000    0.000    0.000    0.000 deprecation.py:123(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 ndjson.py:177(scan_ndjson)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         5046 function calls (5003 primitive calls) in 0.700 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 329 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       33    0.494    0.015    0.494    0.015 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        4    0.198    0.050    0.198    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        2    0.001    0.000    0.001    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "       21    0.000    0.000    0.000    0.000 socket.py:623(send)\n",
      "        7    0.000    0.000    0.000    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "  870/862    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method new_from_ndjson}\n",
      "        4    0.000    0.000    0.200    0.050 base_events.py:1962(_run_once)\n",
      "       11    0.000    0.000    0.000    0.000 frame.py:4140(_filter)\n",
      "       10    0.000    0.000    0.000    0.000 _strptime.py:413(_strptime)\n",
      "       70    0.000    0.000    0.000    0.000 expr.py:21(parse_into_expression)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'str_slice' of 'builtins.PyExpr' objects}\n",
      "      265    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x100eefcf0}\n",
      "       12    0.000    0.000    0.000    0.000 lit.py:30(lit)\n",
      "       33    0.000    0.000    0.494    0.015 frame.py:2198(collect)\n",
      "        1    0.000    0.000    0.000    0.000 string.py:2206(slice)\n",
      "       33    0.000    0.000    0.494    0.015 opt_flags.py:312(wrapper)\n",
      "       11    0.000    0.000    0.000    0.000 frame.py:6924(group_by)\n",
      "        7    0.000    0.000    0.000    0.000 attrsettr.py:43(__getattr__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10f4e7490>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "print(\"Profiling POLARS implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q1_time_polars(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El profiling de Polars muestra que el mayor tiempo acumulado est√° en `collect()` que ejecuta toda la query lazy. Las funciones de Rust (via FFI) dominan el total time - `scan_ndjson`, `select`, `filter` son muy r√°pidas. El overhead de Python es m√≠nimo: la mayor√≠a del tiempo est√° en operaciones nativas compiladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling PANDAS implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         991834 function calls (990882 primitive calls) in 3.812 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 905 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      3/2    0.000    0.000    3.811    1.905 interactiveshell.py:3665(run_code)\n",
      "        2    0.000    0.000    3.811    1.905 {built-in method builtins.exec}\n",
      "        1    0.007    0.007    3.811    3.811 1676788238.py:1(<module>)\n",
      "        1    0.288    0.288    3.803    3.803 3429205578.py:1(q1_time_pandas)\n",
      "        1    0.019    0.019    3.062    3.062 _json.py:505(read_json)\n",
      "        1    0.000    0.000    2.545    2.545 _json.py:991(read)\n",
      "        1    0.001    0.001    2.248    2.248 _json.py:1022(_get_object_parser)\n",
      "        1    0.000    0.000    2.248    2.248 _json.py:1174(parse)\n",
      "        1    0.433    0.433    2.044    2.044 _json.py:1386(_parse)\n",
      "        1    1.251    1.251    1.251    1.251 {built-in method pandas._libs.json.ujson_loads}\n",
      "        3    0.000    0.000    0.468    0.156 frame.py:698(__init__)\n",
      "        4    0.000    0.000    0.299    0.075 selectors.py:540(select)\n",
      "        4    0.000    0.000    0.299    0.075 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.229    0.229    0.299    0.299 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.080    0.080    0.297    0.297 _json.py:971(_combine_lines)\n",
      "        1    0.000    0.000    0.289    0.289 construction.py:506(nested_data_to_arrays)\n",
      "        1    0.001    0.001    0.289    0.289 construction.py:793(to_arrays)\n",
      "        1    0.149    0.149    0.207    0.207 construction.py:891(_list_of_dict_to_arrays)\n",
      "        1    0.009    0.009    0.202    0.202 _json.py:1452(_try_convert_types)\n",
      "        4    0.001    0.000    0.197    0.049 base_events.py:1962(_run_once)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         991834 function calls (990882 primitive calls) in 3.812 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 905 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    1.251    1.251    1.251    1.251 {built-in method pandas._libs.json.ujson_loads}\n",
      "        1    0.433    0.433    2.044    2.044 _json.py:1386(_parse)\n",
      "        1    0.288    0.288    3.803    3.803 3429205578.py:1(q1_time_pandas)\n",
      "        1    0.229    0.229    0.299    0.299 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.164    0.164    0.164    0.164 datetimes.py:746(_format_native_types)\n",
      "       52    0.157    0.003    0.157    0.003 {method 'split' of 'str' objects}\n",
      "        1    0.149    0.149    0.207    0.207 construction.py:891(_list_of_dict_to_arrays)\n",
      "   117408    0.127    0.000    0.127    0.000 {method 'strip' of 'str' objects}\n",
      "        5    0.093    0.019    0.093    0.019 {method 'join' of 'str' objects}\n",
      "       10    0.086    0.009    0.125    0.013 managers.py:2295(_merge_blocks)\n",
      "       21    0.080    0.004    0.081    0.004 construction.py:1028(convert)\n",
      "        1    0.080    0.080    0.297    0.297 _json.py:971(_combine_lines)\n",
      "        1    0.070    0.070    0.070    0.070 {built-in method _codecs.utf_8_decode}\n",
      "        8    0.067    0.008    0.067    0.008 datetimes.py:487(_to_datetime_with_unit)\n",
      "       10    0.065    0.006    0.065    0.006 array_ops.py:113(comp_method_OBJECT_ARRAY)\n",
      "   117408    0.049    0.000    0.055    0.000 construction.py:915(<genexpr>)\n",
      "       19    0.044    0.002    0.047    0.002 managers.py:2265(_stack_arrays)\n",
      "        7    0.039    0.006    0.039    0.006 shape_base.py:218(vstack)\n",
      "   117409    0.031    0.000    0.158    0.000 _json.py:976(<genexpr>)\n",
      "        3    0.030    0.010    0.178    0.059 construction.py:96(arrays_to_mgr)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x15b5cbbf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling PANDAS implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q1_time_pandas(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El profiling de Pandas revela que `read_json()` consume ~60-70% del tiempo total - parsing completo del archivo es el bottleneck principal. `.apply()` con lambda para extraer username es costoso (operaci√≥n row-by-row). `.astype(str)` tambi√©n aparece en el top. Los groupby/sort son relativamente eficientes gracias a NumPy, pero el overhead de Python en parsing y transformaciones es evidente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones del Profiling: Polars vs Pandas\n",
    "\n",
    "**Diferencias arquitecturales clave:**\n",
    "\n",
    "1. **Bottleneck principal:**\n",
    "   - **Polars**: Tiempo distribuido eficientemente. `collect()` ejecuta query optimizada (0.494s de 0.700s totales = 70%), operaciones Rust dominan\n",
    "   - **Pandas**: `read_json()` es el cuello de botella (3.062s de 3.812s totales = 80%). Parsing eager sin optimizaci√≥n\n",
    "\n",
    "2. **Overhead de Python:**\n",
    "   - **Polars**: M√≠nimo (5046 llamadas totales). La mayor√≠a del tiempo en c√≥digo nativo (Rust via FFI). Python solo orquesta\n",
    "   - **Pandas**: Significativo (991,834 llamadas). `.apply()` con lambdas es row-by-row en Python puro. `.astype()` requiere conversiones costosas\n",
    "\n",
    "3. **Estrategia de ejecuci√≥n:**\n",
    "   - **Polars**: Lazy evaluation permite optimizar query plan antes de ejecutar. Solo procesa columnas necesarias\n",
    "   - **Pandas**: Eager evaluation. Lee TODO el JSON (ujson_loads: 1.251s), luego transforma. No puede optimizar hasta tener todos los datos\n",
    "\n",
    "4. **Implicaciones para TIME-optimization:**\n",
    "   - La ventaja de **8.51x** de Polars se explica principalmente por:\n",
    "     - Parsing selectivo (solo date, user.username)\n",
    "     - Operaciones vectorizadas en Rust (33 collect() = 0.494s)\n",
    "     - Query optimization autom√°tica\n",
    "   - El tiempo de Pandas est√° dominado por parsing completo (80% del tiempo) + overhead Python en transformaciones\n",
    "\n",
    "**Trade-off identificado:** Polars requiere pensar en lazy queries, pero el beneficio en performance es sustancial para datasets grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Consumo de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mide el RSS (Resident Set Size) antes y despu√©s de cada ejecuci√≥n. El delta indica cu√°nta memoria adicional consume cada implementaci√≥n. Se ejecuta `gc.collect()` entre mediciones para limpiar memoria residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison: Polars vs Pandas\n",
      "================================================================================\n",
      "\n",
      "POLARS:\n",
      "  Memory before:    2366.48 MB\n",
      "  Memory after:     2495.33 MB\n",
      "  Delta:             128.84 MB\n",
      "\n",
      "PANDAS:\n",
      "  Memory before:    2495.33 MB\n",
      "  Memory after:     3607.48 MB\n",
      "  Delta:            1112.16 MB\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "  Polars delta:      128.84 MB\n",
      "  Pandas delta:     1112.16 MB\n",
      "  Difference:        983.31 MB\n",
      "  Winner:        Polars (8.63x more efficient)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "print(\"Memory Comparison: Polars vs Pandas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gc.collect()\n",
    "mem_before_polars = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q1_time_polars(str(dataset_path))\n",
    "mem_after_polars = process.memory_info().rss / (1024 * 1024)\n",
    "delta_polars = mem_after_polars - mem_before_polars\n",
    "\n",
    "print(f\"\\nPOLARS:\")\n",
    "print(f\"  Memory before: {mem_before_polars:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_polars:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_polars:>10.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "mem_before_pandas = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q1_time_pandas(str(dataset_path))\n",
    "mem_after_pandas = process.memory_info().rss / (1024 * 1024)\n",
    "delta_pandas = mem_after_pandas - mem_before_pandas\n",
    "\n",
    "print(f\"\\nPANDAS:\")\n",
    "print(f\"  Memory before: {mem_before_pandas:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_pandas:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_pandas:>10.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Polars delta:  {delta_polars:>10.2f} MB\")\n",
    "print(f\"  Pandas delta:  {delta_pandas:>10.2f} MB\")\n",
    "print(f\"  Difference:    {abs(delta_pandas - delta_polars):>10.2f} MB\")\n",
    "\n",
    "if delta_polars < delta_pandas:\n",
    "    ratio = delta_pandas / delta_polars if delta_polars > 0 else float('inf')\n",
    "    print(f\"  Winner:        Polars ({ratio:.2f}x more efficient)\")\n",
    "else:\n",
    "    ratio = delta_polars / delta_pandas if delta_pandas > 0 else float('inf')\n",
    "    print(f\"  Winner:        Pandas ({ratio:.2f}x more efficient)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars consume **128.84 MB** vs **1,112.16 MB** de Pandas (**8.63x m√°s eficiente**). La diferencia es dram√°tica: 983.31 MB menos (casi 1 GB de ahorro). Esto se debe al storage columnar de Arrow y a que Polars solo extrae los campos necesarios durante el parsing. Pandas carga todas las columnas del JSON con overhead de Python objects, mientras Polars mantiene representaci√≥n compacta en Arrow format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Q1 - MEMORY-Optimized Experiments\n",
    "\n",
    "Los experimentos anteriores (TIME-optimized) cargaban el dataset completo en memoria para m√°xima velocidad. Ahora evaluamos **enfoques streaming** que priorizan m√≠nimo consumo de memoria a costa de mayor tiempo de ejecuci√≥n.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Validar el trade-off memoria vs tiempo:\n",
    "- ¬øCu√°nta memoria se ahorra con streaming?\n",
    "- ¬øCu√°nto tiempo adicional toma?\n",
    "- ¬øLos resultados son id√©nticos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Polars Streaming (MEMORY-optimized)\n",
    "\n",
    "Estrategia: usar lazy evaluation de Polars sin collect() temprano. Las agregaciones se procesan en streaming sin materializar todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory_polars(file_path: str) -> List[Tuple[date, str]]:\n",
    "    # Crear un LazyFrame a partir del archivo JSON Lines.\n",
    "    # No se carga el dataset completo en memoria.\n",
    "    # Solo se seleccionan los campos estrictamente necesarios:\n",
    "    # - date_only: fecha truncada a nivel d√≠a\n",
    "    # - username: nombre de usuario del autor del tweet\n",
    "    lazy_df = (\n",
    "        pl.scan_ndjson(file_path)\n",
    "        .select([\n",
    "            pl.col(\"date\").str.slice(0, 10).alias(\"date_only\"),\n",
    "            pl.col(\"user\").struct.field(\"username\").alias(\"username\")\n",
    "        ])\n",
    "        # Filtrar registros inv√°lidos de forma expl√≠cita\n",
    "        .filter(\n",
    "            pl.col(\"username\").is_not_null() &\n",
    "            pl.col(\"date_only\").is_not_null()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Primera pasada sobre el dataset (streaming):\n",
    "    # Se agrupa por fecha y se cuentan los tweets por d√≠a.\n",
    "    # Se ordena para obtener el top 10 de fechas m√°s activas.\n",
    "    # El collect() materializa solo este resultado agregado,\n",
    "    # no el DataFrame completo.\n",
    "    top_dates = (\n",
    "        lazy_df\n",
    "        .group_by(\"date_only\")\n",
    "        .agg(pl.len().alias(\"tweet_count\"))\n",
    "        .sort([\"tweet_count\", \"date_only\"], descending=[True, False])\n",
    "        .head(10)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Para cada una de las fechas top, se ejecuta una pasada adicional\n",
    "    # sobre el dataset para encontrar el usuario m√°s activo ese d√≠a.\n",
    "    for row in top_dates.iter_rows(named=True):\n",
    "        date_str = row[\"date_only\"]\n",
    "\n",
    "        # Filtrar por fecha espec√≠fica y agrupar por usuario.\n",
    "        # Cada collect() ejecuta un scan independiente del archivo,\n",
    "        # priorizando bajo uso de memoria sobre velocidad.\n",
    "        top_user = (\n",
    "            lazy_df\n",
    "            .filter(pl.col(\"date_only\") == date_str)\n",
    "            .group_by(\"username\")\n",
    "            .agg(pl.len().alias(\"user_tweet_count\"))\n",
    "            .sort([\"user_tweet_count\", \"username\"], descending=[True, False])\n",
    "            .head(1)\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        # Extraer el username ganador\n",
    "        username = top_user[\"username\"][0]\n",
    "\n",
    "        # Convertir la fecha de string a datetime.date\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "        # Agregar el resultado final\n",
    "        results.append((date_obj, username))\n",
    "\n",
    "    # Retornar la lista de resultados en el formato solicitado\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Pandas Chunked Processing (MEMORY-optimized)\n",
    "\n",
    "Estrategia: procesar el dataset por chunks usando `chunksize`. Mantener contadores incrementales sin crear DataFrames intermedios grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory_pandas(file_path: str) -> List[Tuple[date, str]]:\n",
    "    # Importar estructuras eficientes para conteos incrementales\n",
    "    # - Counter: conteo simple (fecha -> #tweets)\n",
    "    # - defaultdict(Counter): conteo anidado (fecha -> (username -> #tweets))\n",
    "    from collections import defaultdict, Counter\n",
    "\n",
    "    # Conteo total de tweets por fecha (YYYY-MM-DD)\n",
    "    date_counts = Counter()\n",
    "\n",
    "    # Conteo de tweets por usuario dentro de cada fecha\n",
    "    # date_user_counts[date_str][username] += 1\n",
    "    date_user_counts = defaultdict(Counter)\n",
    "\n",
    "    # Procesar el dataset en chunks para no cargar el archivo completo en memoria\n",
    "    chunk_size = 10000\n",
    "\n",
    "    # pd.read_json con chunksize permite iterar por DataFrames peque√±os\n",
    "    for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "        # Extraer la fecha a nivel d√≠a\n",
    "        chunk['date_only'] = chunk['date'].astype(str).str[:10]\n",
    "\n",
    "        # Extraer el username desde el diccionario en la columna 'user'\n",
    "        # apply con lambda hace extracci√≥n row-by-row (costosa, pero simple)\n",
    "        chunk['username'] = chunk['user'].apply(\n",
    "            lambda x: x.get('username') if isinstance(x, dict) else None\n",
    "        )\n",
    "\n",
    "        # Mantener solo columnas necesarias y eliminar filas inv√°lidas\n",
    "        chunk = chunk[['date_only', 'username']].dropna()\n",
    "\n",
    "        # Actualizar conteos de forma incremental sin almacenar tweets completos\n",
    "        # Nota: iterrows() es lento, pero minimiza memoria y evita DataFrames grandes persistentes\n",
    "        for _, row in chunk.iterrows():\n",
    "            date_str = row['date_only']\n",
    "            username = row['username']\n",
    "\n",
    "            # Conteo global por fecha\n",
    "            date_counts[date_str] += 1\n",
    "\n",
    "            # Conteo por fecha y usuario\n",
    "            date_user_counts[date_str][username] += 1\n",
    "\n",
    "    # Obtener top 10 fechas por n√∫mero de tweets\n",
    "    # Orden determin√≠stico:\n",
    "    # - conteo descendente\n",
    "    # - fecha ascendente en empates\n",
    "    top_dates = sorted(\n",
    "        date_counts.items(),\n",
    "        key=lambda x: (-x[1], x[0])\n",
    "    )[:10]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Para cada fecha top, seleccionar el usuario m√°s activo\n",
    "    # Orden determin√≠stico:\n",
    "    # - conteo descendente\n",
    "    # - username ascendente en empates\n",
    "    for date_str, _ in top_dates:\n",
    "        user_counts = date_user_counts[date_str]\n",
    "        top_user = sorted(\n",
    "            user_counts.items(),\n",
    "            key=lambda x: (-x[1], x[0])\n",
    "        )[0][0]\n",
    "\n",
    "        # Convertir la fecha a datetime.date\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "        # Agregar el resultado final\n",
    "        results.append((date_obj, top_user))\n",
    "\n",
    "    # Retornar lista de tuplas (fecha, username) en el formato solicitado\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verificaci√≥n: MEMORY Implementations\n",
    "\n",
    "Validar que los enfoques MEMORY producen resultados id√©nticos a los enfoques TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_memory_polars = q1_memory_polars(str(dataset_path))\n",
    "result_memory_pandas = q1_memory_pandas(str(dataset_path))\n",
    "\n",
    "print(\"Verification: Comparing All 4 Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_match = True\n",
    "\n",
    "if result_memory_polars == result_polars:\n",
    "    print(\"‚úÖ Polars MEMORY == Polars TIME\")\n",
    "else:\n",
    "    print(\"‚ùå Polars MEMORY != Polars TIME\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_pandas == result_pandas:\n",
    "    print(\"‚úÖ Pandas MEMORY == Pandas TIME\")\n",
    "else:\n",
    "    print(\"‚ùå Pandas MEMORY != Pandas TIME\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_polars == result_memory_pandas:\n",
    "    print(\"‚úÖ Polars MEMORY == Pandas MEMORY\")\n",
    "else:\n",
    "    print(\"‚ùå Polars MEMORY != Pandas MEMORY\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_polars == result_polars and result_polars == result_pandas:\n",
    "    print(\"‚úÖ All TIME approaches match\")\n",
    "else:\n",
    "    print(\"‚ùå TIME approaches don't match\")\n",
    "    all_match = False\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\nüéâ ALL FOUR APPROACHES PRODUCE IDENTICAL RESULTS\")\n",
    "    print(f\"   {len(result_memory_polars)} tuples verified across 4 implementations\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Results differ between approaches!\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de la Verificaci√≥n\n",
    "\n",
    "Esta verificaci√≥n es **cr√≠tica** porque valida que:\n",
    "\n",
    "1. **Correctitud**: Todos los enfoques resuelven el problema correctamente\n",
    "2. **Equivalencia**: La optimizaci√≥n (TIME vs MEMORY) no afecta los resultados\n",
    "3. **Confianza**: Podemos elegir cualquier enfoque bas√°ndonos solo en performance/memoria\n",
    "\n",
    "**¬øPor qu√© podr√≠an diferir?**:\n",
    "- **Bugs en implementaci√≥n**: Errores l√≥gicos en streaming o chunking\n",
    "- **Ordenamiento inconsistente**: Si hay empates y el orden de desempate difiere\n",
    "- **Manejo de casos borde**: Null values, datos malformados, etc.\n",
    "\n",
    "**Si la verificaci√≥n falla**:\n",
    "1. Revisar l√≥gica de ordenamiento (empates en counts)\n",
    "2. Verificar filtrado de nulls en todas las implementaciones\n",
    "3. Comparar manualmente algunos resultados espec√≠ficos\n",
    "\n",
    "La verificaci√≥n exitosa nos da **confianza** para proceder con benchmarking y an√°lisis de trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarks MEMORY: Tiempo de Ejecuci√≥n\n",
    "\n",
    "Medici√≥n de performance de los enfoques MEMORY-optimized con 3 runs cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Comparison: MEMORY-Optimized Approaches\n",
      "================================================================================\n",
      "\n",
      "Running Polars MEMORY implementation 3 times...\n",
      "  Run 1: 3.738s\n",
      "  Run 2: 3.241s\n",
      "  Run 3: 3.270s\n",
      "\n",
      "Running Pandas MEMORY implementation 3 times...\n",
      "  Run 1: 4.002s\n",
      "  Run 2: 4.017s\n",
      "  Run 3: 3.957s\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "\n",
      "Library                Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Polars MEMORY       3.241s     3.417s     3.738s\n",
      "Pandas MEMORY       3.957s     3.992s     4.017s\n",
      "\n",
      "Speedup:        1.17x (Polars MEMORY is 1.17x faster)\n",
      "Difference:     0.576s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "print(\"Time Comparison: MEMORY-Optimized Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nRunning Polars MEMORY implementation {n_runs} times...\")\n",
    "polars_memory_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q1_memory_polars(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    polars_memory_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "polars_memory_avg = sum(polars_memory_times) / len(polars_memory_times)\n",
    "polars_memory_min = min(polars_memory_times)\n",
    "polars_memory_max = max(polars_memory_times)\n",
    "\n",
    "print(f\"\\nRunning Pandas MEMORY implementation {n_runs} times...\")\n",
    "pandas_memory_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q1_memory_pandas(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    pandas_memory_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "pandas_memory_avg = sum(pandas_memory_times) / len(pandas_memory_times)\n",
    "pandas_memory_min = min(pandas_memory_times)\n",
    "pandas_memory_max = max(pandas_memory_times)\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Library':<15} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Polars MEMORY':<15} {polars_memory_min:>9.3f}s {polars_memory_avg:>9.3f}s {polars_memory_max:>9.3f}s\")\n",
    "print(f\"{'Pandas MEMORY':<15} {pandas_memory_min:>9.3f}s {pandas_memory_avg:>9.3f}s {pandas_memory_max:>9.3f}s\")\n",
    "\n",
    "speedup = pandas_memory_avg / polars_memory_avg if polars_memory_avg > 0 else float('inf')\n",
    "diff = abs(pandas_memory_avg - polars_memory_avg)\n",
    "\n",
    "print(f\"\\n{'Speedup:':<15} {speedup:.2f}x (Polars MEMORY is {speedup:.2f}x faster)\")\n",
    "print(f\"{'Difference:':<15} {diff:.3f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Benchmarks MEMORY\n",
    "\n",
    "Los resultados reales muestran una situaci√≥n **sorprendente**:\n",
    "\n",
    "**Polars MEMORY (Streaming)**:\n",
    "- **Tiempo real**: 3.417s (vs 0.325s TIME) ‚Üí **10.5x m√°s lento** que TIME\n",
    "- **Raz√≥n**: 11 scans completos del archivo (1 para top dates + 10 para top users)\n",
    "- Cada `collect()` ejecuta un full scan ‚Üí overhead de I/O masivo\n",
    "- El query optimizer no puede evitar re-leer el archivo en cada collect()\n",
    "\n",
    "**Pandas MEMORY (Chunked)**:\n",
    "- **Tiempo real**: 3.992s (vs 2.770s TIME) ‚Üí Solo **1.44x m√°s lento** que TIME\n",
    "- **Raz√≥n**: Procesamiento incremental es m√°s eficiente de lo esperado\n",
    "- Chunking con 10k filas + `.iterrows()` tiene overhead, pero es predecible\n",
    "- Solo lee el archivo UNA vez (vs 11 veces de Polars MEMORY)\n",
    "\n",
    "**Comparaci√≥n MEMORY vs MEMORY**:\n",
    "- Polars MEMORY es solo **1.17x m√°s r√°pido** que Pandas MEMORY (3.417s vs 3.992s)\n",
    "- ¬°Pandas MEMORY es competitivo! A diferencia de TIME donde Polars domina 8.51x\n",
    "\n",
    "**Insight clave**: \n",
    "- **Polars MEMORY**: El streaming con m√∫ltiples collect() tiene costo ALTO en I/O\n",
    "- **Pandas MEMORY**: Single-pass chunking es m√°s eficiente que esperado\n",
    "- Trade-off invertido: Pandas MEMORY tiene mejor ratio tiempo/memoria que Polars MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## cProfile MEMORY: An√°lisis de Latencia\n",
    "\n",
    "Profiling detallado de los enfoques MEMORY-optimized para identificar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling POLARS MEMORY implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         4048 function calls (4027 primitive calls) in 3.649 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 314 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       11    0.000    0.000    3.444    0.313 opt_flags.py:312(wrapper)\n",
      "       11    0.000    0.000    3.444    0.313 frame.py:2198(collect)\n",
      "       11    3.444    0.313    3.444    0.313 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        4    0.000    0.000    0.200    0.050 base_events.py:1962(_run_once)\n",
      "        4    0.000    0.000    0.199    0.050 selectors.py:540(select)\n",
      "        4    0.199    0.050    0.199    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.000    0.000    0.002    0.002 history.py:1025(writeout_cache)\n",
      "        1    0.000    0.000    0.002    0.002 history.py:1009(_writeout_input_cache)\n",
      "        2    0.001    0.001    0.001    0.001 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        4    0.000    0.000    0.001    0.000 events.py:87(_run)\n",
      "        4    0.000    0.000    0.001    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        3    0.000    0.000    0.001    0.000 ioloop.py:750(_run_callback)\n",
      "        3    0.000    0.000    0.001    0.000 zmqstream.py:573(_handle_events)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method strptime}\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:611(_flush)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "       11    0.000    0.000    0.000    0.000 frame.py:4225(filter)\n",
      "       10    0.000    0.000    0.000    0.000 _strptime.py:670(_strptime_datetime)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:684(<lambda>)\n",
      "       11    0.000    0.000    0.000    0.000 frame.py:4140(_filter)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         4048 function calls (4027 primitive calls) in 3.649 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 314 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       11    3.444    0.313    3.444    0.313 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        4    0.199    0.050    0.199    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        2    0.001    0.001    0.001    0.001 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        7    0.000    0.000    0.000    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "       21    0.000    0.000    0.000    0.000 socket.py:623(send)\n",
      "       10    0.000    0.000    0.000    0.000 _strptime.py:413(_strptime)\n",
      "  815/807    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       11    0.000    0.000    0.000    0.000 frame.py:4140(_filter)\n",
      "        4    0.000    0.000    0.200    0.050 base_events.py:1962(_run_once)\n",
      "       70    0.000    0.000    0.000    0.000 expr.py:21(parse_into_expression)\n",
      "       11    0.000    0.000    3.444    0.313 frame.py:2198(collect)\n",
      "       11    0.000    0.000    3.444    0.313 opt_flags.py:312(wrapper)\n",
      "      145    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       12    0.000    0.000    0.000    0.000 lit.py:30(lit)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method strptime}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       96    0.000    0.000    0.000    0.000 enum.py:1589(_get_value)\n",
      "       11    0.000    0.000    0.000    0.000 wrap.py:12(wrap_df)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10f4bf570>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling POLARS MEMORY implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q1_memory_polars(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis cProfile: Polars MEMORY vs Polars TIME\n",
    "\n",
    "**Datos reales del profiling**:\n",
    "\n",
    "1. **M√∫ltiples llamadas a `collect()`**:\n",
    "   - **Polars TIME**: 33 `collect()` tomando 0.494s (materializaci√≥n √∫nica del DataFrame)\n",
    "   - **Polars MEMORY**: 11 `collect()` tomando **3.444s** (94.4% del tiempo total de 3.649s)\n",
    "   - Cada `collect()` en MEMORY toma ~0.313s (promedio) vs 0.015s en TIME\n",
    "   - **Raz√≥n**: Cada collect() escanea TODO el archivo (389 MB) ‚Üí 11 √ó 389 MB = ~4.3 GB le√≠dos\n",
    "\n",
    "2. **Total function calls**:\n",
    "   - **Polars TIME**: 5,046 llamadas\n",
    "   - **Polars MEMORY**: 4,048 llamadas (menos overhead Python, pero m√°s I/O)\n",
    "\n",
    "3. **Implicaciones medidas**:\n",
    "   - **Tiempo**: 11 scans del archivo domina el tiempo (3.444s / 3.649s = 94%)\n",
    "   - **Memoria**: Cada collect() materializa solo el resultado peque√±o (~7 MB total)\n",
    "   - **I/O**: Bottleneck confirmado - leer archivo 11 veces vs 1 vez\n",
    "\n",
    "**Conclusi√≥n real**: \n",
    "- Polars MEMORY sacrifica **10.5x en tiempo** para ahorrar **94.4% de memoria**\n",
    "- El streaming NO es gratis: m√∫ltiples pases sobre el archivo tienen costo real\n",
    "- Para este tama√±o de dataset (389 MB), el overhead de I/O domina sobre el beneficio de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison: MEMORY-Optimized Approaches\n",
      "================================================================================\n",
      "\n",
      "POLARS MEMORY:\n",
      "  Memory before:     865.84 MB\n",
      "  Memory after:      873.06 MB\n",
      "  Delta:               7.22 MB\n",
      "\n",
      "PANDAS MEMORY:\n",
      "  Memory before:     873.06 MB\n",
      "  Memory after:      874.25 MB\n",
      "  Delta:               1.19 MB\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "  Polars MEMORY delta:        7.22 MB\n",
      "  Pandas MEMORY delta:        1.19 MB\n",
      "  Difference:                 6.03 MB\n",
      "  Winner:               Pandas MEMORY (6.08x more efficient)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: TIME vs MEMORY Approaches\n",
      "================================================================================\n",
      "\n",
      "Polars:\n",
      "  TIME approach:       128.84 MB\n",
      "  MEMORY approach:       7.22 MB\n",
      "  Savings:             121.62 MB (94.4% reduction)\n",
      "\n",
      "Pandas:\n",
      "  TIME approach:      1112.16 MB\n",
      "  MEMORY approach:       1.19 MB\n",
      "  Savings:            1110.97 MB (99.9% reduction)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory Comparison: MEMORY-Optimized Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gc.collect()\n",
    "mem_before_polars_memory = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q1_memory_polars(str(dataset_path))\n",
    "mem_after_polars_memory = process.memory_info().rss / (1024 * 1024)\n",
    "delta_polars_memory = mem_after_polars_memory - mem_before_polars_memory\n",
    "\n",
    "print(f\"\\nPOLARS MEMORY:\")\n",
    "print(f\"  Memory before: {mem_before_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_polars_memory:>10.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "mem_before_pandas_memory = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q1_memory_pandas(str(dataset_path))\n",
    "mem_after_pandas_memory = process.memory_info().rss / (1024 * 1024)\n",
    "delta_pandas_memory = mem_after_pandas_memory - mem_before_pandas_memory\n",
    "\n",
    "print(f\"\\nPANDAS MEMORY:\")\n",
    "print(f\"  Memory before: {mem_before_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_pandas_memory:>10.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Polars MEMORY delta:  {delta_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Pandas MEMORY delta:  {delta_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Difference:           {abs(delta_pandas_memory - delta_polars_memory):>10.2f} MB\")\n",
    "\n",
    "if delta_polars_memory < delta_pandas_memory:\n",
    "    ratio = delta_pandas_memory / delta_polars_memory if delta_polars_memory > 0 else float('inf')\n",
    "    print(f\"  Winner:               Polars MEMORY ({ratio:.2f}x more efficient)\")\n",
    "else:\n",
    "    ratio = delta_polars_memory / delta_pandas_memory if delta_pandas_memory > 0 else float('inf')\n",
    "    print(f\"  Winner:               Pandas MEMORY ({ratio:.2f}x more efficient)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: TIME vs MEMORY Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nPolars:\")\n",
    "print(f\"  TIME approach:   {delta_polars:>10.2f} MB\")\n",
    "print(f\"  MEMORY approach: {delta_polars_memory:>10.2f} MB\")\n",
    "if delta_polars_memory < delta_polars:\n",
    "    savings = delta_polars - delta_polars_memory\n",
    "    reduction = (savings / delta_polars) * 100 if delta_polars > 0 else 0\n",
    "    print(f\"  Savings:         {savings:>10.2f} MB ({reduction:.1f}% reduction)\")\n",
    "else:\n",
    "    overhead = delta_polars_memory - delta_polars\n",
    "    increase = (overhead / delta_polars) * 100 if delta_polars > 0 else 0\n",
    "    print(f\"  Overhead:        {overhead:>10.2f} MB ({increase:.1f}% increase)\")\n",
    "\n",
    "print(f\"\\nPandas:\")\n",
    "print(f\"  TIME approach:   {delta_pandas:>10.2f} MB\")\n",
    "print(f\"  MEMORY approach: {delta_pandas_memory:>10.2f} MB\")\n",
    "if delta_pandas_memory < delta_pandas:\n",
    "    savings = delta_pandas - delta_pandas_memory\n",
    "    reduction = (savings / delta_pandas) * 100 if delta_pandas > 0 else 0\n",
    "    print(f\"  Savings:         {savings:>10.2f} MB ({reduction:.1f}% reduction)\")\n",
    "else:\n",
    "    overhead = delta_pandas_memory - delta_pandas\n",
    "    increase = (overhead / delta_pandas) * 100 if delta_pandas > 0 else 0\n",
    "    print(f\"  Overhead:        {overhead:>10.2f} MB ({increase:.1f}% increase)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis de Consumo de Memoria: MEMORY vs TIME (Resultados Reales)\n",
    "\n",
    "**Resultados sorprendentes - Pandas MEMORY gana en memoria:**\n",
    "\n",
    "#### Polars MEMORY vs Polars TIME:\n",
    "- **Polars TIME**: 128.84 MB\n",
    "- **Polars MEMORY**: 7.22 MB\n",
    "- **Ahorro**: 121.62 MB (**94.4% de reducci√≥n**)\n",
    "- **Raz√≥n**: No materializa DataFrame completo, solo mantiene agregados peque√±os\n",
    "\n",
    "#### Pandas MEMORY vs Pandas TIME:\n",
    "- **Pandas TIME**: 1,112.16 MB  \n",
    "- **Pandas MEMORY**: 1.19 MB (!!)\n",
    "- **Ahorro**: 1,110.97 MB (**99.9% de reducci√≥n**)\n",
    "- **Raz√≥n**: Procesamiento por chunks descarta cada DataFrame inmediatamente, solo mantiene Counters\n",
    "\n",
    "#### Comparaci√≥n Polars MEMORY vs Pandas MEMORY:\n",
    "- **Ganador real**: ¬°PANDAS MEMORY! (1.19 MB vs 7.22 MB)\n",
    "- **Pandas MEMORY es 6.08x m√°s eficiente** que Polars MEMORY\n",
    "- **Raz√≥n 1**: Los Counters de Python son extremadamente compactos para este caso de uso\n",
    "- **Raz√≥n 2**: Polars mantiene alguna estructura Arrow incluso en streaming\n",
    "- **Raz√≥n 3**: El overhead de 10k rows por chunk en Pandas es despreciable vs diccionarios peque√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Globales Q1 ‚Äì Comparaci√≥n TIME vs MEMORY (Polars vs Pandas)\n",
    "\n",
    "Este an√°lisis consolida todos los experimentos realizados para Q1, comparando tiempo de ejecuci√≥n, uso de memoria y trade-offs arquitecturales entre Polars y Pandas bajo enfoques optimizados por tiempo y por memoria.  \n",
    "Todos los enfoques producen resultados id√©nticos, por lo que la evaluaci√≥n se centra exclusivamente en performance y consumo de RAM.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Tiempo de Ejecuci√≥n\n",
    "\n",
    "| Enfoque | Biblioteca | Tiempo promedio |\n",
    "|--------|-----------|-----------------|\n",
    "| TIME | **Polars** | **0.325 s** |\n",
    "| TIME | Pandas | 2.770 s |\n",
    "| MEMORY | **Polars** | **3.417 s** |\n",
    "| MEMORY | Pandas | 3.992 s |\n",
    "\n",
    "**Conclusiones clave**:\n",
    "- Polars es consistentemente m√°s r√°pido que Pandas en ambos enfoques.\n",
    "- En MEMORY, la diferencia entre Polars y Pandas es peque√±a (mismo orden de magnitud), pero Polars mantiene ventaja.\n",
    "- El overhead del streaming es esperado y aceptable frente al beneficio de reducir memoria.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Uso de Memoria (Delta RSS)\n",
    "\n",
    "| Enfoque | Biblioteca | Delta de memoria |\n",
    "|--------|-----------|------------------|\n",
    "| MEMORY | Pandas | **1.19 MB** |\n",
    "| MEMORY | Polars | 7.22 MB |\n",
    "| TIME | Polars | 128.84 MB |\n",
    "| TIME | Pandas | 1112.16 MB |\n",
    "\n",
    "**Interpretaci√≥n**:\n",
    "- La diferencia de memoria entre Pandas MEMORY y Polars MEMORY es peque√±a en t√©rminos absolutos para este dataset.\n",
    "- Dado el tama√±o relativamente reducido de los datos, **ambos enfoques MEMORY mantienen un footprint muy bajo**.\n",
    "- A medida que el dataset crezca, **la memoria en ambos enfoques MEMORY deber√≠a crecer lentamente**, mientras que el tiempo se vuelve el factor dominante.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Escalabilidad Esperada\n",
    "\n",
    "- En datasets mayores, el **costo relativo de memoria entre Pandas MEMORY y Polars MEMORY deja de ser decisivo**, ya que ambos evitan materializar grandes estructuras.\n",
    "- En cambio, **la escalabilidad en tiempo s√≠ se vuelve cr√≠tica**:\n",
    "  - Polars se beneficia de ejecuci√≥n en Rust, almacenamiento columnar Arrow y paralelizaci√≥n.\n",
    "  - Pandas mantiene overhead en parsing y ejecuci√≥n en Python.\n",
    "\n",
    "**Implicaci√≥n clave**: para datos grandes, la diferencia de tiempo entre Polars y Pandas tender√° a ampliarse, mientras que la diferencia de memoria seguir√° siendo acotada.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Trade-offs Arquitecturales\n",
    "\n",
    "- **Polars TIME**: mejor balance general cuando la RAM permite cargar el dataset completo.\n",
    "- **Polars MEMORY**: alternativa natural cuando la RAM no alcanza, con buena escalabilidad temporal.\n",
    "- **Pandas MEMORY**: muy eficiente en memoria, pero sin ventajas claras en tiempo frente a Polars.\n",
    "- **Pandas TIME**: no recomendable por consumo elevado de RAM y menor performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Recomendaci√≥n Final\n",
    "\n",
    "Dado que:\n",
    "- la diferencia de memoria entre enfoques MEMORY es peque√±a en t√©rminos pr√°cticos,\n",
    "- Polars escala mejor en tiempo,\n",
    "- y Polars ofrece ambos modos (TIME y MEMORY) con la misma sem√°ntica,\n",
    "\n",
    "la recomendaci√≥n es clara:\n",
    "\n",
    "**Usar Polars en todos los escenarios**:\n",
    "- **Polars TIME** cuando la RAM permite cargar el dataset completo.\n",
    "- **Polars MEMORY** cuando la RAM es el factor limitante.\n",
    "\n",
    "Pandas queda relegado a escenarios de compatibilidad o restricci√≥n de stack, no por ventajas t√©cnicas.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusi√≥n Final\n",
    "\n",
    "La elecci√≥n no debe basarse √∫nicamente en el consumo m√≠nimo de memoria observado en datasets peque√±os, sino en **c√≥mo escalan las soluciones**.  \n",
    "En ese contexto, **Polars ofrece el mejor equilibrio presente y futuro**, tanto en tiempo como en memoria, y es la opci√≥n recomendada para Q1 en producci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

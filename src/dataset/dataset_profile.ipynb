{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Profiling - LATAM Data Engineer Challenge\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook realiza un análisis exhaustivo del dataset **antes** de implementar las soluciones a las preguntas q1, q2 y q3.\n",
    "\n",
    "**¿Por qué hacer profiling primero?**\n",
    "\n",
    "1. **Identificar casos borde** que podrían romper las implementaciones\n",
    "2. **Validar calidad de datos** y detectar registros inválidos\n",
    "3. **Guiar decisiones de diseño** basadas en evidencia, no suposiciones\n",
    "4. **Estimar tiempos de procesamiento** para elegir estrategias time vs memory\n",
    "5. **Evitar retrabajos** por descubrir problemas tarde\n",
    "\n",
    "## Metodología\n",
    "\n",
    "- **Streaming**: No cargamos el dataset completo en memoria\n",
    "- **Muestreo inteligente**: Analizamos todo el dataset cuando es posible\n",
    "- **Métricas accionables**: Cada hallazgo se traduce en una decisión técnica\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga y Validación Inicial\n",
    "\n",
    "Primero verificamos que el dataset esté disponible y en formato JSON Lines correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found: 388.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "DATASET_PATH = \"../../data/raw/farmers-protest-tweets-2021-2-4.json\"\n",
    "SAMPLE_SIZE = None  # None = todo el dataset, N = primeras N líneas\n",
    "\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"\\nDownload the dataset first:\")\n",
    "    print(\"  python src/dataset/download_dataset.py\")\n",
    "else:\n",
    "    file_size_mb = dataset_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Dataset found: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total lines...\n",
      "\n",
      "Total lines: 117,407\n",
      "Counting time: 0.28 seconds\n",
      "Throughput: 413520 lines/second\n"
     ]
    }
   ],
   "source": [
    "# Conteo rápido de líneas totales\n",
    "print(\"Counting total lines...\")\n",
    "\n",
    "total_lines = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    for line in f:\n",
    "        total_lines += 1\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTotal lines: {total_lines:,}\")\n",
    "print(f\"Counting time: {elapsed:.2f} seconds\")\n",
    "print(f\"Throughput: {total_lines/elapsed:.0f} lines/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis de Esquema y Calidad\n",
    "\n",
    "Este análisis se divide en dos partes:\n",
    "\n",
    "**Parte 1: Estructura y Esquema**\n",
    "- Análisis rápido de los primeros 1,000 tweets\n",
    "- Identificación de todos los campos presentes\n",
    "- Tipos de datos por campo\n",
    "- Ejemplo de estructura completa\n",
    "\n",
    "**Parte 2: Análisis Estadístico Completo**\n",
    "- Procesamiento de todo el dataset\n",
    "- Campos faltantes o null\n",
    "- Tipos de datos incorrectos\n",
    "- Errores de parseo JSON\n",
    "- Métricas de calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  SCHEMA AND STRUCTURE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Analyzing first 1,000 tweets to understand data structure...\n",
      "\n",
      "Lines analyzed: 1,000\n",
      "Non-dictionary lines found: 0\n",
      "\n",
      "All fields found in tweets:\n",
      "  - content: str(1000)\n",
      "  - conversationId: int(1000)\n",
      "  - date: str(1000)\n",
      "  - id: int(1000)\n",
      "  - lang: str(1000)\n",
      "  - likeCount: int(1000)\n",
      "  - media: NoneType(735), list(265)\n",
      "  - mentionedUsers: NoneType(709), list(291)\n",
      "  - outlinks: list(1000)\n",
      "  - quoteCount: int(1000)\n",
      "  - quotedTweet: NoneType(620), dict(380)\n",
      "  - renderedContent: str(1000)\n",
      "  - replyCount: int(1000)\n",
      "  - retweetCount: int(1000)\n",
      "  - retweetedTweet: NoneType(1000)\n",
      "  - source: str(1000)\n",
      "  - sourceLabel: str(990), NoneType(10)\n",
      "  - sourceUrl: str(990), NoneType(10)\n",
      "  - tcooutlinks: list(1000)\n",
      "  - url: str(1000)\n",
      "  - user: dict(1000)\n",
      "\n",
      "Example tweet structure:\n",
      "------------------------------------------------------------\n",
      "{\n",
      "  \"url\": \"https://twitter.com/ArjunSinghPanam/status/1364506249291784198\",\n",
      "  \"date\": \"2021-02-24T09:23:35+00:00\",\n",
      "  \"content\": \"The world progresses while the Indian police and Govt are still trying to take India back to the horrific past through its tyranny. \\n\\n@narendramodi @DelhiPolice Shame on you. \\n\\n#ModiDontSellFarmers \\n#FarmersProtest \\n#FreeNodeepKaur https://t.co/es3kn0IQAF\",\n",
      "  \"renderedContent\": \"The world progresses while the Indian police and Govt are still trying to take India back to the horrific past through its tyranny. \\n\\n@narendramodi @DelhiPolice Shame on you. \\n\\n#ModiDontSellFarmers \\n#FarmersProtest \\n#FreeNodeepKaur twitter.com/ravisinghka/st…\",\n",
      "  \"id\": 1364506249291784198,\n",
      "  \"user\": {\n",
      "    \"username\": \"ArjunSinghPanam\",\n",
      "    \"displayname\": \"Arjun Singh Panam\",\n",
      "    \"id\": 45091142,\n",
      "    \"description\": \"Global Citizen, Actor, Director: Sky is the roof above my head, the world is the road I travel, love is my food & mother earth is my bed. Roy in @CosmosMovie\",\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# PART 1: Schema and Structure Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"  SCHEMA AND STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAnalyzing first 1,000 tweets to understand data structure...\\n\")\n",
    "\n",
    "schema_stats = {\n",
    "    'all_fields': set(),\n",
    "    'field_types': defaultdict(Counter),\n",
    "    'sample_tweets': [],\n",
    "    'lines_analyzed': 0,\n",
    "    'non_dict_count': 0,\n",
    "}\n",
    "\n",
    "SCHEMA_SAMPLE_SIZE = 1000\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        if line_num > SCHEMA_SAMPLE_SIZE:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            tweet = orjson.loads(line)\n",
    "            \n",
    "            # Verificar que sea un diccionario\n",
    "            if not isinstance(tweet, dict):\n",
    "                schema_stats['non_dict_count'] += 1\n",
    "                continue\n",
    "            \n",
    "            schema_stats['lines_analyzed'] += 1\n",
    "            \n",
    "            # Guardar primeros 3 tweets como muestra\n",
    "            if len(schema_stats['sample_tweets']) < 3:\n",
    "                schema_stats['sample_tweets'].append(tweet)\n",
    "            \n",
    "            # Recolectar todos los campos\n",
    "            for field in tweet.keys():\n",
    "                schema_stats['all_fields'].add(field)\n",
    "                \n",
    "                # Registrar tipo de dato\n",
    "                field_value = tweet[field]\n",
    "                field_type = type(field_value).__name__\n",
    "                schema_stats['field_types'][field][field_type] += 1\n",
    "                \n",
    "        except orjson.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "print(f\"Lines analyzed: {schema_stats['lines_analyzed']:,}\")\n",
    "print(f\"Non-dictionary lines found: {schema_stats['non_dict_count']}\")\n",
    "print()\n",
    "\n",
    "# Mostrar todos los campos encontrados\n",
    "print(\"All fields found in tweets:\")\n",
    "for field in sorted(schema_stats['all_fields']):\n",
    "    types = schema_stats['field_types'][field]\n",
    "    type_str = ', '.join([f\"{t}({c})\" for t, c in types.most_common()])\n",
    "    print(f\"  - {field}: {type_str}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Mostrar ejemplo de estructura completa\n",
    "if schema_stats['sample_tweets']:\n",
    "    print(\"Example tweet structure:\")\n",
    "    print(\"-\" * 60)\n",
    "    import json\n",
    "    sample = schema_stats['sample_tweets'][0]\n",
    "    print(json.dumps(sample, indent=2, ensure_ascii=False)[:1000] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de esquema (muestra inicial)\n",
    "\n",
    "Se analizaron **1.000 tweets** para validar la estructura del dataset antes de implementar las soluciones.\n",
    "\n",
    "- Todas las líneas son JSON válidas, no se encontraron registros corruptos.\n",
    "- El esquema es consistente entre tweets.\n",
    "- Los campos clave (`date`, `content`, `user`) están presentes en el 100% de la muestra.\n",
    "- `mentionedUsers` es opcional (≈71% `None`, ≈29% lista), por lo que debe manejarse explícitamente.\n",
    "- Otros campos como `media` y `quotedTweet` también pueden ser `None`.\n",
    "\n",
    "**Implicaciones:** el dataset puede procesarse en streaming sin limpieza previa, usando `mentionedUsers` como fuente principal para menciones y parseando `date` directamente en formato ISO 8601.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  COMPLETE STATISTICAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Analyzing all tweets for detailed statistics...\n",
      "\n",
      "Processed: 110,000 lines (17336 lines/sec)\n",
      "\n",
      "Analysis completed in 6.76 seconds\n",
      "Average throughput: 17359 lines/sec\n"
     ]
    }
   ],
   "source": [
    "# PART 2: Complete Statistical Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  COMPLETE STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAnalyzing all tweets for detailed statistics...\\n\")\n",
    "\n",
    "stats = {\n",
    "    'valid_lines': 0,\n",
    "    'invalid_lines': 0,\n",
    "    'non_dict_lines': 0,\n",
    "    'parse_errors': [],\n",
    "    'missing_fields': defaultdict(int),\n",
    "    'date_formats': Counter(),\n",
    "    'content_lengths': [],\n",
    "    'mentions_distribution': [],\n",
    "    'emoji_count': 0,\n",
    "    'tweets_with_emojis': 0,\n",
    "    'sample_tweets': [],\n",
    "}\n",
    "\n",
    "mentions_stats = {\n",
    "    'with_mentions': 0,\n",
    "    'without_mentions': 0,\n",
    "    'null_mentions': 0,\n",
    "    'empty_list': 0,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        \n",
    "        # Progreso cada 10k líneas\n",
    "        if line_num % 10000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            throughput = line_num / elapsed if elapsed > 0 else 0\n",
    "            print(f\"Processed: {line_num:,} lines ({throughput:.0f} lines/sec)\", end='\\r')\n",
    "        \n",
    "        # Limitar a muestra si se especificó\n",
    "        if SAMPLE_SIZE and line_num > SAMPLE_SIZE:\n",
    "            break\n",
    "        \n",
    "        # Parsear línea\n",
    "        try:\n",
    "            tweet = orjson.loads(line)\n",
    "            \n",
    "            # Verificar que sea un diccionario (no un número u otro tipo)\n",
    "            if not isinstance(tweet, dict):\n",
    "                stats['non_dict_lines'] += 1\n",
    "                stats['invalid_lines'] += 1\n",
    "                if len(stats['parse_errors']) < 5:\n",
    "                    stats['parse_errors'].append({\n",
    "                        'line': line_num,\n",
    "                        'error': f'Not a dictionary: {type(tweet).__name__}',\n",
    "                        'sample': str(tweet)[:100]\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            stats['valid_lines'] += 1\n",
    "            \n",
    "            # Guardar algunos tweets de muestra\n",
    "            if len(stats['sample_tweets']) < 3:\n",
    "                stats['sample_tweets'].append(tweet)\n",
    "            \n",
    "            # Analizar campos críticos\n",
    "            for field in ['date', 'content', 'user', 'mentionedUsers']:\n",
    "                if field not in tweet or tweet[field] is None:\n",
    "                    stats['missing_fields'][field] += 1\n",
    "            \n",
    "            # Analizar fecha\n",
    "            if 'date' in tweet and tweet['date']:\n",
    "                date_str = str(tweet['date'])\n",
    "                if 'T' in date_str:\n",
    "                    stats['date_formats']['ISO-8601'] += 1\n",
    "                else:\n",
    "                    stats['date_formats']['OTHER'] += 1\n",
    "            \n",
    "            # Analizar contenido\n",
    "            if 'content' in tweet and tweet['content']:\n",
    "                content = str(tweet['content'])\n",
    "                stats['content_lengths'].append(len(content))\n",
    "                \n",
    "                # Detectar emojis\n",
    "                emojis_in_tweet = emoji.emoji_count(content)\n",
    "                if emojis_in_tweet > 0:\n",
    "                    stats['tweets_with_emojis'] += 1\n",
    "                    stats['emoji_count'] += emojis_in_tweet\n",
    "            \n",
    "            # Analizar menciones\n",
    "            if 'mentionedUsers' not in tweet or tweet['mentionedUsers'] is None:\n",
    "                mentions_stats['null_mentions'] += 1\n",
    "            elif isinstance(tweet['mentionedUsers'], list):\n",
    "                num_mentions = len(tweet['mentionedUsers'])\n",
    "                stats['mentions_distribution'].append(num_mentions)\n",
    "                \n",
    "                if num_mentions == 0:\n",
    "                    mentions_stats['empty_list'] += 1\n",
    "                    mentions_stats['without_mentions'] += 1\n",
    "                else:\n",
    "                    mentions_stats['with_mentions'] += 1\n",
    "            \n",
    "            # Verificar user.username\n",
    "            if 'user' in tweet and isinstance(tweet['user'], dict):\n",
    "                if 'username' not in tweet['user'] or tweet['user']['username'] is None:\n",
    "                    stats['missing_fields']['user.username'] += 1\n",
    "            \n",
    "        except orjson.JSONDecodeError as e:\n",
    "            stats['invalid_lines'] += 1\n",
    "            if len(stats['parse_errors']) < 5:\n",
    "                stats['parse_errors'].append({\n",
    "                    'line': line_num,\n",
    "                    'error': str(e),\n",
    "                    'sample': line[:100].decode('utf-8', errors='ignore')\n",
    "                })\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\\nAnalysis completed in {processing_time:.2f} seconds\")\n",
    "print(f\"Average throughput: {(stats['valid_lines'] + stats['invalid_lines'])/processing_time:.0f} lines/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis estadístico completo del dataset\n",
    "\n",
    "En esta sección se recorrió el **dataset completo en streaming** para obtener estadísticas detalladas de calidad, contenido y estructura, sin cargar el archivo completo en memoria.\n",
    "\n",
    "Durante el recorrido se midió el **throughput de procesamiento**, alcanzando aproximadamente **17.3K tweets por segundo**, lo que confirma que el enfoque de lectura línea a línea con `orjson` es adecuado para los enfoques optimizados en memoria.\n",
    "\n",
    "Este análisis permitió:\n",
    "- Cuantificar la presencia de campos faltantes en atributos críticos.\n",
    "- Confirmar el formato de fechas predominante (ISO 8601).\n",
    "- Medir la distribución de longitudes de texto.\n",
    "- Evaluar la frecuencia y distribución de menciones (`mentionedUsers`).\n",
    "- Detectar la presencia y cantidad de emojis en el contenido.\n",
    "- Identificar líneas inválidas o no conformes (si existieran).\n",
    "\n",
    "Los resultados validan que el dataset puede procesarse eficientemente en una sola pasada, y sirven como base para definir las estrategias finales de implementación para q1, q2 y q3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: Validación Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  BASIC VALIDATION\n",
      "============================================================\n",
      "Total processed: 117,407 lines\n",
      "Valid lines: 117,407 (100.00%)\n",
      "Invalid lines: 0 (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_processed = stats['valid_lines'] + stats['invalid_lines']\n",
    "invalid_pct = (stats['invalid_lines'] / total_processed * 100) if total_processed > 0 else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  BASIC VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total processed: {total_processed:,} lines\")\n",
    "print(f\"Valid lines: {stats['valid_lines']:,} ({100-invalid_pct:.2f}%)\")\n",
    "print(f\"Invalid lines: {stats['invalid_lines']:,} ({invalid_pct:.2f}%)\")\n",
    "\n",
    "if stats.get('non_dict_lines', 0) > 0:\n",
    "    print(f\"  - Non-dictionary JSON: {stats['non_dict_lines']:,}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if stats['parse_errors']:\n",
    "    print(\"Parse errors (first 5):\")\n",
    "    for error in stats['parse_errors']:\n",
    "        print(f\"  Line {error['line']}: {error['error']}\")\n",
    "        print(f\"  Sample: {error['sample'][:80]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación básica del dataset\n",
    "\n",
    "Se procesaron **117.407 líneas**, todas válidas (100%), sin registros corruptos ni errores de parseo, lo que confirma que el dataset puede procesarse directamente sin una etapa previa de limpieza estructural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: Calidad de Datos (Campos Faltantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  DATA QUALITY - MISSING FIELDS\n",
      "============================================================\n",
      "         Field  Missing Percentage\n",
      "mentionedUsers    79373     67.60%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  DATA QUALITY - MISSING FIELDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if stats['missing_fields']:\n",
    "    missing_df = pd.DataFrame([\n",
    "        {\n",
    "            'Field': field,\n",
    "            'Missing': count,\n",
    "            'Percentage': f\"{(count/stats['valid_lines']*100):.2f}%\"\n",
    "        }\n",
    "        for field, count in sorted(stats['missing_fields'].items(), key=lambda x: x[1], reverse=True)\n",
    "    ])\n",
    "    \n",
    "    print(missing_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missing fields detected\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calidad de datos, campos faltantes\n",
    "\n",
    "El campo **`mentionedUsers`** está ausente en aproximadamente **67.6%** de los tweets. Esto implica que, para **q3**, la lógica debe tratar explícitamente este campo como opcional y considerar que la ausencia de menciones es el caso dominante. Para **q1** y **q2**, este faltante no afecta el cálculo, ya que no dependen de este atributo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de Campos Relevantes para las Preguntas\n",
    "\n",
    "### 3.1. Campo `date` (Relevante para Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  DATE ANALYSIS (Q1)\n",
      "============================================================\n",
      "\n",
      "Detected formats:\n",
      "  ISO-8601: 117,407 (100.00%)\n",
      "\n",
      "Example date: 2021-02-24T09:23:35+00:00\n",
      "\n",
      "Tweets without date: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  DATE ANALYSIS (Q1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if stats['date_formats']:\n",
    "    print(\"\\nDetected formats:\")\n",
    "    for fmt, count in stats['date_formats'].most_common():\n",
    "        pct = (count / stats['valid_lines'] * 100) if stats['valid_lines'] > 0 else 0\n",
    "        print(f\"  {fmt}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Mostrar ejemplo de fecha\n",
    "if stats['sample_tweets'] and 'date' in stats['sample_tweets'][0]:\n",
    "    print(f\"\\nExample date: {stats['sample_tweets'][0]['date']}\")\n",
    "\n",
    "print(f\"\\nTweets without date: {stats['missing_fields'].get('date', 0):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de fechas (Q1)\n",
    "\n",
    "El **100%** de los tweets contiene el campo `date` en formato **ISO 8601**, sin registros faltantes. Esto permite parsear y agrupar fechas de forma directa y eficiente para **q1**, sin necesidad de normalización adicional ni manejo de casos borde por ausencia de fecha.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Campo `content` (Relevante para Q2 - Emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  CONTENT ANALYSIS (Q2)\n",
      "============================================================\n",
      "\n",
      "Tweet length statistics:\n",
      "  Minimum: 15 chars\n",
      "  P50 (median): 128 chars\n",
      "  P95: 298 chars\n",
      "  P99: 312 chars\n",
      "  Maximum: 962 chars\n",
      "\n",
      "Tweets without content: 0\n",
      "\n",
      "Emoji Analysis:\n",
      "  Tweets with emojis: 16,874 (14.37%)\n",
      "  Total emojis detected: 42,922\n",
      "  Average emojis per tweet (with emojis): 2.54\n",
      "\n",
      "Example content: The world progresses while the Indian police and Govt are still trying to take India back to the horrific past through its tyranny. \n",
      "\n",
      "@narendramodi @D...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  CONTENT ANALYSIS (Q2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if stats['content_lengths']:\n",
    "    content_series = pd.Series(stats['content_lengths'])\n",
    "    \n",
    "    print(\"\\nTweet length statistics:\")\n",
    "    print(f\"  Minimum: {content_series.min():,} chars\")\n",
    "    print(f\"  P50 (median): {content_series.quantile(0.5):.0f} chars\")\n",
    "    print(f\"  P95: {content_series.quantile(0.95):.0f} chars\")\n",
    "    print(f\"  P99: {content_series.quantile(0.99):.0f} chars\")\n",
    "    print(f\"  Maximum: {content_series.max():,} chars\")\n",
    "\n",
    "print(f\"\\nTweets without content: {stats['missing_fields'].get('content', 0):,}\")\n",
    "\n",
    "print(\"\\nEmoji Analysis:\")\n",
    "pct_with_emojis = (stats['tweets_with_emojis'] / stats['valid_lines'] * 100) if stats['valid_lines'] > 0 else 0\n",
    "print(f\"  Tweets with emojis: {stats['tweets_with_emojis']:,} ({pct_with_emojis:.2f}%)\")\n",
    "print(f\"  Total emojis detected: {stats['emoji_count']:,}\")\n",
    "if stats['tweets_with_emojis'] > 0:\n",
    "    avg_emojis = stats['emoji_count'] / stats['tweets_with_emojis']\n",
    "    print(f\"  Average emojis per tweet (with emojis): {avg_emojis:.2f}\")\n",
    "\n",
    "# Mostrar ejemplo de contenido\n",
    "if stats['sample_tweets'] and 'content' in stats['sample_tweets'][0]:\n",
    "    content_sample = stats['sample_tweets'][0]['content'][:150]\n",
    "    print(f\"\\nExample content: {content_sample}...\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de contenido y emojis (Q2)\n",
    "\n",
    "El contenido textual es consistente y está presente en el **100%** de los tweets, con longitudes moderadas (mediana ≈128 caracteres), lo que permite procesarlo eficientemente en streaming. Aproximadamente **14.4%** de los tweets contiene emojis, con un promedio de **2.5 emojis por tweet** cuando están presentes. Estos resultados indican que **q2** puede resolverse sin preprocesamiento adicional, aplicando detección de emojis directamente sobre el campo `content`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Campo `mentionedUsers` (Relevante para Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  MENTIONS ANALYSIS (Q3)\n",
      "============================================================\n",
      "\n",
      "Mentions distribution:\n",
      "  With mentions: 38,034 (32.40%)\n",
      "  Without mentions: 0 (0.00%)\n",
      "  Null field: 79,373 (67.60%)\n",
      "  Empty list: 0 (0.00%)\n",
      "\n",
      "Mentions per tweet statistics:\n",
      "  Average: 2.72 mentions/tweet\n",
      "  Maximum: 50 mentions in a tweet\n",
      "\n",
      "Mentions count distribution:\n",
      "  1 mentions: 19,422 tweets (51.06%)\n",
      "  2 mentions: 7,920 tweets (20.82%)\n",
      "  3 mentions: 3,251 tweets (8.55%)\n",
      "  4 mentions: 1,878 tweets (4.94%)\n",
      "  5 mentions: 1,059 tweets (2.78%)\n",
      "  6 mentions: 971 tweets (2.55%)\n",
      "  7 mentions: 571 tweets (1.50%)\n",
      "  8 mentions: 518 tweets (1.36%)\n",
      "  9 mentions: 540 tweets (1.42%)\n",
      "  10 mentions: 362 tweets (0.95%)\n",
      "\n",
      "Example mentions: [{'username': 'narendramodi', 'displayname': 'Narendra Modi', 'id': 18839785, 'description': None, 'rawDescription': None, 'descriptionUrls': None, 'verified': None, 'created': None, 'followersCount': None, 'friendsCount': None, 'statusesCount': None, 'favouritesCount': None, 'listedCount': None, 'mediaCount': None, 'location': None, 'protected': None, 'linkUrl': None, 'linkTcourl': None, 'profileImageUrl': None, 'profileBannerUrl': None, 'url': 'https://twitter.com/narendramodi'}, {'username': 'DelhiPolice', 'displayname': '#DilKiPolice Delhi Police', 'id': 1850705408, 'description': None, 'rawDescription': None, 'descriptionUrls': None, 'verified': None, 'created': None, 'followersCount': None, 'friendsCount': None, 'statusesCount': None, 'favouritesCount': None, 'listedCount': None, 'mediaCount': None, 'location': None, 'protected': None, 'linkUrl': None, 'linkTcourl': None, 'profileImageUrl': None, 'profileBannerUrl': None, 'url': 'https://twitter.com/DelhiPolice'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  MENTIONS ANALYSIS (Q3)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total = stats['valid_lines']\n",
    "\n",
    "print(\"\\nMentions distribution:\")\n",
    "print(f\"  With mentions: {mentions_stats['with_mentions']:,} ({mentions_stats['with_mentions']/total*100:.2f}%)\")\n",
    "print(f\"  Without mentions: {mentions_stats['without_mentions']:,} ({mentions_stats['without_mentions']/total*100:.2f}%)\")\n",
    "print(f\"  Null field: {mentions_stats['null_mentions']:,} ({mentions_stats['null_mentions']/total*100:.2f}%)\")\n",
    "print(f\"  Empty list: {mentions_stats['empty_list']:,} ({mentions_stats['empty_list']/total*100:.2f}%)\")\n",
    "\n",
    "if stats['mentions_distribution']:\n",
    "    mentions_series = pd.Series(stats['mentions_distribution'])\n",
    "    \n",
    "    print(\"\\nMentions per tweet statistics:\")\n",
    "    print(f\"  Average: {mentions_series.mean():.2f} mentions/tweet\")\n",
    "    print(f\"  Maximum: {mentions_series.max()} mentions in a tweet\")\n",
    "    \n",
    "    print(\"\\nMentions count distribution:\")\n",
    "    mention_counts = mentions_series.value_counts().sort_index().head(10)\n",
    "    for num_mentions, count in mention_counts.items():\n",
    "        pct = (count / len(mentions_series) * 100)\n",
    "        print(f\"  {num_mentions} mentions: {count:,} tweets ({pct:.2f}%)\")\n",
    "\n",
    "# Mostrar ejemplo de menciones\n",
    "if stats['sample_tweets']:\n",
    "    for tweet in stats['sample_tweets']:\n",
    "        if 'mentionedUsers' in tweet and tweet['mentionedUsers'] and len(tweet['mentionedUsers']) > 0:\n",
    "            print(f\"\\nExample mentions: {tweet['mentionedUsers'][:3]}\")\n",
    "            break\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de menciones (Q3)\n",
    "\n",
    "El **67.6%** de los tweets no contiene el campo `mentionedUsers`, mientras que el **32.4%** sí incluye menciones. Cuando existen, las menciones son moderadas (promedio ≈2.7 por tweet), aunque hay casos extremos con hasta 50 menciones. Esto confirma que **q3** debe tratar `mentionedUsers` como campo opcional y puede calcular la influencia de usuarios eficientemente iterando solo sobre los tweets que contienen menciones, sin necesidad de analizar el texto completo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estructura de un Tweet (Ejemplo Real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  COMPLETE TWEET EXAMPLE\n",
      "============================================================\n",
      "{\n",
      "  \"date\": \"2021-02-24T09:23:35+00:00\",\n",
      "  \"content\": \"The world progresses while the Indian police and Govt are still trying to take India back to the horrific past through its tyranny. \\n\\n@narendramodi @DelhiPolice Shame on you. \\n\\n#ModiDontSellFarmers \\n#...\",\n",
      "  \"user\": {\n",
      "    \"username\": \"ArjunSinghPanam\"\n",
      "  },\n",
      "  \"mentionedUsers\": [\n",
      "    {\n",
      "      \"username\": \"narendramodi\",\n",
      "      \"displayname\": \"Narendra Modi\",\n",
      "      \"id\": 18839785,\n",
      "      \"description\": null,\n",
      "      \"rawDescription\": null,\n",
      "      \"descriptionUrls\": null,\n",
      "      \"verified\": null,\n",
      "      \"created\": null,\n",
      "      \"followersCount\": null,\n",
      "      \"friendsCount\": null,\n",
      "      \"statusesCount\": null,\n",
      "      \"favouritesCount\": null,\n",
      "      \"listedCount\": null,\n",
      "      \"mediaCount\": null,\n",
      "      \"location\": null,\n",
      "      \"protected\": null,\n",
      "      \"linkUrl\": null,\n",
      "      \"linkTcourl\": null,\n",
      "      \"profileImageUrl\": null,\n",
      "      \"profileBannerUrl\": null,\n",
      "      \"url\": \"https://twitter.com/narendramodi\"\n",
      "    },\n",
      "    {\n",
      "      \"username\": \"DelhiPolice\",\n",
      "      \"displayname\": \"#DilKiPolice Delhi Police\",\n",
      "      \"id\": 1850705408,\n",
      "      \"description\": null,\n",
      "      \"rawDescription\": null,\n",
      "      \"descriptionUrls\": null,\n",
      "      \"verified\": null,\n",
      "      \"created\": null,\n",
      "      \"followersCount\": null,\n",
      "      \"friendsCount\": null,\n",
      "      \"statusesCount\": null,\n",
      "      \"favouritesCount\": null,\n",
      "      \"listedCount\": null,\n",
      "      \"mediaCount\": null,\n",
      "      \"location\": null,\n",
      "      \"protected\": null,\n",
      "      \"linkUrl\": null,\n",
      "      \"linkTcourl\": null,\n",
      "      \"profileImageUrl\": null,\n",
      "      \"profileBannerUrl\": null,\n",
      "      \"url\": \"https://twitter.com/DelhiPolice\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if stats['sample_tweets']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"  COMPLETE TWEET EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sample = stats['sample_tweets'][0]\n",
    "    \n",
    "    # Mostrar solo campos relevantes para el challenge\n",
    "    relevant_fields = {\n",
    "        'date': sample.get('date'),\n",
    "        'content': sample.get('content', '')[:200] + '...' if sample.get('content') else None,\n",
    "        'user': {\n",
    "            'username': sample.get('user', {}).get('username') if isinstance(sample.get('user'), dict) else None\n",
    "        },\n",
    "        'mentionedUsers': sample.get('mentionedUsers'),\n",
    "    }\n",
    "    \n",
    "    print(json.dumps(relevant_fields, indent=2, ensure_ascii=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance y Estimaciones\n",
    "\n",
    "Estos datos son críticos para decidir entre enfoques time-optimized vs memory-optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  PERFORMANCE AND ESTIMATIONS\n",
      "============================================================\n",
      "\n",
      "Complete processing time: 6.76 seconds\n",
      "Streaming read throughput: 17359 lines/second\n",
      "Throughput: ~1041555 lines/minute\n",
      "\n",
      "Implications for time vs memory:\n",
      "  - Streaming is viable: high throughput\n",
      "  - Memory approach: streaming line by line\n",
      "  - Time approach: load in memory with pandas/polars if sufficient RAM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  PERFORMANCE AND ESTIMATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "throughput = (stats['valid_lines'] + stats['invalid_lines']) / processing_time\n",
    "\n",
    "print(f\"\\nComplete processing time: {processing_time:.2f} seconds\")\n",
    "print(f\"Streaming read throughput: {throughput:.0f} lines/second\")\n",
    "print(f\"Throughput: ~{throughput*60:.0f} lines/minute\")\n",
    "\n",
    "# Estimaciones\n",
    "if total_lines > (stats['valid_lines'] + stats['invalid_lines']):\n",
    "    estimated_time = total_lines / throughput\n",
    "    print(f\"\\nEstimated time for complete dataset: {estimated_time:.2f} seconds (~{estimated_time/60:.1f} minutes)\")\n",
    "\n",
    "print(\"\\nImplications for time vs memory:\")\n",
    "print(\"  - Streaming is viable: high throughput\")\n",
    "print(\"  - Memory approach: streaming line by line\")\n",
    "print(\"  - Time approach: load in memory with pandas/polars if sufficient RAM\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendimiento y estimaciones\n",
    "\n",
    "El dataset completo puede procesarse en **~6.8 segundos** usando lectura en streaming, con un throughput de **~17.3K tweets por segundo**. Esto confirma que el enfoque optimizado en memoria puede resolverse eficientemente en una sola pasada. Dado este rendimiento y la disponibilidad de RAM, el enfoque optimizado en tiempo puede cargar el dataset en memoria (por ejemplo con pandas o polars) para acelerar operaciones agregadas sin riesgo significativo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Casos Borde y Decisiones Técnicas\n",
    "\n",
    "### Decisiones basadas en evidencia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusiones y Decisiones Técnicas Finales\n",
    "\n",
    "### Resumen Ejecutivo\n",
    "\n",
    "El profiling exhaustivo del dataset reveló **117.407 tweets** (389 MB) con una calidad de datos superior a la esperada:\n",
    "\n",
    "1. **Dataset completamente válido**: 100% de líneas parseables, 0 errores de JSON\n",
    "2. **Campos críticos completos**: `date`, `content` y `user.username` presentes en el 100% de los registros\n",
    "3. **Esquema consistente**: 21 campos identificados con tipos de datos estables\n",
    "4. **Performance excelente**: 17.3K tweets/segundo en lectura streaming (6.8 segundos para procesamiento completo)\n",
    "\n",
    "### Hallazgos Clave por Pregunta\n",
    "\n",
    "#### Q1: Top 10 fechas con más tweets\n",
    "\n",
    "**Datos confirmados:**\n",
    "- **100%** de tweets tiene campo `date` en formato ISO 8601\n",
    "- **100%** de tweets tiene `user.username`\n",
    "- **0** registros requieren limpieza o normalización\n",
    "\n",
    "**Decisión técnica:**\n",
    "- Parsear con `datetime.fromisoformat()` sin validaciones adicionales\n",
    "- Extraer solo la fecha (ignorar hora) para agrupar por día\n",
    "- Contar usuarios únicos por fecha usando estructuras simples (Counter, defaultdict)\n",
    "\n",
    "#### Q2: Top 10 emojis más usados\n",
    "\n",
    "**Datos confirmados:**\n",
    "- **100%** de tweets tiene campo `content`\n",
    "- **14.4%** de tweets contiene al menos un emoji (16.874 tweets)\n",
    "- **42.922 emojis totales**, promedio de 2.5 por tweet con emojis\n",
    "\n",
    "**Decisión técnica:**\n",
    "- Aplicar librería `emoji` directamente sobre `content` sin preprocesamiento\n",
    "- Usar `emoji.emoji_list()` para manejar emojis compuestos correctamente\n",
    "- No extraer de campos alternativos (renderedContent), `content` es suficiente\n",
    "\n",
    "#### Q3: Top 10 usuarios más influyentes (por menciones recibidas)\n",
    "\n",
    "**Datos confirmados:**\n",
    "- **67.6%** de tweets NO tiene `mentionedUsers` (campo null)\n",
    "- **32.4%** de tweets SÍ tiene menciones (38.034 tweets)\n",
    "- Promedio de **2.7 menciones/tweet** cuando existen\n",
    "- Casos extremos: hasta **50 menciones** en un solo tweet\n",
    "\n",
    "**Decisión técnica:**\n",
    "- Tratar `mentionedUsers == null` como ausencia de menciones (no como error)\n",
    "- Iterar solo sobre tweets con menciones (saltar 67.6% del dataset)\n",
    "- Extraer `username` del objeto estructurado en la lista\n",
    "- No parsear texto, confiar en campo estructurado\n",
    "\n",
    "### Performance y Estrategia de Implementación\n",
    "\n",
    "**Enfoque optimizado en TIEMPO:**\n",
    "- Cargar dataset completo en memoria con **Polars**\n",
    "- Aprovechar operaciones vectorizadas y paralelización automática\n",
    "- Dataset cabe holgadamente en RAM moderna (389 MB)\n",
    "- Tiempo esperado: **< 2 segundos** por pregunta\n",
    "\n",
    "**Enfoque optimizado en MEMORIA:**\n",
    "- Streaming línea por línea con **orjson**\n",
    "- Estructuras minimalistas: `Counter`, `defaultdict`, `heapq`\n",
    "- Una sola pasada por el dataset, sin carga completa\n",
    "- Tiempo esperado: **6-8 segundos** por pregunta\n",
    "\n",
    "### Casos Borde Confirmados\n",
    "\n",
    "| Caso | Frecuencia | Estrategia |\n",
    "|------|-----------|-----------|\n",
    "| Tweets sin `date` | 0 (0%) | No requiere manejo |\n",
    "| Tweets sin `content` | 0 (0%) | No requiere manejo |\n",
    "| Tweets sin `user.username` | 0 (0%) | No requiere manejo |\n",
    "| Tweets sin `mentionedUsers` | 79.373 (67.6%) | Saltar en Q3 |\n",
    "| Líneas JSON inválidas | 0 (0%) | No requiere manejo |\n",
    "\n",
    "### Decisiones de Diseño Finales\n",
    "\n",
    "**✅ Haremos:**\n",
    "- Parseo directo sin validación defensiva excesiva (calidad confirmada)\n",
    "- Streaming para enfoque memory, carga completa para enfoque time\n",
    "- Uso de `mentionedUsers` estructurado (no parsing de texto)\n",
    "- Manejo explícito de `mentionedUsers == null` en Q3\n",
    "\n",
    "**❌ NO haremos:**\n",
    "- Limpieza o normalización de datos (no es necesaria)\n",
    "- Imputación de campos faltantes (no existen casos críticos)\n",
    "- Extracción de menciones desde texto (campo estructurado disponible)\n",
    "- Validación exhaustiva línea por línea (100% de datos válidos)\n",
    "\n",
    "### Próximos Pasos\n",
    "\n",
    "1. ✅ **Profiling completado** → Decisiones técnicas documentadas\n",
    "2. → **Implementar Q1**: Top fechas + usuarios por fecha\n",
    "3. → **Implementar Q2**: Top emojis\n",
    "4. → **Implementar Q3**: Top usuarios influyentes\n",
    "\n",
    "**Este profiling elimina incertidumbre técnica y permite implementación directa sin iteraciones exploratorias.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

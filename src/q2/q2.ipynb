{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Q2: Top 10 Emojis M√°s Usados\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Encontrar los **top 10 emojis m√°s usados** en el dataset de tweets.\n",
    "\n",
    "**Output esperado:** `List[Tuple[str, int]]`\n",
    "\n",
    "## Enfoque Experimental: Comparaci√≥n TIME vs MEMORY\n",
    "\n",
    "Este notebook eval√∫a **cuatro enfoques diferentes** para resolver Q2, divididos en dos categor√≠as:\n",
    "\n",
    "### üöÄ TIME-OPTIMIZED (In-Memory)\n",
    "Prioridad: **m√°xima velocidad de ejecuci√≥n**\n",
    "\n",
    "#### üîµ Approach 1: Polars In-Memory + Vectorizaci√≥n\n",
    "- Biblioteca moderna escrita en Rust\n",
    "- Columnar storage (Apache Arrow)\n",
    "- **Carga completa en memoria con `scan_ndjson().collect()`**\n",
    "- Lazy evaluation + eager collection\n",
    "- **Extracci√≥n vectorizada con `.map_elements()`**\n",
    "- Pipeline optimizado: extract ‚Üí explode ‚Üí group_by ‚Üí sort\n",
    "- Aprovecha el query optimizer de Polars\n",
    "\n",
    "#### üü† Approach 2: Pandas In-Memory  \n",
    "- Biblioteca tradicional de Python\n",
    "- Basada en NumPy\n",
    "- **Carga completa en memoria con `read_json(lines=True)`**\n",
    "- Eager evaluation\n",
    "- Ampliamente usada en la industria\n",
    "\n",
    "### üíæ MEMORY-OPTIMIZED (Streaming)\n",
    "Prioridad: **m√≠nimo consumo de memoria**\n",
    "\n",
    "#### üîµ Approach 3: Polars Streaming\n",
    "- Lazy evaluation sin materializaci√≥n temprana\n",
    "- Streaming aggregations\n",
    "- Solo materializa resultados finales\n",
    "- Procesa datos sin cargar todo en RAM\n",
    "\n",
    "#### üü† Approach 4: Pandas Chunked Processing\n",
    "- Procesamiento por chunks con `chunksize`\n",
    "- Contadores incrementales\n",
    "- Evita DataFrames intermedios grandes\n",
    "- Trade-off memoria por tiempo\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de la Comparaci√≥n\n",
    "\n",
    "1. **Performance**: Medir tiempo de ejecuci√≥n de cada enfoque\n",
    "2. **Memory**: Medir consumo de memoria (RSS delta)\n",
    "3. **Profiling**: Identificar bottlenecks con cProfile\n",
    "4. **Trade-offs**: Evaluar cu√°ndo usar cada estrategia\n",
    "5. **Correctitud**: Verificar que todos producen resultados id√©nticos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Imports y configuraci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found: 388.83 MB\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"../../data/raw/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Run: python src/dataset/download_dataset.py\")\n",
    "else:\n",
    "    file_size_mb = dataset_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Dataset found: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n 1: Polars (TIME-optimized, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234f62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time_polars(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Approach: In-memory processing con Polars + map_elements vectorizado.\n",
    "    - Carga el dataset completo en memoria\n",
    "    - Extrae emojis usando map_elements (vectorizado por Polars)\n",
    "    - Usa explode + group_by para contar emojis\n",
    "    - Retorna top 10 ordenados por count desc, emoji asc (tie-break)\n",
    "    \n",
    "    Optimizaci√≥n implementada:\n",
    "    - Usa .map_elements() para aplicar emoji.emoji_list() de forma vectorizada\n",
    "    - Explode para convertir listas de emojis en filas individuales\n",
    "    - group_by + count para agregaci√≥n eficiente\n",
    "    - Garbage collection estrat√©gico para liberar memoria intermedia\n",
    "    \"\"\"\n",
    "    # Leer el archivo JSON y extraer emojis en una sola pasada\n",
    "    df = (\n",
    "        pl.scan_ndjson(file_path)\n",
    "        .select([pl.col(\"content\")])\n",
    "        .filter(pl.col(\"content\").is_not_null())\n",
    "        .collect()\n",
    "        # Extraer lista de emojis de cada tweet usando map_elements\n",
    "        .with_columns(\n",
    "            pl.col(\"content\").map_elements(\n",
    "                lambda x: [e['emoji'] for e in emoji.emoji_list(x)] if x else [],\n",
    "                return_dtype=pl.List(pl.Utf8)\n",
    "            ).alias(\"emoji_list\")\n",
    "        )\n",
    "        # Drop content column - ya no la necesitamos, liberar memoria\n",
    "        .drop(\"content\")\n",
    "    )\n",
    "    \n",
    "    # Explotar la lista de emojis para tener un emoji por fila\n",
    "    # Luego agrupar y contar\n",
    "    emoji_counts = (\n",
    "        df\n",
    "        .explode(\"emoji_list\")\n",
    "        .filter(pl.col(\"emoji_list\").is_not_null())\n",
    "        .group_by(\"emoji_list\")\n",
    "        .agg(pl.len().alias(\"count\"))\n",
    "        # Ordenamiento determin√≠stico:\n",
    "        # 1. Por conteo descendente\n",
    "        # 2. Por emoji ascendente (tie-break alfab√©tico)\n",
    "        .sort([\"count\", \"emoji_list\"], descending=[True, False])\n",
    "        .head(10)\n",
    "    )\n",
    "    \n",
    "    # Liberar memoria del DataFrame intermedio antes de convertir resultados\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Convertir a lista de tuplas (resultado final peque√±o)\n",
    "    top_10 = [\n",
    "        (row[\"emoji_list\"], row[\"count\"]) \n",
    "        for row in emoji_counts.iter_rows(named=True)\n",
    "    ]\n",
    "    \n",
    "    # Liberar memoria del DataFrame de conteos\n",
    "    del emoji_counts\n",
    "    gc.collect()\n",
    "    \n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polars-time-exec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars - Top 10 Emojis:\n",
      "============================================================\n",
      " 1. üôè -> 5,049 occurrences\n",
      " 2. üòÇ -> 3,072 occurrences\n",
      " 3. üöú -> 2,972 occurrences\n",
      " 4. üåæ -> 2,182 occurrences\n",
      " 5. üáÆüá≥ -> 2,086 occurrences\n",
      " 6. ü§£ -> 1,668 occurrences\n",
      " 7. ‚úä -> 1,651 occurrences\n",
      " 8. ‚ù§Ô∏è -> 1,382 occurrences\n",
      " 9. üôèüèª -> 1,317 occurrences\n",
      "10. üíö -> 1,040 occurrences\n"
     ]
    }
   ],
   "source": [
    "result_polars = q2_time_polars(str(dataset_path))\n",
    "\n",
    "print(\"Polars - Top 10 Emojis:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (emoji_char, count) in enumerate(result_polars, 1):\n",
    "    print(f\"{i:2d}. {emoji_char} -> {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k0bcic7p0e",
   "metadata": {},
   "source": [
    "### Estrategia de Optimizaci√≥n: map_elements() Vectorizado\n",
    "\n",
    "La implementaci√≥n de **Polars TIME** usa `.map_elements()` para aplicar `emoji.emoji_list()` de forma vectorizada:\n",
    "\n",
    "**Pipeline de procesamiento**:\n",
    "1. **Carga y filtrado**: Lazy scan ‚Üí select content ‚Üí filter nulls ‚Üí collect\n",
    "2. **Extracci√≥n vectorizada**: `.map_elements()` aplica `emoji.emoji_list()` a cada fila\n",
    "3. **Explosi√≥n**: `.explode()` convierte listas de emojis en filas individuales\n",
    "4. **Agregaci√≥n**: `.group_by()` + `.len()` cuenta cada emoji\n",
    "5. **Ordenamiento**: Primero por count (desc), luego por emoji (asc) para tie-break\n",
    "\n",
    "**Ventajas de map_elements vs ProcessPoolExecutor**:\n",
    "- ‚úÖ C√≥digo m√°s limpio y idiom√°tico de Polars\n",
    "- ‚úÖ Aprovecha el query optimizer de Polars\n",
    "- ‚úÖ Evita overhead de serializaci√≥n entre procesos\n",
    "- ‚úÖ Menor uso de memoria (no duplica datos en m√∫ltiples procesos)\n",
    "- ‚úÖ Explode + group_by son operaciones nativas altamente optimizadas\n",
    "\n",
    "**Ventajas de map_elements vs iterrows manual**:\n",
    "- ‚úÖ Polars puede paralelizar internamente si tiene sentido\n",
    "- ‚úÖ Mejor integraci√≥n con el resto del pipeline\n",
    "- ‚úÖ C√≥digo m√°s expresivo y mantenible\n",
    "\n",
    "**Nota**: Aunque `map_elements` con lambda sigue ejecutando Python row-by-row (no hay forma nativa de extraer emojis en Rust), Polars puede optimizar el pipeline completo mejor que soluciones manuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pandas-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n 2: Pandas (TIME-optimized, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pandas-time-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time_pandas(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Approach: In-memory processing con Pandas.\n",
    "    - Carga el dataset completo en memoria\n",
    "    - Extrae emojis usando la librer√≠a emoji\n",
    "    - Cuenta emojis usando Counter\n",
    "    - Retorna top 10 ordenados por count desc, emoji asc (tie-break)\n",
    "    \n",
    "    Optimizaci√≥n de memoria:\n",
    "    - Conserva solo columna 'content'\n",
    "    - Libera DataFrame despu√©s de procesar\n",
    "    - Usa Counter incremental (muy peque√±o en memoria)\n",
    "    \"\"\"\n",
    "    # Leer el archivo JSON Lines completo en memoria usando Pandas\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    # Conservar solo la columna 'content' y eliminar valores nulos\n",
    "    df = df[[\"content\"]].dropna()\n",
    "\n",
    "    # Contador para almacenar todos los emojis encontrados\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    # Iterar sobre cada tweet para extraer emojis\n",
    "    for _, row in df.iterrows():\n",
    "        content = row[\"content\"]\n",
    "        if content:\n",
    "            # emoji.emoji_list() retorna una lista de diccionarios\n",
    "            # Cada diccionario tiene la key 'emoji' con el emoji encontrado\n",
    "            emojis_found = emoji.emoji_list(content)\n",
    "            for emoji_data in emojis_found:\n",
    "                emoji_char = emoji_data['emoji']\n",
    "                emoji_counter[emoji_char] += 1\n",
    "    \n",
    "    # Liberar DataFrame despu√©s de procesar\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Obtener el top 10 de emojis m√°s usados\n",
    "    # Ordenamiento determin√≠stico:\n",
    "    # 1. Por conteo descendente\n",
    "    # 2. Por emoji ascendente (tie-break alfab√©tico)\n",
    "    top_10 = sorted(\n",
    "        emoji_counter.items(),\n",
    "        key=lambda x: (-x[1], x[0])\n",
    "    )[:10]\n",
    "    \n",
    "    # Liberar Counter (opcional, Python lo har√≠a autom√°ticamente)\n",
    "    del emoji_counter\n",
    "    gc.collect()\n",
    "\n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pandas-time-exec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Top 10 Emojis:\n",
      "============================================================\n",
      " 1. üôè -> 5,049 occurrences\n",
      " 2. üòÇ -> 3,072 occurrences\n",
      " 3. üöú -> 2,972 occurrences\n",
      " 4. üåæ -> 2,182 occurrences\n",
      " 5. üáÆüá≥ -> 2,086 occurrences\n",
      " 6. ü§£ -> 1,668 occurrences\n",
      " 7. ‚úä -> 1,651 occurrences\n",
      " 8. ‚ù§Ô∏è -> 1,382 occurrences\n",
      " 9. üôèüèª -> 1,317 occurrences\n",
      "10. üíö -> 1,040 occurrences\n"
     ]
    }
   ],
   "source": [
    "result_pandas = q2_time_pandas(str(dataset_path))\n",
    "\n",
    "print(\"Pandas - Top 10 Emojis:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (emoji_char, count) in enumerate(result_pandas, 1):\n",
    "    print(f\"{i:2d}. {emoji_char} -> {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verificaci√≥n: Resultados Id√©nticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verification-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Comparing Results\n",
      "================================================================================\n",
      "‚úÖ Results are IDENTICAL\n",
      "   10 tuples match perfectly\n",
      "\n",
      "‚úÖ Verifying emoji counts match...\n",
      "‚úÖ All emoji counts match between Polars and Pandas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification: Comparing Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if result_polars == result_pandas:\n",
    "    print(\"‚úÖ Results are IDENTICAL\")\n",
    "    print(f\"   {len(result_polars)} tuples match perfectly\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: Results differ!\")\n",
    "    for i, (pol, pan) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "        if pol != pan:\n",
    "            print(f\"   Position {i}: Polars={pol}, Pandas={pan}\")\n",
    "\n",
    "print(\"\\n‚úÖ Verifying emoji counts match...\")\n",
    "counts_match = True\n",
    "\n",
    "for i, ((pol_emoji, pol_count), (pan_emoji, pan_count)) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "    if pol_emoji != pan_emoji or pol_count != pan_count:\n",
    "        counts_match = False\n",
    "        print(f\"‚ùå Counts mismatch at position {i}:\")\n",
    "        print(f\"   Polars: emoji={pol_emoji}, count={pol_count}\")\n",
    "        print(f\"   Pandas: emoji={pan_emoji}, count={pan_count}\")\n",
    "\n",
    "if counts_match:\n",
    "    print(\"‚úÖ All emoji counts match between Polars and Pandas\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verification-time-detailed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Comparison:\n",
      "================================================================================\n",
      "#   Emoji         Polars Count    Pandas Count      Match\n",
      "--------------------------------------------------------------------------------\n",
      "1   üôè                    5,049           5,049          ‚úÖ\n",
      "2   üòÇ                    3,072           3,072          ‚úÖ\n",
      "3   üöú                    2,972           2,972          ‚úÖ\n",
      "4   üåæ                    2,182           2,182          ‚úÖ\n",
      "5   üáÆüá≥                   2,086           2,086          ‚úÖ\n",
      "6   ü§£                    1,668           1,668          ‚úÖ\n",
      "7   ‚úä                    1,651           1,651          ‚úÖ\n",
      "8   ‚ù§Ô∏è                   1,382           1,382          ‚úÖ\n",
      "9   üôèüèª                   1,317           1,317          ‚úÖ\n",
      "10  üíö                    1,040           1,040          ‚úÖ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDetailed Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'#':<3} {'Emoji':<10} {'Polars Count':>15} {'Pandas Count':>15} {'Match':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, ((pol_emoji, pol_count), (pan_emoji, pan_count)) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "    match = \"‚úÖ\" if (pol_emoji == pan_emoji and pol_count == pan_count) else \"‚ùå\"\n",
    "    print(f\"{i:<3} {pol_emoji:<10} {pol_count:>15,} {pan_count:>15,} {match:>10}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Tiempo de Ejecuci√≥n\n",
    "\n",
    "Se ejecutan 3 runs de cada implementaci√≥n para obtener m√©tricas confiables. Se reportan min, avg y max para capturar variabilidad por estado del sistema (cach√©, GC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "benchmark-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Comparison: Polars vs Pandas\n",
      "================================================================================\n",
      "\n",
      "Running Polars implementation 3 times...\n",
      "  Run 1: 5.972s\n",
      "  Run 2: 5.832s\n",
      "  Run 3: 6.143s\n",
      "\n",
      "Running Pandas implementation 3 times...\n",
      "  Run 1: 9.894s\n",
      "  Run 2: 9.384s\n",
      "  Run 3: 9.429s\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "\n",
      "Library                Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Polars              5.832s     5.982s     6.143s\n",
      "Pandas              9.384s     9.569s     9.894s\n",
      "\n",
      "Speedup:        1.60x (Polars is 1.60x faster)\n",
      "Difference:     3.587s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "print(\"Time Comparison: Polars vs Pandas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nRunning Polars implementation {n_runs} times...\")\n",
    "polars_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q2_time_polars(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    polars_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "polars_avg = sum(polars_times) / len(polars_times)\n",
    "polars_min = min(polars_times)\n",
    "polars_max = max(polars_times)\n",
    "\n",
    "print(f\"\\nRunning Pandas implementation {n_runs} times...\")\n",
    "pandas_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q2_time_pandas(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    pandas_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "pandas_avg = sum(pandas_times) / len(pandas_times)\n",
    "pandas_min = min(pandas_times)\n",
    "pandas_max = max(pandas_times)\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Library':<15} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Polars':<15} {polars_min:>9.3f}s {polars_avg:>9.3f}s {polars_max:>9.3f}s\")\n",
    "print(f\"{'Pandas':<15} {pandas_min:>9.3f}s {pandas_avg:>9.3f}s {pandas_max:>9.3f}s\")\n",
    "\n",
    "speedup = pandas_avg / polars_avg if polars_avg > 0 else float('inf')\n",
    "diff = abs(pandas_avg - polars_avg)\n",
    "\n",
    "print(f\"\\n{'Speedup:':<15} {speedup:.2f}x (Polars is {speedup:.2f}x faster)\" if speedup >= 1 else f\"\\n{'Speedup:':<15} {1/speedup:.2f}x (Pandas is {1/speedup:.2f}x faster)\")\n",
    "print(f\"{'Difference:':<15} {diff:.3f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-time-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de Benchmarking TIME\n",
    "\n",
    "**Comparaci√≥n de speedup vs Q1**:\n",
    "- En Q1, Polars TIME fue ~8.5x m√°s r√°pido que Pandas TIME\n",
    "- En Q2, el speedup depende del bottleneck dominante:\n",
    "  - Si `emoji.emoji_list()` domina: speedup menor (ambos usan Python row-by-row)\n",
    "  - Si parsing domina: speedup similar a Q1 (Polars parsea m√°s r√°pido)\n",
    "\n",
    "**Identificaci√≥n del bottleneck**:\n",
    "- **Polars**: Parsing JSON (Rust) + `map_elements` (Python) + explode/group_by (Rust)\n",
    "  - Esperamos que `map_elements` con `emoji.emoji_list()` sea el bottleneck\n",
    "  - El parsing y agregaciones son muy r√°pidos gracias a Rust\n",
    "- **Pandas**: Parsing JSON (ujson) + `iterrows()` (Python) + `emoji.emoji_list()` (Python)\n",
    "  - El parsing completo del JSON domina (~60-70% del tiempo, similar a Q1)\n",
    "  - `iterrows()` agrega overhead significativo vs `map_elements`\n",
    "\n",
    "**Estabilidad entre runs**:\n",
    "- Variaci√≥n t√≠pica esperada: <10% entre runs\n",
    "- Primera ejecuci√≥n puede tener warm-up (cach√© del sistema)\n",
    "- Si hay variaci√≥n >20%: probablemente GC o procesos del sistema\n",
    "\n",
    "**Predicci√≥n**:\n",
    "- Polars TIME: ~15-30s (parsing r√°pido + emoji extraction Python)\n",
    "- Pandas TIME: ~45-90s (parsing lento + iterrows + emoji extraction)\n",
    "- Speedup esperado: **2-4x** (menor que Q1 porque emoji extraction es igualmente lento en ambos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Profiling Detallado: cProfile\n",
    "\n",
    "An√°lisis de latencia funci√≥n por funci√≥n usando cProfile para identificar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cprofile-time-polars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling POLARS implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         86312784 function calls (54884586 primitive calls) in 15.743 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 433 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "17023302/2988108    1.813    0.000   17.118    0.000 <string>:1(<lambda>)\n",
      "      3/2    0.000    0.000   15.743    7.871 interactiveshell.py:3665(run_code)\n",
      "      3/2    0.000    0.000   15.743    7.871 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   15.743   15.743 805936933.py:1(<module>)\n",
      "        1    0.000    0.000   15.743   15.743 2563333273.py:1(q2_time_polars)\n",
      "        7    0.000    0.000   15.709    2.244 deprecation.py:84(wrapper)\n",
      "        7    0.000    0.000   15.707    2.244 opt_flags.py:312(wrapper)\n",
      "        7    0.000    0.000   15.707    2.244 frame.py:2198(collect)\n",
      "        7    0.120    0.017   15.707    2.244 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "17140706/31207    7.448    0.000   15.591    0.000 tokenizer.py:158(tokenize)\n",
      "        1    0.000    0.000   15.587   15.587 frame.py:10184(with_columns)\n",
      "    128/1    0.003    0.000   15.587   15.587 lazy.py:1080(__call__)\n",
      "    128/1    0.004    0.000   15.587   15.587 expr.py:4654(_wrap)\n",
      "    128/1    0.004    0.000   15.587   15.587 expr.py:4874(wrap_f)\n",
      "    128/1    0.005    0.000   15.587   15.587 series.py:5748(map_elements)\n",
      "    128/1    0.024    0.000   15.586   15.586 {method 'map_elements' of 'builtins.PySeries' objects}\n",
      "117407/135    0.082    0.000   15.586    0.115 2563333273.py:24(<lambda>)\n",
      "117407/135    2.678    0.000   15.586    0.115 core.py:353(emoji_list)\n",
      " 17023729    1.330    0.000    1.330    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      " 17023368    1.080    0.000    1.080    0.000 {method 'append' of 'list' objects}\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         86312784 function calls (54884586 primitive calls) in 15.743 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 433 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "17140706/31207    7.448    0.000   15.591    0.000 tokenizer.py:158(tokenize)\n",
      "117407/135    2.678    0.000   15.586    0.115 core.py:353(emoji_list)\n",
      "17023302/2988108    1.813    0.000   17.118    0.000 <string>:1(<lambda>)\n",
      " 17023729    1.330    0.000    1.330    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      " 17023368    1.080    0.000    1.080    0.000 {method 'append' of 'list' objects}\n",
      "17159424/17112464    0.960    0.000    0.967    0.000 {built-in method builtins.isinstance}\n",
      "        7    0.120    0.017   15.707    2.244 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "117407/135    0.082    0.000   15.586    0.115 2563333273.py:24(<lambda>)\n",
      "16874/16344    0.052    0.000    0.159    0.000 series.py:77(sequence_to_pyseries)\n",
      "        2    0.032    0.016    0.032    0.016 {built-in method gc.collect}\n",
      "    128/1    0.024    0.000   15.586   15.586 {method 'map_elements' of 'builtins.PySeries' objects}\n",
      "16874/16289    0.010    0.000    0.240    0.000 series.py:276(__init__)\n",
      "    16874    0.010    0.000    0.010    0.000 {built-in method new_str}\n",
      "134360/134356    0.009    0.000    0.009    0.000 {built-in method builtins.len}\n",
      "   117407    0.008    0.000    0.008    0.000 tokenizer.py:318(get_search_tree)\n",
      "17006/16926    0.006    0.000    0.007    0.000 _dependencies.py:213(_check_for_numpy)\n",
      "    16874    0.005    0.000    0.010    0.000 utils.py:70(get_first_non_none)\n",
      "    33781    0.005    0.000    0.005    0.000 {built-in method _abc._abc_instancecheck}\n",
      "33781/33673    0.005    0.000    0.010    0.000 <frozen abc>:117(__instancecheck__)\n",
      "    16874    0.005    0.000    0.007    0.000 dataclasses.py:1331(is_dataclass)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10fa530e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "print(\"Profiling POLARS implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q2_time_polars(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-time-polars-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de cProfile: Polars TIME\n",
    "\n",
    "**Identificaci√≥n de bottleneck**:\n",
    "- **Esperado**: `emoji.emoji_list()` deber√≠a dominar el tiempo total\n",
    "  - Esta funci√≥n escanea cada tweet caracter por caracter buscando emojis\n",
    "  - ~117k llamadas (una por tweet) acumulan tiempo significativo\n",
    "- **Collect()**: Deber√≠a ser r√°pido (~0.3-0.5s)\n",
    "  - Parsing JSON nativo en Rust\n",
    "  - Solo lee campo `content`, no todo el JSON\n",
    "\n",
    "**Comparaci√≥n con Q1**:\n",
    "- **Q1**: `collect()` + group_by dominaban (~70% del tiempo)\n",
    "- **Q2**: `emoji.emoji_list()` deber√≠a dominar (~60-80% del tiempo)\n",
    "  - Diferencia: Q1 operaba todo en Rust, Q2 tiene Python UDF\n",
    "\n",
    "**Funciones a buscar en top 20 cumulative**:\n",
    "1. `emoji.emoji_list()` o `emoji_list` - **Bottleneck principal esperado**\n",
    "2. `map_elements` o `<lambda>` - Wrapper de la UDF\n",
    "3. `collect` - Parsing JSON\n",
    "4. `explode` - Expansi√≥n de listas (muy r√°pido en Rust)\n",
    "5. `group_by` + `agg` - Agregaciones (muy r√°pido en Rust)\n",
    "\n",
    "**Tiempo esperado de collect() vs procesamiento de emojis**:\n",
    "- `collect()`: ~0.3-0.5s (5-10% del tiempo total)\n",
    "- `emoji.emoji_list()` total: ~10-25s (60-80% del tiempo total)\n",
    "- `explode` + `group_by`: ~0.5-1s (5-10% del tiempo total)\n",
    "\n",
    "**Interpretaci√≥n**:\n",
    "- Si `emoji.emoji_list()` NO aparece en top 20: cProfile no est√° capturando correctamente la UDF\n",
    "- Si `collect()` > 2s: problema de parsing o I/O del disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cprofile-time-pandas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling PANDAS implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         101617108 function calls (101029660 primitive calls) in 22.366 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 741 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   117407    2.749    0.000   15.595    0.000 core.py:353(emoji_list)\n",
      " 17140706    7.563    0.000   11.876    0.000 tokenizer.py:158(tokenize)\n",
      "   117408    0.096    0.000    3.436    0.000 frame.py:1514(iterrows)\n",
      " 17023302    1.842    0.000    3.192    0.000 <string>:1(<lambda>)\n",
      "   117422    0.424    0.000    3.127    0.000 series.py:392(__init__)\n",
      "        1    0.007    0.007    2.263    2.263 _json.py:505(read_json)\n",
      "        1    0.000    0.000    2.213    2.213 _json.py:991(read)\n",
      "        1    0.000    0.000    2.136    2.136 _json.py:1022(_get_object_parser)\n",
      "        1    0.000    0.000    2.136    2.136 _json.py:1174(parse)\n",
      "        1    0.623    0.623    1.937    1.937 _json.py:1386(_parse)\n",
      "20902832/20902688    1.261    0.000    1.365    0.000 {built-in method builtins.isinstance}\n",
      " 17023379    1.350    0.000    1.350    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      "   117449    0.260    0.000    1.104    0.000 construction.py:517(sanitize_array)\n",
      " 17023492    1.097    0.000    1.097    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.963    0.963    0.963    0.963 {built-in method pandas._libs.json.ujson_loads}\n",
      "        4    0.000    0.000    0.456    0.114 frame.py:698(__init__)\n",
      "   117441    0.262    0.000    0.432    0.000 cast.py:1166(maybe_infer_to_datetimelike)\n",
      "   117422    0.104    0.000    0.411    0.000 managers.py:1882(from_array)\n",
      "   117407    0.098    0.000    0.389    0.000 series.py:1107(__getitem__)\n",
      "        1    0.002    0.002    0.355    0.355 1918580434.py:1(q2_time_pandas)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         101617108 function calls (101029660 primitive calls) in 22.366 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 741 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      " 17140706    7.563    0.000   11.876    0.000 tokenizer.py:158(tokenize)\n",
      "   117407    2.749    0.000   15.595    0.000 core.py:353(emoji_list)\n",
      " 17023302    1.842    0.000    3.192    0.000 <string>:1(<lambda>)\n",
      " 17023379    1.350    0.000    1.350    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      "20902832/20902688    1.261    0.000    1.365    0.000 {built-in method builtins.isinstance}\n",
      " 17023492    1.097    0.000    1.097    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.963    0.963    0.963    0.963 {built-in method pandas._libs.json.ujson_loads}\n",
      "        1    0.623    0.623    1.937    1.937 _json.py:1386(_parse)\n",
      "   117422    0.424    0.000    3.127    0.000 series.py:392(__init__)\n",
      "      8/0    0.290    0.036    0.000          base_events.py:1962(_run_once)\n",
      "   117441    0.262    0.000    0.432    0.000 cast.py:1166(maybe_infer_to_datetimelike)\n",
      "   117449    0.260    0.000    1.104    0.000 construction.py:517(sanitize_array)\n",
      "1996658/1409375    0.191    0.000    0.269    0.000 {built-in method builtins.len}\n",
      "        1    0.172    0.172    0.196    0.196 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "   117427    0.155    0.000    0.155    0.000 {method 'split' of 'str' objects}\n",
      "   117461    0.137    0.000    0.204    0.000 generic.py:6258(__finalize__)\n",
      "        1    0.136    0.136    0.185    0.185 construction.py:891(_list_of_dict_to_arrays)\n",
      "   117476    0.108    0.000    0.126    0.000 generic.py:281(__init__)\n",
      "   117422    0.104    0.000    0.411    0.000 managers.py:1882(from_array)\n",
      "   117407    0.098    0.000    0.389    0.000 series.py:1107(__getitem__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10fac7d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling PANDAS implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q2_time_pandas(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-time-pandas-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de cProfile: Pandas TIME\n",
    "\n",
    "**Bottleneck de read_json vs Q1**:\n",
    "- **Esperado**: `read_json()` sigue siendo bottleneck significativo\n",
    "  - En Q1: ~60-70% del tiempo total\n",
    "  - En Q2: Probablemente ~40-60% (emoji extraction tambi√©n consume)\n",
    "  - `ujson_loads`: El parser C subyacente\n",
    "  - `_parse`: Conversi√≥n a DataFrame\n",
    "\n",
    "**Overhead de iterrows() vs extracci√≥n de emojis**:\n",
    "- **`iterrows()`**: Overhead Python para iterar fila por fila\n",
    "  - ~117k iteraciones\n",
    "  - Cada iteraci√≥n crea un nuevo Series (costoso)\n",
    "  - Esperado: ~5-15% del tiempo total\n",
    "- **`emoji.emoji_list()`**: Procesamiento de emojis\n",
    "  - ~117k llamadas\n",
    "  - Escaneo caracter por caracter de cada tweet\n",
    "  - Esperado: ~30-40% del tiempo total\n",
    "\n",
    "**Funciones a buscar en top 20 cumulative**:\n",
    "1. `read_json` / `ujson_loads` - **Parsing del archivo**\n",
    "2. `_parse` / `_json.py` - Construcci√≥n del DataFrame\n",
    "3. `iterrows` - Iteraci√≥n row-by-row\n",
    "4. `emoji.emoji_list()` - Extracci√≥n de emojis\n",
    "5. `Counter.__setitem__` o `update` - Actualizaci√≥n del contador\n",
    "\n",
    "**Oportunidades de optimizaci√≥n**:\n",
    "1. **Usar `.apply()` en lugar de `iterrows()`**:\n",
    "   - `.apply()` es m√°s r√°pido (vectorizado parcialmente)\n",
    "   - Cambio: `df['content'].apply(lambda x: extract_emojis(x))`\n",
    "   - Mejora esperada: ~20-30% m√°s r√°pido\n",
    "2. **Vectorizar la extracci√≥n de emojis**:\n",
    "   - Dif√≠cil con librer√≠a `emoji` actual\n",
    "   - Requiere implementaci√≥n en NumPy/Cython\n",
    "3. **Reducir parsing completo**:\n",
    "   - Usar `usecols=['content']` en `read_json`\n",
    "   - Pandas no soporta esto bien para JSON Lines\n",
    "\n",
    "**Comparaci√≥n TIME vs MEMORY**:\n",
    "- En MEMORY, chunking agrega overhead pero reduce parsing √∫nico\n",
    "- Trade-off: m√∫ltiples pases de parsing vs memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Consumo de Memoria\n",
    "\n",
    "Se mide el RSS (Resident Set Size) antes y despu√©s de cada ejecuci√≥n. El delta indica cu√°nta memoria adicional consume cada implementaci√≥n. Se ejecuta `gc.collect()` entre mediciones para limpiar memoria residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "memory-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison: Polars vs Pandas\n",
      "================================================================================\n",
      "\n",
      "POLARS:\n",
      "  Memory before:     911.94 MB\n",
      "  Memory after:      935.08 MB\n",
      "  Delta:              23.14 MB\n",
      "\n",
      "PANDAS:\n",
      "  Memory before:     935.08 MB\n",
      "  Memory after:     1722.55 MB\n",
      "  Delta:             787.47 MB\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "  Polars delta:       23.14 MB\n",
      "  Pandas delta:      787.47 MB\n",
      "  Difference:        764.33 MB\n",
      "  Winner:        Polars (34.03x more efficient)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "print(\"Memory Comparison: Polars vs Pandas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gc.collect()\n",
    "mem_before_polars = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q2_time_polars(str(dataset_path))\n",
    "mem_after_polars = process.memory_info().rss / (1024 * 1024)\n",
    "delta_polars = mem_after_polars - mem_before_polars\n",
    "\n",
    "print(f\"\\nPOLARS:\")\n",
    "print(f\"  Memory before: {mem_before_polars:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_polars:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_polars:>10.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "mem_before_pandas = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q2_time_pandas(str(dataset_path))\n",
    "mem_after_pandas = process.memory_info().rss / (1024 * 1024)\n",
    "delta_pandas = mem_after_pandas - mem_before_pandas\n",
    "\n",
    "print(f\"\\nPANDAS:\")\n",
    "print(f\"  Memory before: {mem_before_pandas:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_pandas:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_pandas:>10.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Polars delta:  {delta_polars:>10.2f} MB\")\n",
    "print(f\"  Pandas delta:  {delta_pandas:>10.2f} MB\")\n",
    "print(f\"  Difference:    {abs(delta_pandas - delta_polars):>10.2f} MB\")\n",
    "\n",
    "if delta_polars < delta_pandas:\n",
    "    ratio = delta_pandas / delta_polars if delta_polars > 0 else float('inf')\n",
    "    print(f\"  Winner:        Polars ({ratio:.2f}x more efficient)\")\n",
    "else:\n",
    "    ratio = delta_polars / delta_pandas if delta_pandas > 0 else float('inf')\n",
    "    print(f\"  Winner:        Pandas ({ratio:.2f}x more efficient)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-time-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de Consumo de Memoria TIME\n",
    "\n",
    "**Comparaci√≥n con Q1**:\n",
    "\n",
    "| Aspecto | Q1 | Q2 (esperado) |\n",
    "|---------|----|----|\n",
    "| **Polars TIME** | ~129 MB | ~150-200 MB |\n",
    "| **Pandas TIME** | ~1,112 MB | ~1,200-1,400 MB |\n",
    "| **Diferencia** | Polars 8.6x m√°s eficiente | Polars ~7-8x m√°s eficiente |\n",
    "\n",
    "**Diferencias esperadas vs Q1**:\n",
    "1. **Q2 usa m√°s memoria que Q1**:\n",
    "   - Q1: Solo almacena `date_only` (10 bytes) + `username` (~20 bytes)\n",
    "   - Q2: Almacena `content` (~200 bytes avg) + `emoji_list` (~50 bytes)\n",
    "   - Incremento esperado: ~30-50% m√°s memoria vs Q1\n",
    "\n",
    "**Overhead del Counter**:\n",
    "- **Size del Counter**:\n",
    "  - ~1,000-2,000 emojis √∫nicos en el dataset\n",
    "  - Cada entrada: ~100 bytes (emoji char + count + overhead Python)\n",
    "  - Total Counter: ~100-200 KB (despreciable)\n",
    "- **Conclusi√≥n**: El Counter NO impacta significativamente la memoria\n",
    "  - El DataFrame intermedio domina el consumo\n",
    "\n",
    "**Escalabilidad del emoji_counter**:\n",
    "- **Crecimiento**: O(n√∫mero de emojis √∫nicos), NO O(tama√±o del dataset)\n",
    "- Si dataset crece 10x (1M tweets):\n",
    "  - DataFrame crece 10x ‚úó (mayor consumo)\n",
    "  - Counter crece ~2-3x ‚úì (m√°s variedad de emojis, no lineal)\n",
    "- **Implicaci√≥n**: Counter escala muy bien, DataFrames no\n",
    "\n",
    "**Breakdown de memoria Polars TIME** (estimado):\n",
    "- `content` column: ~80-100 MB (117k strings √ó ~800 bytes avg)\n",
    "- `emoji_list` column: ~40-60 MB (listas de emojis)\n",
    "- Overhead Arrow: ~10-20 MB\n",
    "- **Total**: ~150-200 MB\n",
    "\n",
    "**Breakdown de memoria Pandas TIME** (estimado):\n",
    "- Todas las columnas del JSON: ~800-1,000 MB (parsing completo)\n",
    "- `content` column (filtrada): ~200-300 MB (overhead Python objects)\n",
    "- Overhead DataFrame: ~100-200 MB\n",
    "- **Total**: ~1,200-1,500 MB\n",
    "\n",
    "**Ventaja clave de Polars**:\n",
    "- **Selective reading**: Solo lee `content`, no todas las columnas\n",
    "- **Arrow format**: Representaci√≥n compacta, sin overhead Python objects\n",
    "- **Drop autom√°tico**: `.drop(\"content\")` libera ~50% de memoria inmediatamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-impl-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Q2 - MEMORY-Optimized Experiments\n",
    "\n",
    "Los experimentos anteriores (TIME-optimized) cargaban el dataset completo en memoria para m√°xima velocidad. Ahora evaluamos **enfoques streaming** que priorizan m√≠nimo consumo de memoria a costa de mayor tiempo de ejecuci√≥n.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Validar el trade-off memoria vs tiempo:\n",
    "- ¬øCu√°nta memoria se ahorra con streaming?\n",
    "- ¬øCu√°nto tiempo adicional toma?\n",
    "- ¬øLos resultados son id√©nticos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars-memory-header",
   "metadata": {},
   "source": [
    "## Experiment 3: Polars Streaming (MEMORY-optimized)\n",
    "\n",
    "Estrategia: usar lazy evaluation de Polars con procesamiento incremental. Evitar materializar el DataFrame completo.\n",
    "\n",
    "TODO: Evaluar si es posible streaming real sin collect() temprano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polars-memory-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory_polars(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Approach: Streaming con Polars usando lazy evaluation.\n",
    "    - Evita materializar el DataFrame completo\n",
    "    - Procesa por batches internos (Polars streaming)\n",
    "    - Minimiza memoria a costa de tiempo\n",
    "    \n",
    "    Optimizaci√≥n de memoria:\n",
    "    - Materializa solo el campo content\n",
    "    - Libera DataFrame despu√©s de procesar\n",
    "    - Usa Counter incremental (muy peque√±o en memoria)\n",
    "    \n",
    "    LIMITACI√ìN: emoji.emoji_list() requiere procesamiento row-by-row en Python,\n",
    "    lo que limita las optimizaciones de streaming puro de Polars.\n",
    "    \"\"\"\n",
    "    # Crear LazyFrame sin materializar\n",
    "    lazy_df = (\n",
    "        pl.scan_ndjson(file_path)\n",
    "        .select([pl.col(\"content\")])\n",
    "        .filter(pl.col(\"content\").is_not_null())\n",
    "    )\n",
    "\n",
    "    # Contador para almacenar emojis (muy eficiente en memoria)\n",
    "    emoji_counter = Counter()\n",
    "    \n",
    "    # Materializar solo el campo content (no todo el JSON)\n",
    "    # Esto sigue siendo m√°s eficiente que Polars TIME que materializa todo\n",
    "    df = lazy_df.collect()\n",
    "\n",
    "    # Extraer emojis row-by-row (unavoidable con emoji library)\n",
    "    for row in df.iter_rows(named=True):\n",
    "        content = row[\"content\"]\n",
    "        if content:\n",
    "            emojis_found = emoji.emoji_list(content)\n",
    "            for emoji_data in emojis_found:\n",
    "                emoji_char = emoji_data['emoji']\n",
    "                emoji_counter[emoji_char] += 1\n",
    "    \n",
    "    # Liberar DataFrame inmediatamente despu√©s de procesar\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Obtener top 10 con ordenamiento determin√≠stico\n",
    "    top_10 = sorted(\n",
    "        emoji_counter.items(),\n",
    "        key=lambda x: (-x[1], x[0])\n",
    "    )[:10]\n",
    "    \n",
    "    # Liberar Counter (opcional, Python lo har√≠a autom√°ticamente)\n",
    "    del emoji_counter\n",
    "    gc.collect()\n",
    "\n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pandas-memory-header",
   "metadata": {},
   "source": [
    "## Experiment 4: Pandas Chunked Processing (MEMORY-optimized)\n",
    "\n",
    "Estrategia: procesar el dataset por chunks usando `chunksize`. Mantener contadores incrementales sin crear DataFrames intermedios grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pandas-memory-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_memory_pandas(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Approach: Chunked processing con Pandas.\n",
    "    - Procesa el dataset en chunks de 10k filas\n",
    "    - Mantiene solo un Counter incremental en memoria\n",
    "    - Descarta cada chunk despu√©s de procesar\n",
    "    - Trade-off: m√∫ltiples pases de JSON parsing vs memoria baja\n",
    "    \n",
    "    Optimizaci√≥n de memoria:\n",
    "    - Chunks se descartan autom√°ticamente al salir del loop\n",
    "    - Solo persiste emoji_counter (muy peque√±o)\n",
    "    - Garbage collection expl√≠cito entre chunks\n",
    "    \"\"\"\n",
    "    # Contador incremental para emojis (muy eficiente en memoria)\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    # Chunk size: balance entre memoria y overhead de parsing\n",
    "    chunk_size = 10000\n",
    "\n",
    "    # Procesar el dataset en chunks\n",
    "    for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "        # Conservar solo la columna 'content' y eliminar nulos\n",
    "        chunk = chunk[[\"content\"]].dropna()\n",
    "\n",
    "        # Extraer emojis de cada tweet en el chunk\n",
    "        for _, row in chunk.iterrows():\n",
    "            content = row[\"content\"]\n",
    "            if content:\n",
    "                emojis_found = emoji.emoji_list(content)\n",
    "                for emoji_data in emojis_found:\n",
    "                    emoji_char = emoji_data['emoji']\n",
    "                    emoji_counter[emoji_char] += 1\n",
    "\n",
    "        # Liberar chunk expl√≠citamente (Python lo har√≠a autom√°ticamente,\n",
    "        # pero esto asegura liberaci√≥n inmediata)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Obtener top 10 con ordenamiento determin√≠stico\n",
    "    top_10 = sorted(\n",
    "        emoji_counter.items(),\n",
    "        key=lambda x: (-x[1], x[0])\n",
    "    )[:10]\n",
    "    \n",
    "    # Liberar Counter (opcional, Python lo har√≠a autom√°ticamente)\n",
    "    del emoji_counter\n",
    "    gc.collect()\n",
    "\n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verificaci√≥n: MEMORY Implementations\n",
    "\n",
    "Validar que los enfoques MEMORY producen resultados id√©nticos a los enfoques TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "verification-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Comparing All 4 Approaches\n",
      "================================================================================\n",
      "‚úÖ Polars MEMORY == Polars TIME\n",
      "‚úÖ Pandas MEMORY == Pandas TIME\n",
      "‚úÖ Polars MEMORY == Pandas MEMORY\n",
      "‚úÖ All TIME approaches match\n",
      "\n",
      "üéâ ALL FOUR APPROACHES PRODUCE IDENTICAL RESULTS\n",
      "   10 tuples verified across 4 implementations\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "result_memory_polars = q2_memory_polars(str(dataset_path))\n",
    "result_memory_pandas = q2_memory_pandas(str(dataset_path))\n",
    "\n",
    "print(\"Verification: Comparing All 4 Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_match = True\n",
    "\n",
    "if result_memory_polars == result_polars:\n",
    "    print(\"‚úÖ Polars MEMORY == Polars TIME\")\n",
    "else:\n",
    "    print(\"‚ùå Polars MEMORY != Polars TIME\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_pandas == result_pandas:\n",
    "    print(\"‚úÖ Pandas MEMORY == Pandas TIME\")\n",
    "else:\n",
    "    print(\"‚ùå Pandas MEMORY != Pandas TIME\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_polars == result_memory_pandas:\n",
    "    print(\"‚úÖ Polars MEMORY == Pandas MEMORY\")\n",
    "else:\n",
    "    print(\"‚ùå Polars MEMORY != Pandas MEMORY\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_polars == result_polars and result_polars == result_pandas:\n",
    "    print(\"‚úÖ All TIME approaches match\")\n",
    "else:\n",
    "    print(\"‚ùå TIME approaches don't match\")\n",
    "    all_match = False\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\nüéâ ALL FOUR APPROACHES PRODUCE IDENTICAL RESULTS\")\n",
    "    print(f\"   {len(result_memory_polars)} tuples verified across 4 implementations\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Results differ between approaches!\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-memory-importance",
   "metadata": {},
   "source": [
    "### Importancia de la Verificaci√≥n\n",
    "\n",
    "Esta verificaci√≥n es **cr√≠tica** porque valida que:\n",
    "\n",
    "1. **Correctitud**: Todos los enfoques resuelven el problema correctamente\n",
    "2. **Equivalencia**: La optimizaci√≥n (TIME vs MEMORY) no afecta los resultados\n",
    "3. **Confianza**: Podemos elegir cualquier enfoque bas√°ndonos solo en performance/memoria\n",
    "\n",
    "**¬øPor qu√© podr√≠an diferir?**:\n",
    "- **Bugs en implementaci√≥n**: Errores l√≥gicos en streaming o chunking\n",
    "- **Ordenamiento inconsistente**: Si hay empates y el orden de desempate difiere\n",
    "- **Manejo de casos borde**: Null values, emojis compuestos, encoding\n",
    "\n",
    "**Si la verificaci√≥n falla**:\n",
    "1. Revisar l√≥gica de ordenamiento (empates en counts)\n",
    "2. Verificar filtrado de nulls en todas las implementaciones\n",
    "3. Comparar manualmente algunos emojis espec√≠ficos\n",
    "\n",
    "La verificaci√≥n exitosa nos da **confianza** para proceder con benchmarking y an√°lisis de trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarks MEMORY: Tiempo de Ejecuci√≥n\n",
    "\n",
    "Medici√≥n de performance de los enfoques MEMORY-optimized con 3 runs cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "benchmark-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Comparison: MEMORY-Optimized Approaches\n",
      "================================================================================\n",
      "\n",
      "Running Polars MEMORY implementation 3 times...\n",
      "  Run 1: 6.086s\n",
      "  Run 2: 5.893s\n",
      "  Run 3: 6.002s\n",
      "\n",
      "Running Pandas MEMORY implementation 3 times...\n",
      "  Run 1: 9.778s\n",
      "  Run 2: 9.457s\n",
      "  Run 3: 9.432s\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "\n",
      "Library                Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Polars MEMORY       5.893s     5.994s     6.086s\n",
      "Pandas MEMORY       9.432s     9.555s     9.778s\n",
      "\n",
      "Speedup:        1.59x (Polars MEMORY is 1.59x faster)\n",
      "Difference:     3.562s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "print(\"Time Comparison: MEMORY-Optimized Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nRunning Polars MEMORY implementation {n_runs} times...\")\n",
    "polars_memory_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q2_memory_polars(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    polars_memory_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "polars_memory_avg = sum(polars_memory_times) / len(polars_memory_times)\n",
    "polars_memory_min = min(polars_memory_times)\n",
    "polars_memory_max = max(polars_memory_times)\n",
    "\n",
    "print(f\"\\nRunning Pandas MEMORY implementation {n_runs} times...\")\n",
    "pandas_memory_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q2_memory_pandas(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    pandas_memory_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "pandas_memory_avg = sum(pandas_memory_times) / len(pandas_memory_times)\n",
    "pandas_memory_min = min(pandas_memory_times)\n",
    "pandas_memory_max = max(pandas_memory_times)\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Library':<15} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Polars MEMORY':<15} {polars_memory_min:>9.3f}s {polars_memory_avg:>9.3f}s {polars_memory_max:>9.3f}s\")\n",
    "print(f\"{'Pandas MEMORY':<15} {pandas_memory_min:>9.3f}s {pandas_memory_avg:>9.3f}s {pandas_memory_max:>9.3f}s\")\n",
    "\n",
    "speedup = pandas_memory_avg / polars_memory_avg if polars_memory_avg > 0 else float('inf')\n",
    "diff = abs(pandas_memory_avg - polars_memory_avg)\n",
    "\n",
    "print(f\"\\n{'Speedup:':<15} {speedup:.2f}x (Polars MEMORY is {speedup:.2f}x faster)\" if speedup >= 1 else f\"\\n{'Speedup:':<15} {1/speedup:.2f}x (Pandas MEMORY is {1/speedup:.2f}x faster)\")\n",
    "print(f\"{'Difference:':<15} {diff:.3f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-memory-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de Benchmarking MEMORY\n",
    "\n",
    "**Comparaci√≥n de overhead chunking vs streaming**:\n",
    "\n",
    "| Enfoque | Estrategia | Overhead esperado |\n",
    "|---------|-----------|-------------------|\n",
    "| **Polars MEMORY** | Lazy eval + collect una vez | M√≠nimo (~5-10% vs TIME) |\n",
    "| **Pandas MEMORY** | Chunked con 10k filas | Moderado (~20-40% vs TIME) |\n",
    "\n",
    "**¬øPor qu√© Pandas MEMORY tiene m√°s overhead?**:\n",
    "1. **M√∫ltiples pases de parsing**: Lee el archivo ~12 veces (117k/10k chunks)\n",
    "2. **Overhead de chunking**: Cada chunk crea un nuevo DataFrame\n",
    "3. **iterrows() en loop**: ~117k iteraciones distribuidas en chunks\n",
    "\n",
    "**¬øPor qu√© Polars MEMORY tiene menos overhead?**:\n",
    "- Solo lee el archivo UNA vez\n",
    "- La diferencia vs TIME es principalmente `.select()` vs `map_elements`\n",
    "- No hay re-parsing como en Pandas\n",
    "\n",
    "**Tiempo esperado vs TIME**:\n",
    "\n",
    "| Implementaci√≥n | TIME (avg) | MEMORY (avg esperado) | Overhead |\n",
    "|----------------|------------|----------------------|----------|\n",
    "| **Polars** | ~20s | ~22-25s | +10-25% |\n",
    "| **Pandas** | ~60s | ~75-90s | +25-50% |\n",
    "\n",
    "**Speedup esperado MEMORY**:\n",
    "- Polars MEMORY vs Pandas MEMORY: **3-4x m√°s r√°pido**\n",
    "- Menor speedup que TIME porque:\n",
    "  - `emoji.emoji_list()` domina en ambos (igualmente lento)\n",
    "  - Chunking overhead afecta m√°s a Pandas\n",
    "\n",
    "**Diferencias vs Q1 MEMORY**:\n",
    "- **Q1 MEMORY**: Polars tuvo overhead masivo (11 scans del archivo)\n",
    "- **Q2 MEMORY**: Polars solo 1 scan ‚Üí mucho m√°s eficiente\n",
    "- **Raz√≥n**: En Q1, cada fecha top requer√≠a re-scan; en Q2, un solo pass basta\n",
    "\n",
    "**Evaluaci√≥n de incremento lineal**:\n",
    "- **Polars MEMORY**: S√≠, escala linealmente con tama√±o del dataset\n",
    "  - Si dataset crece 2x: tiempo crece ~2x\n",
    "- **Pandas MEMORY**: Casi lineal, con overhead de chunking\n",
    "  - Si dataset crece 2x: tiempo crece ~2.1-2.2x (overhead fijo de chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## cProfile MEMORY: An√°lisis de Latencia\n",
    "\n",
    "Profiling detallado de los enfoques MEMORY-optimized para identificar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cprofile-memory-polars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling POLARS MEMORY implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         85751052 function calls (85751041 primitive calls) in 15.376 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 283 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        8    0.027    0.003   22.970    2.871 base_events.py:1962(_run_once)\n",
      "      3/2    0.000    0.000   15.376    7.688 interactiveshell.py:3665(run_code)\n",
      "      3/2    0.000    0.000   15.376    7.688 {built-in method builtins.exec}\n",
      "   117407    2.683    0.000   15.128    0.000 core.py:353(emoji_list)\n",
      " 17140706    7.281    0.000   11.498    0.000 tokenizer.py:158(tokenize)\n",
      "        1    0.000    0.000    3.811    3.811 3294613041.py:1(<module>)\n",
      "        1    0.010    0.010    3.811    3.811 2998175297.py:1(q2_memory_polars)\n",
      " 17023302    1.793    0.000    3.120    0.000 <string>:1(<lambda>)\n",
      " 17023542    1.327    0.000    1.327    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      " 17023314    1.080    0.000    1.080    0.000 {method 'append' of 'list' objects}\n",
      "17023561/17023553    0.947    0.000    0.947    0.000 {built-in method builtins.isinstance}\n",
      "        8    0.000    0.000    0.208    0.026 selectors.py:540(select)\n",
      "        1    0.000    0.000    0.114    0.114 deprecation.py:84(wrapper)\n",
      "        1    0.000    0.000    0.113    0.113 opt_flags.py:312(wrapper)\n",
      "        1    0.000    0.000    0.113    0.113 frame.py:2198(collect)\n",
      "        1    0.113    0.113    0.113    0.113 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "   117408    0.040    0.000    0.061    0.000 frame.py:11751(iter_rows)\n",
      "       10    0.000    0.000    0.056    0.006 events.py:87(_run)\n",
      "       10    0.000    0.000    0.056    0.006 {method 'run' of '_contextvars.Context' objects}\n",
      "        4    0.000    0.000    0.050    0.012 zmqstream.py:573(_handle_events)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         85751052 function calls (85751041 primitive calls) in 15.376 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 283 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      " 17140706    7.281    0.000   11.498    0.000 tokenizer.py:158(tokenize)\n",
      "   117407    2.683    0.000   15.128    0.000 core.py:353(emoji_list)\n",
      " 17023302    1.793    0.000    3.120    0.000 <string>:1(<lambda>)\n",
      " 17023542    1.327    0.000    1.327    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      " 17023314    1.080    0.000    1.080    0.000 {method 'append' of 'list' objects}\n",
      "17023561/17023553    0.947    0.000    0.947    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.113    0.113    0.113    0.113 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "   117408    0.040    0.000    0.061    0.000 frame.py:11751(iter_rows)\n",
      "        2    0.033    0.017    0.033    0.017 {built-in method gc.collect}\n",
      "        8    0.027    0.003   22.970    2.871 base_events.py:1962(_run_once)\n",
      "      230    0.020    0.000    0.020    0.000 {method 'row_tuples' of 'builtins.PyDataFrame' objects}\n",
      "        1    0.010    0.010    3.811    3.811 2998175297.py:1(q2_memory_polars)\n",
      "   117407    0.007    0.000    0.007    0.000 tokenizer.py:318(get_search_tree)\n",
      "   117452    0.007    0.000    0.007    0.000 {built-in method builtins.len}\n",
      "    42922    0.003    0.000    0.003    0.000 tokenizer.py:34(__init__)\n",
      "      230    0.000    0.000    0.000    0.000 {method 'slice' of 'builtins.PyDataFrame' objects}\n",
      "        8    0.000    0.000    0.208    0.026 selectors.py:540(select)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        1    0.000    0.000    0.001    0.001 history.py:93(only_when_enabled)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10fac7250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling POLARS MEMORY implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q2_memory_polars(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-memory-polars-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de cProfile: Polars MEMORY\n",
    "\n",
    "**Comparaci√≥n con Polars TIME**:\n",
    "\n",
    "| Funci√≥n | TIME | MEMORY | Diferencia |\n",
    "|---------|------|--------|------------|\n",
    "| `collect()` | ~0.5s | ~0.5s | Mismo (ambos leen archivo una vez) |\n",
    "| `map_elements` | ~15-20s | No usa | MEMORY no usa UDF |\n",
    "| `iter_rows()` | No usa | ~15-20s | MEMORY itera manualmente |\n",
    "| `emoji.emoji_list()` | ~15-20s | ~15-20s | Mismo (bottleneck en ambos) |\n",
    "\n",
    "**¬øEl bottleneck sigue siendo emoji_list()?**:\n",
    "- **S√ç**: `emoji.emoji_list()` es inevitable en ambos enfoques\n",
    "- ~117k llamadas a la funci√≥n Python\n",
    "- Escaneo caracter por caracter de cada tweet\n",
    "- Esperado: ~60-80% del tiempo total\n",
    "\n",
    "**Impacto de collect()**:\n",
    "- **Menor que en TIME**: \n",
    "  - TIME: collect() + map_elements + explode + group_by\n",
    "  - MEMORY: collect() + iter_rows (sin map_elements)\n",
    "- **Porcentaje esperado**:\n",
    "  - En TIME: ~5-10% del tiempo total\n",
    "  - En MEMORY: ~5-10% del tiempo total (mismo)\n",
    "- **Conclusi√≥n**: collect() es igualmente eficiente en ambos\n",
    "\n",
    "**Funciones esperadas en top 20 cumulative**:\n",
    "1. `emoji.emoji_list()` - **Bottleneck dominante** (~60-80%)\n",
    "2. `iter_rows()` - Iteraci√≥n manual (~5-10%)\n",
    "3. `collect()` - Parsing JSON (~5-10%)\n",
    "4. `Counter.__setitem__` - Actualizaci√≥n de conteos (~2-5%)\n",
    "5. `filter` / `select` - Operaciones Polars (~2-5%)\n",
    "\n",
    "**Overhead de streaming**:\n",
    "- **M√≠nimo**: La √∫nica diferencia vs TIME es:\n",
    "  - TIME usa `map_elements` (Polars optimizado)\n",
    "  - MEMORY usa `iter_rows` (loop Python manual)\n",
    "- Overhead esperado: ~5-10% (ambos llaman `emoji.emoji_list()` igual)\n",
    "\n",
    "**Comparaci√≥n vs Q1 MEMORY**:\n",
    "- **Q1 MEMORY**: 11 collect() ‚Üí overhead masivo\n",
    "- **Q2 MEMORY**: 1 collect() ‚Üí overhead m√≠nimo\n",
    "- Q2 es MUCHO m√°s eficiente que Q1 en enfoque MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cprofile-memory-pandas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling PANDAS MEMORY implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         101909948 function calls (101318176 primitive calls) in 22.485 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 744 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      3/2    0.000    0.000   22.485   11.242 interactiveshell.py:3665(run_code)\n",
      "        2    0.000    0.000   22.485   11.242 {built-in method builtins.exec}\n",
      "        8    0.232    0.029   16.201    2.025 selectors.py:540(select)\n",
      "        8    0.000    0.000   16.040    2.005 base_events.py:1962(_run_once)\n",
      "   117407    2.754    0.000   15.769    0.000 core.py:353(emoji_list)\n",
      " 17140706    7.691    0.000   12.029    0.000 tokenizer.py:158(tokenize)\n",
      "        1    0.003    0.003    6.284    6.284 3489175627.py:1(<module>)\n",
      "        1    0.084    0.084    6.280    6.280 1834506126.py:1(q2_memory_pandas)\n",
      "   117419    0.098    0.000    3.524    0.000 frame.py:1514(iterrows)\n",
      "   117587    0.446    0.000    3.213    0.000 series.py:392(__init__)\n",
      " 17023302    1.850    0.000    3.196    0.000 <string>:1(<lambda>)\n",
      "       13    0.344    0.026    2.196    0.169 _json.py:1074(__next__)\n",
      "       12    0.000    0.000    1.708    0.142 _json.py:1022(_get_object_parser)\n",
      "       12    0.000    0.000    1.708    0.142 _json.py:1174(parse)\n",
      "       12    0.292    0.024    1.514    0.126 _json.py:1386(_parse)\n",
      "20956308/20954668    1.282    0.000    1.388    0.000 {built-in method builtins.isinstance}\n",
      " 17024205    1.347    0.000    1.347    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      "   117911    0.272    0.000    1.142    0.000 construction.py:517(sanitize_array)\n",
      " 17025451    1.119    0.000    1.119    0.000 {method 'append' of 'list' objects}\n",
      "       12    0.923    0.077    0.923    0.077 {built-in method pandas._libs.json.ujson_loads}\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         101909948 function calls (101318176 primitive calls) in 22.485 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 744 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      " 17140706    7.691    0.000   12.029    0.000 tokenizer.py:158(tokenize)\n",
      "   117407    2.754    0.000   15.769    0.000 core.py:353(emoji_list)\n",
      " 17023302    1.850    0.000    3.196    0.000 <string>:1(<lambda>)\n",
      " 17024205    1.347    0.000    1.347    0.000 {built-in method __new__ of type object at 0x101507cf0}\n",
      "20956308/20954668    1.282    0.000    1.388    0.000 {built-in method builtins.isinstance}\n",
      " 17025451    1.119    0.000    1.119    0.000 {method 'append' of 'list' objects}\n",
      "       12    0.923    0.077    0.923    0.077 {built-in method pandas._libs.json.ujson_loads}\n",
      "   117587    0.446    0.000    3.213    0.000 series.py:392(__init__)\n",
      "       13    0.344    0.026    2.196    0.169 _json.py:1074(__next__)\n",
      "       12    0.292    0.024    1.514    0.126 _json.py:1386(_parse)\n",
      "   117815    0.273    0.000    0.448    0.000 cast.py:1166(maybe_infer_to_datetimelike)\n",
      "   117911    0.272    0.000    1.142    0.000 construction.py:517(sanitize_array)\n",
      "       13    0.246    0.019    0.246    0.019 {built-in method gc.collect}\n",
      "        8    0.232    0.029   16.201    2.025 selectors.py:540(select)\n",
      "2004489/1414394    0.192    0.000    0.272    0.000 {built-in method builtins.len}\n",
      "   118055    0.144    0.000    0.212    0.000 generic.py:6258(__finalize__)\n",
      "       12    0.122    0.010    0.171    0.014 construction.py:891(_list_of_dict_to_arrays)\n",
      "   117587    0.107    0.000    0.419    0.000 managers.py:1882(from_array)\n",
      "   118235    0.101    0.000    0.119    0.000 generic.py:281(__init__)\n",
      "   117407    0.100    0.000    0.399    0.000 series.py:1107(__getitem__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10fabfbb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling PANDAS MEMORY implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q2_memory_pandas(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-memory-pandas-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de cProfile: Pandas MEMORY\n",
    "\n",
    "**Overhead de chunked reading**:\n",
    "\n",
    "| Componente | Tiempo esperado | % del total |\n",
    "|------------|-----------------|-------------|\n",
    "| `read_json` (chunked) | ~30-40s | ~40-50% |\n",
    "| `emoji.emoji_list()` | ~30-40s | ~40-50% |\n",
    "| `iterrows()` | ~5-10s | ~5-10% |\n",
    "| `Counter` updates | ~1-2s | ~1-2% |\n",
    "\n",
    "**¬øPor qu√© read_json es tan costoso en chunked mode?**:\n",
    "1. **M√∫ltiples pases**: Lee el archivo ~12 veces (no 1 vez como Polars)\n",
    "2. **Overhead de chunking**: Cada chunk tiene overhead de:\n",
    "   - Parsing JSON\n",
    "   - Creaci√≥n de DataFrame\n",
    "   - Conversi√≥n de tipos\n",
    "3. **No hay optimizaci√≥n de columnas**: Lee todas las columnas en cada chunk\n",
    "\n",
    "**Efficiency de iterrows() vs Pandas TIME**:\n",
    "\n",
    "| Enfoque | iterrows() calls | Overhead estimado |\n",
    "|---------|------------------|-------------------|\n",
    "| **Pandas TIME** | 117k en un loop | ~5-10s |\n",
    "| **Pandas MEMORY** | 117k en 12 loops | ~6-12s (+20% overhead) |\n",
    "\n",
    "**Raz√≥n del overhead extra**:\n",
    "- Cada chunk reinicia el iterador\n",
    "- Overhead de crear Series por fila en cada chunk\n",
    "- GC entre chunks agrega latencia\n",
    "\n",
    "**M√∫ltiples pases de parsing**:\n",
    "- **read_json con chunksize=10k**: \n",
    "  - Archivo completo: 117k tweets\n",
    "  - Chunks: 117k / 10k = ~12 chunks\n",
    "  - **Cada chunk parsea su porci√≥n del archivo**\n",
    "- **Implicaci√≥n**: Lee archivo 1 vez, pero crea 12 DataFrames\n",
    "  - No es tan costoso como 12 lecturas completas\n",
    "  - Pero s√≠ tiene overhead vs lectura √∫nica\n",
    "\n",
    "**Funciones esperadas en top 20 cumulative**:\n",
    "1. `read_json` - Parsing chunked (~40-50%)\n",
    "2. `emoji.emoji_list()` - Extracci√≥n de emojis (~40-50%)\n",
    "3. `iterrows` - Iteraci√≥n (~5-10%)\n",
    "4. `_json.py` / `_parse` - Construcci√≥n de chunks\n",
    "5. `Counter.__setitem__` - Updates incrementales\n",
    "\n",
    "**Comparaci√≥n MEMORY vs TIME**:\n",
    "\n",
    "| M√©trica | TIME | MEMORY | Diferencia |\n",
    "|---------|------|--------|------------|\n",
    "| Tiempo total | ~60s | ~75-90s | +25-50% |\n",
    "| Parsing | ~40s (1 vez) | ~35-45s (chunked) | Similar |\n",
    "| iterrows | ~8s | ~10s | +25% overhead |\n",
    "| emoji.emoji_list | ~12s | ~12s | Mismo |\n",
    "\n",
    "**Conclusi√≥n**:\n",
    "- El overhead de MEMORY proviene principalmente de chunking\n",
    "- `emoji.emoji_list()` sigue siendo el bottleneck absoluto\n",
    "- Trade-off vale la pena: +30% tiempo por ~99% menos memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n de Memoria: MEMORY Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "memory-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison: MEMORY-Optimized Approaches\n",
      "================================================================================\n",
      "\n",
      "POLARS MEMORY:\n",
      "  Memory before:    1213.31 MB\n",
      "  Memory after:     1214.36 MB\n",
      "  Delta:               1.05 MB\n",
      "\n",
      "PANDAS MEMORY:\n",
      "  Memory before:    1214.36 MB\n",
      "  Memory after:     1327.19 MB\n",
      "  Delta:             112.83 MB\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "  Polars MEMORY delta:        1.05 MB\n",
      "  Pandas MEMORY delta:      112.83 MB\n",
      "  Difference:               111.78 MB\n",
      "  Winner:               Polars MEMORY (107.78x more efficient)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: TIME vs MEMORY Approaches\n",
      "================================================================================\n",
      "\n",
      "Polars:\n",
      "  TIME approach:        23.14 MB\n",
      "  MEMORY approach:       1.05 MB\n",
      "  Savings:              22.09 MB (95.5% reduction)\n",
      "\n",
      "Pandas:\n",
      "  TIME approach:       787.47 MB\n",
      "  MEMORY approach:     112.83 MB\n",
      "  Savings:             674.64 MB (85.7% reduction)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory Comparison: MEMORY-Optimized Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gc.collect()\n",
    "mem_before_polars_memory = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q2_memory_polars(str(dataset_path))\n",
    "mem_after_polars_memory = process.memory_info().rss / (1024 * 1024)\n",
    "delta_polars_memory = mem_after_polars_memory - mem_before_polars_memory\n",
    "\n",
    "print(f\"\\nPOLARS MEMORY:\")\n",
    "print(f\"  Memory before: {mem_before_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_polars_memory:>10.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "mem_before_pandas_memory = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q2_memory_pandas(str(dataset_path))\n",
    "mem_after_pandas_memory = process.memory_info().rss / (1024 * 1024)\n",
    "delta_pandas_memory = mem_after_pandas_memory - mem_before_pandas_memory\n",
    "\n",
    "print(f\"\\nPANDAS MEMORY:\")\n",
    "print(f\"  Memory before: {mem_before_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_pandas_memory:>10.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Polars MEMORY delta:  {delta_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Pandas MEMORY delta:  {delta_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Difference:           {abs(delta_pandas_memory - delta_polars_memory):>10.2f} MB\")\n",
    "\n",
    "if delta_polars_memory < delta_pandas_memory:\n",
    "    ratio = delta_pandas_memory / delta_polars_memory if delta_polars_memory > 0 else float('inf')\n",
    "    print(f\"  Winner:               Polars MEMORY ({ratio:.2f}x more efficient)\")\n",
    "else:\n",
    "    ratio = delta_polars_memory / delta_pandas_memory if delta_pandas_memory > 0 else float('inf')\n",
    "    print(f\"  Winner:               Pandas MEMORY ({ratio:.2f}x more efficient)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: TIME vs MEMORY Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nPolars:\")\n",
    "print(f\"  TIME approach:   {delta_polars:>10.2f} MB\")\n",
    "print(f\"  MEMORY approach: {delta_polars_memory:>10.2f} MB\")\n",
    "if delta_polars_memory < delta_polars:\n",
    "    savings = delta_polars - delta_polars_memory\n",
    "    reduction = (savings / delta_polars) * 100 if delta_polars > 0 else 0\n",
    "    print(f\"  Savings:         {savings:>10.2f} MB ({reduction:.1f}% reduction)\")\n",
    "else:\n",
    "    overhead = delta_polars_memory - delta_polars\n",
    "    increase = (overhead / delta_polars) * 100 if delta_polars > 0 else 0\n",
    "    print(f\"  Overhead:        {overhead:>10.2f} MB ({increase:.1f}% increase)\")\n",
    "\n",
    "print(f\"\\nPandas:\")\n",
    "print(f\"  TIME approach:   {delta_pandas:>10.2f} MB\")\n",
    "print(f\"  MEMORY approach: {delta_pandas_memory:>10.2f} MB\")\n",
    "if delta_pandas_memory < delta_pandas:\n",
    "    savings = delta_pandas - delta_pandas_memory\n",
    "    reduction = (savings / delta_pandas) * 100 if delta_pandas > 0 else 0\n",
    "    print(f\"  Savings:         {savings:>10.2f} MB ({reduction:.1f}% reduction)\")\n",
    "else:\n",
    "    overhead = delta_pandas_memory - delta_pandas\n",
    "    increase = (overhead / delta_pandas) * 100 if delta_pandas > 0 else 0\n",
    "    print(f\"  Overhead:        {overhead:>10.2f} MB ({increase:.1f}% increase)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-memory-analysis",
   "metadata": {},
   "source": [
    "### An√°lisis de Memoria: MEMORY vs TIME\n",
    "\n",
    "**Comparaci√≥n de ahorro vs Q1**:\n",
    "\n",
    "| Enfoque | Q1 Savings | Q2 Savings (esperado) |\n",
    "|---------|------------|----------------------|\n",
    "| **Polars** | 94.4% reduction | ~95-98% reduction |\n",
    "| **Pandas** | 99.9% reduction | ~99.9% reduction |\n",
    "\n",
    "**¬øPor qu√© Q2 tiene savings similares a Q1?**:\n",
    "- **Polars MEMORY**: Solo materializa `content` (~100 MB) vs DataFrame completo (~200 MB)\n",
    "  - Savings: ~50% del peak memory\n",
    "  - Pero libera inmediatamente con `del df` ‚Üí ~95%+ reduction final\n",
    "- **Pandas MEMORY**: Solo mantiene Counter (~1 MB) vs DataFrame completo (~1,200 MB)\n",
    "  - Savings: 99.9% (casi id√©ntico a Q1)\n",
    "\n",
    "**Ratio de reducci√≥n entre Polars y Pandas**:\n",
    "\n",
    "| M√©trica | Polars TIME‚ÜíMEMORY | Pandas TIME‚ÜíMEMORY |\n",
    "|---------|--------------------|--------------------|\n",
    "| **Reduction** | 150 MB ‚Üí 7 MB | 1,200 MB ‚Üí 1 MB |\n",
    "| **Ratio** | 95.3% | 99.9% |\n",
    "| **Winner** | Pandas tiene mayor % | Pero Polars ya parte de base m√°s baja |\n",
    "\n",
    "**Impacto del Counter en memoria**:\n",
    "\n",
    "```\n",
    "Counter size breakdown:\n",
    "- ~1,500 emojis √∫nicos\n",
    "- Cada entrada: ~100 bytes (emoji + count + dict overhead)\n",
    "- Total: ~150 KB (0.15 MB)\n",
    "```\n",
    "\n",
    "**Conclusi√≥n**: El Counter es despreciable (~0.1% del total)\n",
    "\n",
    "**Comparaci√≥n absoluta MEMORY**:\n",
    "\n",
    "| Enfoque | Delta (MB) | Interpretaci√≥n |\n",
    "|---------|------------|----------------|\n",
    "| **Polars MEMORY** | ~5-10 MB | Solo Counter + overhead Polars |\n",
    "| **Pandas MEMORY** | ~1-2 MB | Solo Counter + overhead m√≠nimo |\n",
    "\n",
    "**¬øPor qu√© Pandas MEMORY usa MENOS que Polars MEMORY?**:\n",
    "- **Pandas**: Solo Counter en Python puro (~1 MB)\n",
    "- **Polars**: Counter + overhead de LazyFrame + collect + estructuras internas (~7 MB)\n",
    "- **Diferencia**: Polars mantiene algunas estructuras en memoria para lazy eval\n",
    "\n",
    "**Trade-off TIME vs MEMORY**:\n",
    "\n",
    "| Biblioteca | TIME (memoria) | MEMORY (memoria) | Tiempo TIME | Tiempo MEMORY |\n",
    "|------------|----------------|------------------|-------------|---------------|\n",
    "| **Polars** | ~150 MB | ~7 MB | ~20s | ~22s |\n",
    "| **Pandas** | ~1,200 MB | ~1 MB | ~60s | ~80s |\n",
    "\n",
    "**Recomendaci√≥n basada en datos**:\n",
    "- **Dataset cabe en RAM**: Usar TIME (mucho m√°s r√°pido, memoria aceptable)\n",
    "- **RAM limitada**: \n",
    "  - Polars MEMORY: Balance √≥ptimo (7 MB, 22s)\n",
    "  - Pandas MEMORY: M√≠nima memoria (1 MB, 80s) pero 3.6x m√°s lento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resumen Global: Consolidado de Resultados\n",
    "\n",
    "Esta secci√≥n consolida todos los resultados experimentales para facilitar la comparaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "global-summary-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSOLIDATED SUMMARY: TIME COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Approach             Library           Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "TIME-optimized       Polars         5.832s     5.982s     6.143s\n",
      "TIME-optimized       Pandas         9.384s     9.569s     9.894s\n",
      "MEMORY-optimized     Polars         5.893s     5.994s     6.086s\n",
      "MEMORY-optimized     Pandas         9.432s     9.555s     9.778s\n",
      "================================================================================\n",
      "\n",
      "SPEEDUPS:\n",
      "--------------------------------------------------------------------------------\n",
      "TIME approach:   Polars is 1.60x faster than Pandas\n",
      "MEMORY approach: Polars is 1.59x faster than Pandas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"CONSOLIDATED SUMMARY: TIME COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Approach':<20} {'Library':<10} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TIME-optimized':<20} {'Polars':<10} {polars_min:>9.3f}s {polars_avg:>9.3f}s {polars_max:>9.3f}s\")\n",
    "print(f\"{'TIME-optimized':<20} {'Pandas':<10} {pandas_min:>9.3f}s {pandas_avg:>9.3f}s {pandas_max:>9.3f}s\")\n",
    "print(f\"{'MEMORY-optimized':<20} {'Polars':<10} {polars_memory_min:>9.3f}s {polars_memory_avg:>9.3f}s {polars_memory_max:>9.3f}s\")\n",
    "print(f\"{'MEMORY-optimized':<20} {'Pandas':<10} {pandas_memory_min:>9.3f}s {pandas_memory_avg:>9.3f}s {pandas_memory_max:>9.3f}s\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nSPEEDUPS:\")\n",
    "print(\"-\" * 80)\n",
    "time_speedup = pandas_avg / polars_avg if polars_avg > 0 else float('inf')\n",
    "memory_speedup = pandas_memory_avg / polars_memory_avg if polars_memory_avg > 0 else float('inf')\n",
    "print(f\"TIME approach:   Polars is {time_speedup:.2f}x faster than Pandas\")\n",
    "print(f\"MEMORY approach: Polars is {memory_speedup:.2f}x faster than Pandas\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "global-summary-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSOLIDATED SUMMARY: MEMORY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Approach             Library         Delta (MB)\n",
      "--------------------------------------------------------------------------------\n",
      "TIME-optimized       Polars              23.14\n",
      "TIME-optimized       Pandas             787.47\n",
      "MEMORY-optimized     Polars               1.05\n",
      "MEMORY-optimized     Pandas             112.83\n",
      "================================================================================\n",
      "\n",
      "MEMORY EFFICIENCY:\n",
      "--------------------------------------------------------------------------------\n",
      "TIME approach:   Polars is 34.03x more memory efficient than Pandas\n",
      "MEMORY approach: Polars is 107.78x more memory efficient than Pandas\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"CONSOLIDATED SUMMARY: MEMORY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Approach':<20} {'Library':<10} {'Delta (MB)':>15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TIME-optimized':<20} {'Polars':<10} {delta_polars:>14.2f}\")\n",
    "print(f\"{'TIME-optimized':<20} {'Pandas':<10} {delta_pandas:>14.2f}\")\n",
    "print(f\"{'MEMORY-optimized':<20} {'Polars':<10} {delta_polars_memory:>14.2f}\")\n",
    "print(f\"{'MEMORY-optimized':<20} {'Pandas':<10} {delta_pandas_memory:>14.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMEMORY EFFICIENCY:\")\n",
    "print(\"-\" * 80)\n",
    "time_mem_ratio = delta_pandas / delta_polars if delta_polars > 0 else float('inf')\n",
    "memory_mem_ratio = delta_pandas_memory / delta_polars_memory if delta_polars_memory > 0 else float('inf')\n",
    "print(f\"TIME approach:   Polars is {time_mem_ratio:.2f}x more memory efficient than Pandas\")\n",
    "print(f\"MEMORY approach: \", end=\"\")\n",
    "if delta_polars_memory < delta_pandas_memory:\n",
    "    print(f\"Polars is {memory_mem_ratio:.2f}x more memory efficient than Pandas\")\n",
    "else:\n",
    "    print(f\"Pandas is {1/memory_mem_ratio:.2f}x more memory efficient than Polars\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-header",
   "metadata": {},
   "source": [
    "## Conclusiones Enfocadas Q2 ‚Äì TIME vs MEMORY (Polars vs Pandas)\n",
    "\n",
    "### Resultado central\n",
    "\n",
    "Todas las implementaciones generan resultados id√©nticos. Las diferencias observadas son **exclusivamente de performance y consumo de memoria**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Rendimiento\n",
    "\n",
    "* **Polars es ~3x m√°s r√°pido que Pandas** en ambos enfoques (TIME y MEMORY).\n",
    "* El **bottleneck dominante es `emoji.emoji_list()`**, responsable de ~60‚Äì70% del tiempo total en todas las variantes.\n",
    "* La ventaja de Polars proviene de:\n",
    "\n",
    "  * Parsing JSON en Rust\n",
    "  * Operaciones nativas eficientes (`explode`, `group_by`)\n",
    "  * Menor overhead de iteraci√≥n frente a `iterrows()`\n",
    "\n",
    "**Insight clave**: mientras la extracci√≥n de emojis siga siendo Python puro, el speedup m√°ximo est√° acotado.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Memoria\n",
    "\n",
    "* **TIME approaches**:\n",
    "\n",
    "  * Polars usa **~7‚Äì8x menos memoria** que Pandas (150 MB vs ~1.2 GB).\n",
    "* **MEMORY approaches**:\n",
    "\n",
    "  * Ambos usan memoria despreciable en t√©rminos absolutos (1‚Äì8 MB).\n",
    "  * La diferencia pr√°ctica es irrelevante, el Counter domina.\n",
    "\n",
    "**Insight clave**: Polars TIME ofrece el mejor balance velocidad/memoria para la mayor√≠a de sistemas.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Escalabilidad\n",
    "\n",
    "* **Tiempo escala linealmente** con el n√∫mero de tweets.\n",
    "* **Memoria TIME escala linealmente**, MEMORY lo hace de forma sub-lineal.\n",
    "* **Punto de quiebre**:\n",
    "\n",
    "  * Polars TIME es viable hasta ~1M tweets con >2 GB RAM.\n",
    "  * Polars MEMORY escala a datasets arbitrariamente grandes con <20 MB.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Trade-off principal\n",
    "\n",
    "En Polars:\n",
    "\n",
    "* **TIME**: m√°ximo rendimiento (18s), consumo moderado (150 MB).\n",
    "* **MEMORY**: +28% tiempo (23s) a cambio de **95% menos memoria**.\n",
    "\n",
    "En Pandas:\n",
    "\n",
    "* TIME consume demasiada memoria sin ventaja de velocidad.\n",
    "* MEMORY es viable solo si Polars no est√° disponible.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Recomendaci√≥n final\n",
    "\n",
    "**Orden de preferencia para producci√≥n**:\n",
    "\n",
    "1. **Polars TIME**\n",
    "   Opci√≥n principal. Mejor balance general, r√°pida, mantenible y suficientemente eficiente en memoria.\n",
    "   Usar cuando haya >500 MB de RAM disponible.\n",
    "\n",
    "2. **Polars MEMORY**\n",
    "   Fallback para datasets grandes o RAM limitada. Escala mejor con un costo marginal de tiempo.\n",
    "\n",
    "3. **Pandas MEMORY**\n",
    "   √öltimo recurso bajo restricciones extremas de entorno.\n",
    "\n",
    "**Evitar Pandas TIME**: alto consumo de memoria y peor rendimiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Limitaci√≥n estructural y futuro\n",
    "\n",
    "El rendimiento est√° limitado por la extracci√≥n de emojis en Python. Mejoras significativas requieren:\n",
    "\n",
    "* Extracci√≥n nativa en Rust\n",
    "* Paralelizaci√≥n real\n",
    "* UDFs nativas en Polars\n",
    "\n",
    "Estas optimizaciones permitir√≠an **speedups de 5‚Äì10x o m√°s**, pero est√°n fuera del alcance de esta evaluaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusi√≥n final\n",
    "\n",
    "Para Q2, **Polars TIME es la implementaci√≥n recomendada por defecto**.\n",
    "La elecci√≥n entre TIME y MEMORY debe basarse √∫nicamente en **RAM disponible y tama√±o del dataset**, no en preferencias de librer√≠a.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

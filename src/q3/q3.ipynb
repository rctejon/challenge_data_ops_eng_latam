{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Q3: Top 10 Usuarios M√°s Influyentes (por Menciones)\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Calcular el **top 10 hist√≥rico de usuarios m√°s influyentes**, medido como el **n√∫mero total de menciones (@username)** que reciben en todo el dataset.\n",
    "\n",
    "**Output esperado:** `List[Tuple[str, int]]`\n",
    "\n",
    "## Definici√≥n de Influencia\n",
    "\n",
    "- Una **menci√≥n** se define como cualquier aparici√≥n de un username en el campo `mentionedUsers` del tweet.\n",
    "- El campo `mentionedUsers` es una **lista de objetos estructurados** con formato:\n",
    "  ```json\n",
    "  {\n",
    "    \"username\": \"narendramodi\",\n",
    "    \"displayname\": \"Narendra Modi\",\n",
    "    \"id\": 18839785,\n",
    "    \"description\": null\n",
    "  }\n",
    "  ```\n",
    "- **67.6%** de tweets tienen `mentionedUsers = null` (sin menciones).\n",
    "- **32.4%** de tweets tienen menciones (38,034 tweets con promedio de 2.7 menciones/tweet).\n",
    "- El conteo es **global** (no por fecha).\n",
    "- **No se requiere parsing de texto** - usamos el campo estructurado directamente.\n",
    "\n",
    "## Enfoque Experimental: Comparaci√≥n TIME vs MEMORY\n",
    "\n",
    "Este notebook eval√∫a **cuatro enfoques diferentes** para resolver Q3, divididos en dos categor√≠as:\n",
    "\n",
    "### üöÄ TIME-OPTIMIZED (In-Memory)\n",
    "Prioridad: **m√°xima velocidad de ejecuci√≥n**\n",
    "\n",
    "#### üîµ Approach 1: Polars In-Memory\n",
    "- Biblioteca moderna escrita en Rust\n",
    "- Columnar storage (Apache Arrow)\n",
    "- **Carga completa en memoria con `scan_ndjson().collect()`**\n",
    "- Lazy evaluation + eager collection\n",
    "- Operaciones vectorizadas y paralelizadas\n",
    "- Explode de listas anidadas + group_by\n",
    "\n",
    "#### üü† Approach 2: Pandas In-Memory  \n",
    "- Biblioteca tradicional de Python\n",
    "- Basada en NumPy\n",
    "- **Carga completa en memoria con `read_json(lines=True)`**\n",
    "- Eager evaluation\n",
    "- `.explode()` para expandir listas\n",
    "- Ampliamente usada en la industria\n",
    "\n",
    "### üíæ MEMORY-OPTIMIZED (Streaming)\n",
    "Prioridad: **m√≠nimo consumo de memoria**\n",
    "\n",
    "#### üîµ Approach 3: Polars Streaming\n",
    "- Lazy evaluation sin materializaci√≥n temprana\n",
    "- Streaming aggregations\n",
    "- Solo materializa resultados finales\n",
    "- Procesa datos sin cargar todo en RAM\n",
    "\n",
    "#### üü† Approach 4: Pandas Chunked Processing\n",
    "- Procesamiento por chunks con `chunksize`\n",
    "- Contadores incrementales (Counter)\n",
    "- Evita DataFrames intermedios grandes\n",
    "- Trade-off memoria por tiempo\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de la Comparaci√≥n\n",
    "\n",
    "1. **Performance**: Medir tiempo de ejecuci√≥n de cada enfoque\n",
    "2. **Memory**: Medir consumo de memoria (RSS delta)\n",
    "3. **Profiling**: Identificar bottlenecks con cProfile\n",
    "4. **Trade-offs**: Evaluar cu√°ndo usar cada estrategia\n",
    "5. **Correctitud**: Verificar que todos producen resultados id√©nticos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Imports y configuraci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset-path",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found: 388.83 MB\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"../../data/raw/farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "dataset_path = Path(DATASET_PATH)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Run: python src/dataset/download_dataset.py\")\n",
    "else:\n",
    "    file_size_mb = dataset_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Dataset found: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n 1: Polars (TIME-optimized, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polars-time-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time_polars(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # TODO: Implementar extracci√≥n de menciones con Polars TIME-optimized\n",
    "    # Estrategia:\n",
    "    # 1. Escanear y seleccionar solo el campo mentionedUsers\n",
    "    # 2. Filtrar tweets con mentionedUsers no null\n",
    "    # 3. Collect() para materializar en memoria\n",
    "    # 4. Explode de la lista de menciones\n",
    "    # 5. Extraer el campo username de cada objeto\n",
    "    # 6. Group by username y contar\n",
    "    # 7. Sort y head(10)\n",
    "    \n",
    "    # Leer el archivo JSON y extraer solo el campo mentionedUsers\n",
    "    df = (\n",
    "        pl.scan_ndjson(file_path)\n",
    "        .select([pl.col(\"mentionedUsers\")])\n",
    "        # Filtrar tweets que tienen menciones (no null, no empty list)\n",
    "        .filter(\n",
    "            pl.col(\"mentionedUsers\").is_not_null() &\n",
    "            (pl.col(\"mentionedUsers\").list.len() > 0)\n",
    "        )\n",
    "        # Materializar en memoria\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    # Explotar la lista de menciones para tener una fila por menci√≥n\n",
    "    # Cada elemento de la lista es un struct {username, displayname, id, ...}\n",
    "    mentions_df = (\n",
    "        df\n",
    "        .explode(\"mentionedUsers\")\n",
    "        # Extraer el campo username del struct\n",
    "        .with_columns(\n",
    "            pl.col(\"mentionedUsers\").struct.field(\"username\").alias(\"username\")\n",
    "        )\n",
    "        .select([\"username\"])\n",
    "        # Filtrar usernames nulos (por si acaso)\n",
    "        .filter(pl.col(\"username\").is_not_null())\n",
    "    )\n",
    "    \n",
    "    # Contar menciones por usuario y obtener top 10\n",
    "    # Ordenamiento determin√≠stico:\n",
    "    # 1. Por conteo de menciones (descendente)\n",
    "    # 2. Por username (ascendente) para tie-breaks\n",
    "    top_10 = (\n",
    "        mentions_df\n",
    "        .group_by(\"username\")\n",
    "        .agg(pl.len().alias(\"mention_count\"))\n",
    "        .sort([\"mention_count\", \"username\"], descending=[True, False])\n",
    "        .head(10)\n",
    "    )\n",
    "    \n",
    "    # Convertir a lista de tuplas (username, count)\n",
    "    results = [\n",
    "        (row[\"username\"], row[\"mention_count\"]) \n",
    "        for row in top_10.iter_rows(named=True)\n",
    "    ]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polars-time-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars - Top 10 Most Influential Users:\n",
      "============================================================\n",
      " 1. @narendramodi         -> 2,265 mentions\n",
      " 2. @Kisanektamorcha      -> 1,840 mentions\n",
      " 3. @RakeshTikaitBKU      -> 1,644 mentions\n",
      " 4. @PMOIndia             -> 1,427 mentions\n",
      " 5. @RahulGandhi          -> 1,146 mentions\n",
      " 6. @GretaThunberg        -> 1,048 mentions\n",
      " 7. @RaviSinghKA          -> 1,019 mentions\n",
      " 8. @rihanna              -> 986 mentions\n",
      " 9. @UNHumanRights        -> 962 mentions\n",
      "10. @meenaharris          -> 926 mentions\n"
     ]
    }
   ],
   "source": [
    "result_polars = q3_time_polars(str(dataset_path))\n",
    "\n",
    "print(\"Polars - Top 10 Most Influential Users:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (username, count) in enumerate(result_polars, 1):\n",
    "    print(f\"{i:2d}. @{username:<20} -> {count:,} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars-time-analysis",
   "metadata": {},
   "source": "**Resultados Q3 - Polars TIME:**\n\nEl usuario m√°s influyente es **@narendramodi** (Primer Ministro de India) con 2,265 menciones, seguido de organizaciones de agricultores como @Kisanektamorcha (1,840). El top 10 incluye figuras pol√≠ticas (@RahulGandhi, @PMOIndia), activistas internacionales (@GretaThunberg, @rihanna), y organismos de derechos humanos (@UNHumanRights), reflejando el alcance global de las protestas de agricultores en India."
  },
  {
   "cell_type": "markdown",
   "id": "pandas-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementaci√≥n 2: Pandas (TIME-optimized, In-Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pandas-time-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time_pandas(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # TODO: Implementar extracci√≥n de menciones con Pandas TIME-optimized\n",
    "    # Estrategia:\n",
    "    # 1. Leer JSON completo en memoria\n",
    "    # 2. Seleccionar solo mentionedUsers\n",
    "    # 3. Filtrar no null y no empty\n",
    "    # 4. Explode de la lista\n",
    "    # 5. Extraer username de cada dict\n",
    "    # 6. value_counts() y sort\n",
    "    \n",
    "    # Leer el archivo JSON completo en memoria\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Seleccionar solo la columna mentionedUsers\n",
    "    df = df[['mentionedUsers']]\n",
    "    \n",
    "    # Filtrar tweets que tienen menciones (no None, no empty list)\n",
    "    df = df[\n",
    "        df['mentionedUsers'].notna() & \n",
    "        (df['mentionedUsers'].apply(lambda x: isinstance(x, list) and len(x) > 0))\n",
    "    ]\n",
    "    \n",
    "    # Explotar la lista de menciones para tener una fila por menci√≥n\n",
    "    # Cada elemento de la lista es un diccionario {username, displayname, id, ...}\n",
    "    mentions_df = df.explode('mentionedUsers')\n",
    "    \n",
    "    # Extraer el campo username de cada diccionario\n",
    "    mentions_df['username'] = mentions_df['mentionedUsers'].apply(\n",
    "        lambda x: x.get('username') if isinstance(x, dict) else None\n",
    "    )\n",
    "    \n",
    "    # Seleccionar solo la columna username y eliminar nulos\n",
    "    mentions_df = mentions_df[['username']].dropna()\n",
    "    \n",
    "    # Contar menciones por usuario\n",
    "    mention_counts = mentions_df['username'].value_counts()\n",
    "    \n",
    "    # Convertir a DataFrame para ordenamiento determin√≠stico\n",
    "    top_10_df = mention_counts.reset_index()\n",
    "    top_10_df.columns = ['username', 'mention_count']\n",
    "    \n",
    "    # Ordenamiento determin√≠stico:\n",
    "    # 1. Por conteo de menciones (descendente)\n",
    "    # 2. Por username (ascendente) para tie-breaks\n",
    "    top_10_df = top_10_df.sort_values(\n",
    "        ['mention_count', 'username'],\n",
    "        ascending=[False, True]\n",
    "    ).head(10)\n",
    "    \n",
    "    # Convertir a lista de tuplas (username, count)\n",
    "    results = [\n",
    "        (row['username'], row['mention_count']) \n",
    "        for _, row in top_10_df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pandas-time-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Top 10 Most Influential Users:\n",
      "============================================================\n",
      " 1. @narendramodi         -> 2,265 mentions\n",
      " 2. @Kisanektamorcha      -> 1,840 mentions\n",
      " 3. @RakeshTikaitBKU      -> 1,644 mentions\n",
      " 4. @PMOIndia             -> 1,427 mentions\n",
      " 5. @RahulGandhi          -> 1,146 mentions\n",
      " 6. @GretaThunberg        -> 1,048 mentions\n",
      " 7. @RaviSinghKA          -> 1,019 mentions\n",
      " 8. @rihanna              -> 986 mentions\n",
      " 9. @UNHumanRights        -> 962 mentions\n",
      "10. @meenaharris          -> 926 mentions\n"
     ]
    }
   ],
   "source": [
    "result_pandas = q3_time_pandas(str(dataset_path))\n",
    "\n",
    "print(\"Pandas - Top 10 Most Influential Users:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (username, count) in enumerate(result_pandas, 1):\n",
    "    print(f\"{i:2d}. @{username:<20} -> {count:,} mentions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pandas-time-analysis",
   "metadata": {},
   "source": "**Resultados Q3 - Pandas TIME:**\n\nPandas produce **resultados id√©nticos** a Polars: mismo top 10 de usuarios con los mismos conteos exactos. Esto confirma que ambas bibliotecas procesan el campo `mentionedUsers` correctamente, extrayendo y contando menciones de manera consistente."
  },
  {
   "cell_type": "markdown",
   "id": "verification-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verificaci√≥n: Resultados Id√©nticos (TIME Approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verification-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Comparing TIME Results\n",
      "================================================================================\n",
      "‚úÖ Polars TIME and Pandas TIME produce IDENTICAL results\n",
      "   10 tuples match perfectly\n",
      "\n",
      "================================================================================\n",
      "Detailed Comparison:\n",
      "================================================================================\n",
      "Rank   Polars Username           Pandas Username                Count\n",
      "--------------------------------------------------------------------------------\n",
      "1      @narendramodi             @narendramodi                  2,265 ‚úì\n",
      "2      @Kisanektamorcha          @Kisanektamorcha               1,840 ‚úì\n",
      "3      @RakeshTikaitBKU          @RakeshTikaitBKU               1,644 ‚úì\n",
      "4      @PMOIndia                 @PMOIndia                      1,427 ‚úì\n",
      "5      @RahulGandhi              @RahulGandhi                   1,146 ‚úì\n",
      "6      @GretaThunberg            @GretaThunberg                 1,048 ‚úì\n",
      "7      @RaviSinghKA              @RaviSinghKA                   1,019 ‚úì\n",
      "8      @rihanna                  @rihanna                         986 ‚úì\n",
      "9      @UNHumanRights            @UNHumanRights                   962 ‚úì\n",
      "10     @meenaharris              @meenaharris                     926 ‚úì\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification: Comparing TIME Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if result_polars == result_pandas:\n",
    "    print(\"‚úÖ Polars TIME and Pandas TIME produce IDENTICAL results\")\n",
    "    print(f\"   {len(result_polars)} tuples match perfectly\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: Results differ!\")\n",
    "    for i, (pol, pan) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "        if pol != pan:\n",
    "            print(f\"   Position {i}: Polars={pol}, Pandas={pan}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Detailed Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<6} {'Polars Username':<25} {'Pandas Username':<25} {'Count':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, ((user_pol, count_pol), (user_pan, count_pan)) in enumerate(zip(result_polars, result_pandas), 1):\n",
    "    match = \"‚úì\" if (user_pol == user_pan and count_pol == count_pan) else \"‚úó\"\n",
    "    print(f\"{i:<6} @{user_pol:<24} @{user_pan:<24} {count_pol:>10,} {match}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-time-notes",
   "metadata": {},
   "source": "**Verificaci√≥n TIME:**\n\nAmbos enfoques TIME producen **resultados 100% id√©nticos** (10/10 coincidencias perfectas). Esto valida que tanto Polars como Pandas implementan correctamente: (1) filtrado de tweets con menciones, (2) explode de listas anidadas, (3) extracci√≥n del campo `username` de estructuras/diccionarios, y (4) ordenamiento determin√≠stico."
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Tiempo de Ejecuci√≥n (TIME)\n",
    "\n",
    "Se ejecutan 3 runs de cada implementaci√≥n para obtener m√©tricas confiables. Se reportan min, avg y max para capturar variabilidad por estado del sistema (cach√©, GC, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "benchmark-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Comparison: Polars vs Pandas (TIME-optimized)\n",
      "================================================================================\n",
      "\n",
      "Running Polars TIME implementation 3 times...\n",
      "  Run 1: 0.363s\n",
      "  Run 2: 0.271s\n",
      "  Run 3: 0.320s\n",
      "\n",
      "Running Pandas TIME implementation 3 times...\n",
      "  Run 1: 3.340s\n",
      "  Run 2: 2.574s\n",
      "  Run 3: 2.781s\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "\n",
      "Library                Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Polars TIME         0.271s     0.318s     0.363s\n",
      "Pandas TIME         2.574s     2.898s     3.340s\n",
      "\n",
      "Speedup:        9.12x (Polars is 9.12x faster)\n",
      "Difference:     2.581s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "print(\"Time Comparison: Polars vs Pandas (TIME-optimized)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nRunning Polars TIME implementation {n_runs} times...\")\n",
    "polars_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q3_time_polars(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    polars_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "polars_avg = sum(polars_times) / len(polars_times)\n",
    "polars_min = min(polars_times)\n",
    "polars_max = max(polars_times)\n",
    "\n",
    "print(f\"\\nRunning Pandas TIME implementation {n_runs} times...\")\n",
    "pandas_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q3_time_pandas(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    pandas_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "pandas_avg = sum(pandas_times) / len(pandas_times)\n",
    "pandas_min = min(pandas_times)\n",
    "pandas_max = max(pandas_times)\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Library':<15} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Polars TIME':<15} {polars_min:>9.3f}s {polars_avg:>9.3f}s {polars_max:>9.3f}s\")\n",
    "print(f\"{'Pandas TIME':<15} {pandas_min:>9.3f}s {pandas_avg:>9.3f}s {pandas_max:>9.3f}s\")\n",
    "\n",
    "speedup = pandas_avg / polars_avg if polars_avg > 0 else float('inf')\n",
    "diff = abs(pandas_avg - polars_avg)\n",
    "\n",
    "print(f\"\\n{'Speedup:':<15} {speedup:.2f}x (Polars is {speedup:.2f}x faster)\")\n",
    "print(f\"{'Difference:':<15} {diff:.3f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-time-analysis",
   "metadata": {},
   "source": "**Benchmark TIME:**\n\nPolars es **9.12x m√°s r√°pido** que Pandas (0.318s vs 2.898s promedio). \n\n**An√°lisis:**\n- **Polars**: Ejecuci√≥n consistente (~0.27-0.36s) gracias al motor Rust y operaciones vectorizadas sobre Arrow\n- **Pandas**: Variabilidad mayor (2.57-3.34s) por overhead de Python en explode + apply + dict access\n- **Bottleneck**: Para Pandas, el `explode()` + `.apply(lambda x: x.get('username'))` es muy costoso con 117k menciones\n\nLa ventaja de Polars es clara: procesa listas anidadas y structs de manera nativa y eficiente."
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Profiling Detallado: cProfile (TIME)\n",
    "\n",
    "An√°lisis de latencia funci√≥n por funci√≥n usando cProfile para identificar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cprofile-time-polars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling POLARS TIME implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1816 function calls (1799 primitive calls) in 0.351 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 327 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        4    0.000    0.000    0.204    0.051 base_events.py:1962(_run_once)\n",
      "        4    0.000    0.000    0.200    0.050 selectors.py:540(select)\n",
      "        4    0.199    0.050    0.199    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        7    0.000    0.000    0.144    0.021 opt_flags.py:312(wrapper)\n",
      "        7    0.000    0.000    0.144    0.021 frame.py:2198(collect)\n",
      "        7    0.144    0.021    0.144    0.021 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        1    0.000    0.000    0.006    0.006 frame.py:9194(explode)\n",
      "        4    0.000    0.000    0.004    0.001 events.py:87(_run)\n",
      "        4    0.000    0.000    0.004    0.001 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.004    0.004 group_by.py:190(agg)\n",
      "        3    0.000    0.000    0.002    0.001 zmqstream.py:573(_handle_events)\n",
      "        3    0.000    0.000    0.002    0.001 ioloop.py:750(_run_callback)\n",
      "        1    0.000    0.000    0.002    0.002 asyncio.py:206(_handle_events)\n",
      "        7    0.000    0.000    0.002    0.000 attrsettr.py:43(__getattr__)\n",
      "        7    0.002    0.000    0.002    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "        1    0.000    0.000    0.002    0.002 iostream.py:611(_flush)\n",
      "        1    0.000    0.000    0.002    0.002 session.py:755(send)\n",
      "       21    0.002    0.000    0.002    0.000 socket.py:623(send)\n",
      "        1    0.000    0.000    0.002    0.002 iostream.py:272(send_multipart)\n",
      "        1    0.000    0.000    0.002    0.002 iostream.py:260(schedule)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1816 function calls (1799 primitive calls) in 0.351 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 327 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        4    0.199    0.050    0.199    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        7    0.144    0.021    0.144    0.021 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        7    0.002    0.000    0.002    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "       21    0.002    0.000    0.002    0.000 socket.py:623(send)\n",
      "        2    0.001    0.000    0.001    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method new_from_ndjson}\n",
      "       96    0.000    0.000    0.000    0.000 enum.py:1589(_get_value)\n",
      "        6    0.000    0.000    0.000    0.000 _dependencies.py:213(_check_for_numpy)\n",
      "  330/322    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 lit.py:30(lit)\n",
      "        2    0.000    0.000    0.000    0.000 frame.py:4140(_filter)\n",
      "        1    0.000    0.000    0.000    0.000 ndjson.py:177(scan_ndjson)\n",
      "       11    0.000    0.000    0.000    0.000 expr.py:21(parse_into_expression)\n",
      "        4    0.000    0.000    0.204    0.051 base_events.py:1962(_run_once)\n",
      "      2/1    0.000    0.000    0.000    0.000 deprecation.py:123(wrapper)\n",
      "        3    0.000    0.000    0.000    0.000 socket.py:771(recv_multipart)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:705(_flush_buffers)\n",
      "       21    0.000    0.000    0.000    0.000 enum.py:1596(__or__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x111971010>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "print(\"Profiling POLARS TIME implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q3_time_polars(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-time-polars-analysis",
   "metadata": {},
   "source": "**Profiling Polars TIME:**\n\nEl perfil muestra que **144ms (41% del tiempo)** se consume en `collect()` de PyLazyFrame, ejecutando la query lazy en Rust. El resto del tiempo est√° en overhead de sistema (event loop, selectors). \n\n**Conclusi√≥n:** Polars delega eficientemente el trabajo pesado a Rust, minimizando el overhead de Python. Solo 1,816 llamadas totales indican una ejecuci√≥n muy optimizada."
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cprofile-time-pandas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling PANDAS TIME implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1079516 function calls (1078898 primitive calls) in 3.227 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 991 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.162    0.162    2.799    2.799 4107218463.py:1(q3_time_pandas)\n",
      "        1    0.032    0.032    2.506    2.506 _json.py:505(read_json)\n",
      "        1    0.000    0.000    2.430    2.430 _json.py:991(read)\n",
      "        6    0.000    0.000    1.931    0.322 selectors.py:540(select)\n",
      "        6    0.648    0.108    1.931    0.322 {method 'control' of 'select.kqueue' objects}\n",
      "        1    1.003    1.003    1.003    1.003 {built-in method pandas._libs.json.ujson_loads}\n",
      "        1    0.000    0.000    0.697    0.697 _json.py:1022(_get_object_parser)\n",
      "        1    0.000    0.000    0.697    0.697 _json.py:1174(parse)\n",
      "        5    0.000    0.000    0.511    0.102 frame.py:698(__init__)\n",
      "        1    0.089    0.089    0.489    0.489 _json.py:1386(_parse)\n",
      "        1    0.000    0.000    0.306    0.306 construction.py:506(nested_data_to_arrays)\n",
      "        1    0.001    0.001    0.306    0.306 construction.py:793(to_arrays)\n",
      "        1    0.189    0.189    0.277    0.277 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.152    0.152    0.212    0.212 construction.py:891(_list_of_dict_to_arrays)\n",
      "        1    0.010    0.010    0.206    0.206 _json.py:1452(_try_convert_types)\n",
      "        4    0.032    0.008    0.198    0.050 construction.py:96(arrays_to_mgr)\n",
      "        6    0.000    0.000    0.193    0.032 {method 'run' of '_contextvars.Context' objects}\n",
      "        2    0.000    0.000    0.183    0.092 _json.py:1422(_process_converter)\n",
      "        4    0.000    0.000    0.166    0.041 managers.py:2140(create_block_manager_from_column_arrays)\n",
      "        3    0.000    0.000    0.148    0.049 ioloop.py:750(_run_callback)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1079516 function calls (1078898 primitive calls) in 3.227 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 991 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    1.003    1.003    1.003    1.003 {built-in method pandas._libs.json.ujson_loads}\n",
      "        6    0.648    0.108    1.931    0.322 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.189    0.189    0.277    0.277 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.162    0.162    2.799    2.799 4107218463.py:1(q3_time_pandas)\n",
      "        1    0.152    0.152    0.212    0.212 construction.py:891(_list_of_dict_to_arrays)\n",
      "       38    0.148    0.004    0.148    0.004 {method 'split' of 'str' objects}\n",
      "       21    0.092    0.004    0.093    0.004 construction.py:1028(convert)\n",
      "        1    0.089    0.089    0.489    0.489 _json.py:1386(_parse)\n",
      "        1    0.088    0.088    0.088    0.088 {built-in method _codecs.utf_8_decode}\n",
      "        9    0.074    0.008    0.115    0.013 managers.py:2295(_merge_blocks)\n",
      "        8    0.068    0.008    0.068    0.009 datetimes.py:487(_to_datetime_with_unit)\n",
      "   117408    0.068    0.000    0.068    0.000 {method 'strip' of 'str' objects}\n",
      "   117408    0.057    0.000    0.064    0.000 construction.py:915(<genexpr>)\n",
      "       20    0.046    0.002    0.049    0.002 managers.py:2265(_stack_arrays)\n",
      "        6    0.041    0.007    0.041    0.007 shape_base.py:218(vstack)\n",
      "        2    0.032    0.016    0.084    0.042 algorithms.py:1667(map_array)\n",
      "        4    0.032    0.008    0.198    0.050 construction.py:96(arrays_to_mgr)\n",
      "        1    0.032    0.032    2.506    2.506 _json.py:505(read_json)\n",
      "        5    0.028    0.006    0.028    0.006 {method 'join' of 'str' objects}\n",
      "        1    0.018    0.018    0.085    0.085 _json.py:971(_combine_lines)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1119ee0d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling PANDAS TIME implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q3_time_pandas(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-time-pandas-analysis",
   "metadata": {},
   "source": "**Profiling Pandas TIME:**\n\nEl bottleneck principal es **`ujson_loads` (1.0s, 31% del tiempo)** para parsear JSON. Otros costos significativos:\n- **`read()` de archivo** (0.19s)\n- **`_list_of_dict_to_arrays`** (0.15s) - conversi√≥n de listas de dicts\n- **117k llamadas a `.strip()`** (overhead de procesamiento Python)\n\nCon **1.08M llamadas totales** (vs 1.8k de Polars), Pandas muestra mucho mayor overhead de Python por su modelo eager y operaciones row-wise en `.apply()`."
  },
  {
   "cell_type": "markdown",
   "id": "memory-time-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Consumo de Memoria (TIME)\n",
    "\n",
    "Se mide el RSS (Resident Set Size) antes y despu√©s de cada ejecuci√≥n. El delta indica cu√°nta memoria adicional consume cada implementaci√≥n. Se ejecuta `gc.collect()` entre mediciones para limpiar memoria residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "memory-time",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison: Polars vs Pandas (TIME-optimized)\n",
      "================================================================================\n",
      "\n",
      "POLARS TIME:\n",
      "  Memory before:    1159.06 MB\n",
      "  Memory after:     1891.55 MB\n",
      "  Delta:             732.48 MB\n",
      "\n",
      "PANDAS TIME:\n",
      "  Memory before:    1891.55 MB\n",
      "  Memory after:     2577.86 MB\n",
      "  Delta:             686.31 MB\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "  Polars delta:      732.48 MB\n",
      "  Pandas delta:      686.31 MB\n",
      "  Difference:         46.17 MB\n",
      "  Winner:        Pandas (1.07x more efficient)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "print(\"Memory Comparison: Polars vs Pandas (TIME-optimized)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gc.collect()\n",
    "mem_before_polars = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q3_time_polars(str(dataset_path))\n",
    "mem_after_polars = process.memory_info().rss / (1024 * 1024)\n",
    "delta_polars = mem_after_polars - mem_before_polars\n",
    "\n",
    "print(f\"\\nPOLARS TIME:\")\n",
    "print(f\"  Memory before: {mem_before_polars:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_polars:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_polars:>10.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "mem_before_pandas = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q3_time_pandas(str(dataset_path))\n",
    "mem_after_pandas = process.memory_info().rss / (1024 * 1024)\n",
    "delta_pandas = mem_after_pandas - mem_before_pandas\n",
    "\n",
    "print(f\"\\nPANDAS TIME:\")\n",
    "print(f\"  Memory before: {mem_before_pandas:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_pandas:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_pandas:>10.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Polars delta:  {delta_polars:>10.2f} MB\")\n",
    "print(f\"  Pandas delta:  {delta_pandas:>10.2f} MB\")\n",
    "print(f\"  Difference:    {abs(delta_pandas - delta_polars):>10.2f} MB\")\n",
    "\n",
    "if delta_polars < delta_pandas:\n",
    "    ratio = delta_pandas / delta_polars if delta_polars > 0 else float('inf')\n",
    "    print(f\"  Winner:        Polars ({ratio:.2f}x more efficient)\")\n",
    "else:\n",
    "    ratio = delta_polars / delta_pandas if delta_pandas > 0 else float('inf')\n",
    "    print(f\"  Winner:        Pandas ({ratio:.2f}x more efficient)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-time-analysis",
   "metadata": {},
   "source": "**Memory TIME:**\n\n**Pandas usa ligeramente menos memoria** (686 MB vs 732 MB de Polars, diferencia de 46 MB).\n\n**An√°lisis:**\n- **Polars (732 MB)**: Arrow buffers columnares, m√°s eficientes pero con overhead estructural para datos anidados (listas + structs)\n- **Pandas (686 MB)**: Row-oriented storage puede ser m√°s compacto para este tipo de datos con muchos nulls (67.6% sin menciones)\n\n**Diferencia marginal (~6%)** - en la pr√°ctica ambos consumen cantidades similares de RAM para este problema."
  },
  {
   "cell_type": "markdown",
   "id": "memory-approaches-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Q3 - MEMORY-Optimized Experiments\n",
    "\n",
    "Los experimentos anteriores (TIME-optimized) cargaban el dataset completo en memoria para m√°xima velocidad. Ahora evaluamos **enfoques streaming** que priorizan m√≠nimo consumo de memoria a costa de mayor tiempo de ejecuci√≥n.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Validar el trade-off memoria vs tiempo:\n",
    "- ¬øCu√°nta memoria se ahorra con streaming?\n",
    "- ¬øCu√°nto tiempo adicional toma?\n",
    "- ¬øLos resultados son id√©nticos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars-memory-header",
   "metadata": {},
   "source": [
    "## Experiment 3: Polars Streaming (MEMORY-optimized)\n",
    "\n",
    "Estrategia: usar lazy evaluation de Polars sin collect() temprano. Las agregaciones se procesan en streaming sin materializar todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polars-memory-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory_polars(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # TODO: Implementar extracci√≥n de menciones con Polars MEMORY-optimized\n",
    "    # Estrategia:\n",
    "    # 1. Lazy evaluation completa sin collect() intermedio\n",
    "    # 2. Solo materializar el resultado final (top 10)\n",
    "    # 3. Usar streaming aggregations de Polars\n",
    "    \n",
    "    # Crear LazyFrame sin materializar\n",
    "    lazy_df = (\n",
    "        pl.scan_ndjson(file_path)\n",
    "        .select([pl.col(\"mentionedUsers\")])\n",
    "        # Filtrar tweets con menciones (no null, no empty)\n",
    "        .filter(\n",
    "            pl.col(\"mentionedUsers\").is_not_null() &\n",
    "            (pl.col(\"mentionedUsers\").list.len() > 0)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Procesamiento lazy completo: explode, extract, group, sort\n",
    "    # Solo se materializa al final con collect()\n",
    "    top_10 = (\n",
    "        lazy_df\n",
    "        .explode(\"mentionedUsers\")\n",
    "        .with_columns(\n",
    "            pl.col(\"mentionedUsers\").struct.field(\"username\").alias(\"username\")\n",
    "        )\n",
    "        .select([\"username\"])\n",
    "        .filter(pl.col(\"username\").is_not_null())\n",
    "        .group_by(\"username\")\n",
    "        .agg(pl.len().alias(\"mention_count\"))\n",
    "        .sort([\"mention_count\", \"username\"], descending=[True, False])\n",
    "        .head(10)\n",
    "        # Materializar solo el top 10 (muy peque√±o)\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    # Convertir a lista de tuplas\n",
    "    results = [\n",
    "        (row[\"username\"], row[\"mention_count\"]) \n",
    "        for row in top_10.iter_rows(named=True)\n",
    "    ]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pandas-memory-header",
   "metadata": {},
   "source": [
    "## Experiment 4: Pandas Chunked Processing (MEMORY-optimized)\n",
    "\n",
    "Estrategia: procesar el dataset por chunks usando `chunksize`. Mantener contadores incrementales sin crear DataFrames intermedios grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pandas-memory-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory_pandas(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # TODO: Implementar extracci√≥n de menciones con Pandas MEMORY-optimized\n",
    "    # Estrategia:\n",
    "    # 1. Procesar por chunks de 10k filas\n",
    "    # 2. Usar Counter incremental\n",
    "    # 3. Solo mantener contadores en memoria, no DataFrames\n",
    "    \n",
    "    # Counter para almacenar menciones de forma incremental\n",
    "    mention_counter = Counter()\n",
    "    \n",
    "    # Tama√±o de chunk para procesamiento incremental\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    # Procesar el dataset en chunks\n",
    "    for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "        # Seleccionar solo mentionedUsers\n",
    "        chunk = chunk[['mentionedUsers']]\n",
    "        \n",
    "        # Filtrar tweets con menciones (no None, no empty list)\n",
    "        chunk = chunk[\n",
    "            chunk['mentionedUsers'].notna() & \n",
    "            (chunk['mentionedUsers'].apply(lambda x: isinstance(x, list) and len(x) > 0))\n",
    "        ]\n",
    "        \n",
    "        # Iterar sobre el chunk y actualizar contadores\n",
    "        # (evita crear DataFrames intermedios grandes)\n",
    "        for mentions_list in chunk['mentionedUsers']:\n",
    "            for mention_obj in mentions_list:\n",
    "                if isinstance(mention_obj, dict) and 'username' in mention_obj:\n",
    "                    username = mention_obj['username']\n",
    "                    if username is not None:\n",
    "                        mention_counter[username] += 1\n",
    "    \n",
    "    # Obtener top 10 con ordenamiento determin√≠stico\n",
    "    # 1. Por conteo descendente (-x[1])\n",
    "    # 2. Por username ascendente (x[0]) como tie-breaker\n",
    "    top_10 = sorted(\n",
    "        mention_counter.items(),\n",
    "        key=lambda x: (-x[1], x[0])\n",
    "    )[:10]\n",
    "    \n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verificaci√≥n: MEMORY Implementations\n",
    "\n",
    "Validar que los enfoques MEMORY producen resultados id√©nticos a los enfoques TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "verification-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Comparing All 4 Approaches\n",
      "================================================================================\n",
      "‚úÖ Polars MEMORY == Polars TIME\n",
      "‚úÖ Pandas MEMORY == Pandas TIME\n",
      "‚úÖ Polars MEMORY == Pandas MEMORY\n",
      "‚úÖ All TIME approaches match\n",
      "\n",
      "üéâ ALL FOUR APPROACHES PRODUCE IDENTICAL RESULTS\n",
      "   10 tuples verified across 4 implementations\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "result_memory_polars = q3_memory_polars(str(dataset_path))\n",
    "result_memory_pandas = q3_memory_pandas(str(dataset_path))\n",
    "\n",
    "print(\"Verification: Comparing All 4 Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_match = True\n",
    "\n",
    "if result_memory_polars == result_polars:\n",
    "    print(\"‚úÖ Polars MEMORY == Polars TIME\")\n",
    "else:\n",
    "    print(\"‚ùå Polars MEMORY != Polars TIME\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_pandas == result_pandas:\n",
    "    print(\"‚úÖ Pandas MEMORY == Pandas TIME\")\n",
    "else:\n",
    "    print(\"‚ùå Pandas MEMORY != Pandas TIME\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_polars == result_memory_pandas:\n",
    "    print(\"‚úÖ Polars MEMORY == Pandas MEMORY\")\n",
    "else:\n",
    "    print(\"‚ùå Polars MEMORY != Pandas MEMORY\")\n",
    "    all_match = False\n",
    "\n",
    "if result_memory_polars == result_polars and result_polars == result_pandas:\n",
    "    print(\"‚úÖ All TIME approaches match\")\n",
    "else:\n",
    "    print(\"‚ùå TIME approaches don't match\")\n",
    "    all_match = False\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\nüéâ ALL FOUR APPROACHES PRODUCE IDENTICAL RESULTS\")\n",
    "    print(f\"   {len(result_memory_polars)} tuples verified across 4 implementations\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Results differ between approaches!\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verification-memory-notes",
   "metadata": {},
   "source": "**Verificaci√≥n MEMORY:**\n\n‚úÖ **Los 4 enfoques producen resultados id√©nticos** (Polars TIME, Pandas TIME, Polars MEMORY, Pandas MEMORY).\n\nEsto confirma que las optimizaciones de memoria (lazy evaluation en Polars, chunked processing en Pandas) **no afectan la correctitud** de los resultados. El determinismo se mantiene en todos los enfoques."
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmarks MEMORY: Tiempo de Ejecuci√≥n\n",
    "\n",
    "Medici√≥n de performance de los enfoques MEMORY-optimized con 3 runs cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "benchmark-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Comparison: MEMORY-Optimized Approaches\n",
      "================================================================================\n",
      "\n",
      "Running Polars MEMORY implementation 3 times...\n",
      "  Run 1: 0.295s\n",
      "  Run 2: 0.279s\n",
      "  Run 3: 0.258s\n",
      "\n",
      "Running Pandas MEMORY implementation 3 times...\n",
      "  Run 1: 2.486s\n",
      "  Run 2: 2.715s\n",
      "  Run 3: 2.483s\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "\n",
      "Library                Min        Avg        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Polars MEMORY       0.258s     0.278s     0.295s\n",
      "Pandas MEMORY       2.483s     2.561s     2.715s\n",
      "\n",
      "Speedup:        9.23x (Polars MEMORY is 9.23x faster)\n",
      "Difference:     2.284s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "print(\"Time Comparison: MEMORY-Optimized Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nRunning Polars MEMORY implementation {n_runs} times...\")\n",
    "polars_memory_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q3_memory_polars(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    polars_memory_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "polars_memory_avg = sum(polars_memory_times) / len(polars_memory_times)\n",
    "polars_memory_min = min(polars_memory_times)\n",
    "polars_memory_max = max(polars_memory_times)\n",
    "\n",
    "print(f\"\\nRunning Pandas MEMORY implementation {n_runs} times...\")\n",
    "pandas_memory_times = []\n",
    "for i in range(n_runs):\n",
    "    start = time.time()\n",
    "    _ = q3_memory_pandas(str(dataset_path))\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    pandas_memory_times.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.3f}s\")\n",
    "\n",
    "pandas_memory_avg = sum(pandas_memory_times) / len(pandas_memory_times)\n",
    "pandas_memory_min = min(pandas_memory_times)\n",
    "pandas_memory_max = max(pandas_memory_times)\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Library':<15} {'Min':>10} {'Avg':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Polars MEMORY':<15} {polars_memory_min:>9.3f}s {polars_memory_avg:>9.3f}s {polars_memory_max:>9.3f}s\")\n",
    "print(f\"{'Pandas MEMORY':<15} {pandas_memory_min:>9.3f}s {pandas_memory_avg:>9.3f}s {pandas_memory_max:>9.3f}s\")\n",
    "\n",
    "speedup = pandas_memory_avg / polars_memory_avg if polars_memory_avg > 0 else float('inf')\n",
    "diff = abs(pandas_memory_avg - polars_memory_avg)\n",
    "\n",
    "print(f\"\\n{'Speedup:':<15} {speedup:.2f}x (Polars MEMORY is {speedup:.2f}x faster)\")\n",
    "print(f\"{'Difference:':<15} {diff:.3f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-memory-analysis",
   "metadata": {},
   "source": "**Benchmark MEMORY:**\n\nPolars MEMORY es **9.23x m√°s r√°pido** que Pandas MEMORY (0.278s vs 2.561s).\n\n**Hallazgos clave:**\n1. **Polars MEMORY es pr√°cticamente igual de r√°pido que Polars TIME** (0.278s vs 0.318s) - ¬°solo 40ms de diferencia!\n2. **Pandas MEMORY es similar a Pandas TIME** (2.561s vs 2.898s) - ambos lentos por overhead de Python\n3. **La lazy evaluation de Polars no penaliza el tiempo** - el optimizador de queries elimina materializaci√≥n innecesaria\n\nPolars MEMORY ofrece el **mejor balance tiempo-memoria** para este problema."
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## cProfile MEMORY: An√°lisis de Latencia\n",
    "\n",
    "Profiling detallado de los enfoques MEMORY-optimized para identificar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cprofile-memory-polars",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling POLARS MEMORY implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1678 function calls (1667 primitive calls) in 0.337 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 312 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      3/2    0.000    0.000    0.336    0.168 interactiveshell.py:3665(run_code)\n",
      "      3/2    0.000    0.000    0.336    0.168 {built-in method builtins.exec}\n",
      "        1    0.002    0.002    0.335    0.335 797374028.py:1(<module>)\n",
      "        1    0.000    0.000    0.333    0.333 2478468733.py:1(q3_memory_polars)\n",
      "        1    0.000    0.000    0.333    0.333 deprecation.py:84(wrapper)\n",
      "        4    0.000    0.000    0.200    0.050 base_events.py:1962(_run_once)\n",
      "        4    0.000    0.000    0.199    0.050 selectors.py:540(select)\n",
      "        4    0.199    0.050    0.199    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.000    0.000    0.130    0.130 opt_flags.py:312(wrapper)\n",
      "        1    0.000    0.000    0.130    0.130 frame.py:2198(collect)\n",
      "        1    0.130    0.130    0.130    0.130 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        1    0.000    0.000    0.002    0.002 decorator.py:232(fun)\n",
      "        1    0.001    0.001    0.002    0.002 history.py:93(only_when_enabled)\n",
      "        5    0.000    0.000    0.002    0.000 events.py:87(_run)\n",
      "        5    0.000    0.000    0.002    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.001    0.001 history.py:1025(writeout_cache)\n",
      "        4    0.000    0.000    0.001    0.000 zmqstream.py:573(_handle_events)\n",
      "        1    0.000    0.000    0.001    0.001 history.py:1009(_writeout_input_cache)\n",
      "        2    0.001    0.000    0.001    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        2    0.000    0.000    0.001    0.000 asyncio.py:206(_handle_events)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1678 function calls (1667 primitive calls) in 0.337 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 312 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        4    0.199    0.050    0.199    0.050 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.130    0.130    0.130    0.130 {method 'collect' of 'builtins.PyLazyFrame' objects}\n",
      "        1    0.002    0.002    0.335    0.335 797374028.py:1(<module>)\n",
      "        2    0.001    0.000    0.001    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        1    0.001    0.001    0.002    0.002 history.py:93(only_when_enabled)\n",
      "        8    0.000    0.000    0.000    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "       22    0.000    0.000    0.000    0.000 socket.py:623(send)\n",
      "        1    0.000    0.000    0.333    0.333 2478468733.py:1(q3_memory_polars)\n",
      "      3/2    0.000    0.000    0.336    0.168 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "  335/327    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "      102    0.000    0.000    0.000    0.000 enum.py:1589(_get_value)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3148(_bind)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        4    0.000    0.000    0.200    0.050 base_events.py:1962(_run_once)\n",
      "        4    0.000    0.000    0.000    0.000 socket.py:771(recv_multipart)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        4    0.000    0.000    0.199    0.050 selectors.py:540(select)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method new_from_ndjson}\n",
      "       21    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1119ef610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling POLARS MEMORY implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q3_memory_polars(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-memory-polars-analysis",
   "metadata": {},
   "source": "**Profiling Polars MEMORY:**\n\nId√©ntico perfil a Polars TIME: **130ms en `collect()`** (√∫nico punto de materializaci√≥n). El enfoque lazy construye el query plan completo y lo ejecuta en una sola pasada optimizada.\n\n**Sin diferencia observable** entre TIME y MEMORY en Polars para este problema - el optimizador de queries es lo suficientemente inteligente para evitar trabajo redundante."
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cprofile-memory-pandas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling PANDAS MEMORY implementation...\n",
      "================================================================================\n",
      "\n",
      "Top 20 funciones por tiempo acumulado (cumulative time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1188001 function calls (1183131 primitive calls) in 2.625 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 707 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.003    0.003    2.598    2.598 1668774314.py:1(<module>)\n",
      "        1    0.312    0.312    2.595    2.595 3679854850.py:1(q3_memory_pandas)\n",
      "       13    0.334    0.026    2.227    0.171 _json.py:1074(__next__)\n",
      "       12    0.000    0.000    1.659    0.138 _json.py:1022(_get_object_parser)\n",
      "       12    0.000    0.000    1.659    0.138 _json.py:1174(parse)\n",
      "       12    0.324    0.027    1.465    0.122 _json.py:1386(_parse)\n",
      "       12    0.904    0.075    0.904    0.075 {built-in method pandas._libs.json.ujson_loads}\n",
      "       36    0.000    0.000    0.381    0.011 frame.py:698(__init__)\n",
      "       12    0.000    0.000    0.271    0.023 construction.py:506(nested_data_to_arrays)\n",
      "       12    0.001    0.000    0.264    0.022 construction.py:793(to_arrays)\n",
      "       12    0.134    0.011    0.188    0.016 construction.py:891(_list_of_dict_to_arrays)\n",
      "       12    0.003    0.000    0.181    0.015 _json.py:1452(_try_convert_types)\n",
      "       24    0.000    0.000    0.171    0.007 _json.py:1422(_process_converter)\n",
      "       12    0.007    0.001    0.121    0.010 _json.py:1462(_try_convert_dates)\n",
      "        6    0.000    0.000    0.108    0.018 base_events.py:1962(_run_once)\n",
      "        6    0.002    0.000    0.106    0.018 selectors.py:540(select)\n",
      "       36    0.011    0.000    0.106    0.003 construction.py:96(arrays_to_mgr)\n",
      "       12    0.022    0.002    0.103    0.009 _json.py:971(_combine_lines)\n",
      "        6    0.023    0.004    0.100    0.017 {method 'control' of 'select.kqueue' objects}\n",
      "       36    0.000    0.000    0.089    0.002 managers.py:2140(create_block_manager_from_column_arrays)\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 20 funciones por tiempo total (total time):\n",
      "--------------------------------------------------------------------------------\n",
      "         1188001 function calls (1183131 primitive calls) in 2.625 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 707 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       12    0.904    0.075    0.904    0.075 {built-in method pandas._libs.json.ujson_loads}\n",
      "       13    0.334    0.026    2.227    0.171 _json.py:1074(__next__)\n",
      "       12    0.324    0.027    1.465    0.122 _json.py:1386(_parse)\n",
      "        1    0.312    0.312    2.595    2.595 3679854850.py:1(q3_memory_pandas)\n",
      "       12    0.134    0.011    0.188    0.016 construction.py:891(_list_of_dict_to_arrays)\n",
      "      252    0.074    0.000    0.075    0.000 construction.py:1028(convert)\n",
      "       96    0.069    0.001    0.070    0.001 datetimes.py:487(_to_datetime_with_unit)\n",
      "   117419    0.051    0.000    0.057    0.000 construction.py:915(<genexpr>)\n",
      "   117407    0.047    0.000    0.047    0.000 {method 'strip' of 'str' objects}\n",
      "      108    0.031    0.000    0.054    0.000 managers.py:2295(_merge_blocks)\n",
      "      228    0.028    0.000    0.031    0.000 managers.py:2265(_stack_arrays)\n",
      "    49773    0.024    0.000    0.024    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        2    0.023    0.012    0.023    0.012 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        6    0.023    0.004    0.100    0.017 {method 'control' of 'select.kqueue' objects}\n",
      "       16    0.023    0.001    0.023    0.001 {method 'join' of 'str' objects}\n",
      "       12    0.022    0.002    0.103    0.009 _json.py:971(_combine_lines)\n",
      "       72    0.021    0.000    0.021    0.000 shape_base.py:218(vstack)\n",
      "278709/277033    0.013    0.000    0.015    0.000 {built-in method builtins.isinstance}\n",
      "       12    0.012    0.001    0.030    0.003 algorithms.py:1667(map_array)\n",
      "   117407    0.012    0.000    0.018    0.000 3679854850.py:22(<lambda>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1119e76f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Profiling PANDAS MEMORY implementation...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "_ = q3_memory_pandas(str(dataset_path))\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "\n",
    "print(\"\\nTop 20 funciones por tiempo acumulado (cumulative time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)\n",
    "\n",
    "stats.sort_stats('tottime')\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Top 20 funciones por tiempo total (total time):\")\n",
    "print(\"-\" * 80)\n",
    "stats.print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cprofile-memory-pandas-analysis",
   "metadata": {},
   "source": "**Profiling Pandas MEMORY:**\n\nEl bottleneck es el **procesamiento por chunks**: 12 iteraciones de `ujson_loads` (0.9s total) + `__next__` (0.33s) del iterador de chunks.\n\n**Comparado con Pandas TIME:**\n- TIME: 1 carga grande (1.0s en ujson)\n- MEMORY: 12 cargas peque√±as (0.9s total en ujson) + overhead de chunks\n\nEl chunking de Pandas tiene **buen overhead m√≠nimo** (~10% m√°s lento que TIME) pero con beneficio dram√°tico de memoria."
  },
  {
   "cell_type": "markdown",
   "id": "memory-memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparaci√≥n Experimental: Consumo de Memoria (MEMORY)\n",
    "\n",
    "Medici√≥n de RSS para los enfoques MEMORY-optimized y comparaci√≥n con enfoques TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "memory-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Comparison: MEMORY-Optimized Approaches\n",
      "================================================================================\n",
      "\n",
      "POLARS MEMORY:\n",
      "  Memory before:    1517.53 MB\n",
      "  Memory after:     1533.44 MB\n",
      "  Delta:              15.91 MB\n",
      "\n",
      "PANDAS MEMORY:\n",
      "  Memory before:    1533.44 MB\n",
      "  Memory after:     1537.91 MB\n",
      "  Delta:               4.47 MB\n",
      "\n",
      "RESULTS                                 \n",
      "================================================================================\n",
      "  Polars MEMORY delta:       15.91 MB\n",
      "  Pandas MEMORY delta:        4.47 MB\n",
      "  Difference:                11.44 MB\n",
      "  Winner:               Pandas MEMORY (3.56x more efficient)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: TIME vs MEMORY Approaches\n",
      "================================================================================\n",
      "\n",
      "Polars:\n",
      "  TIME approach:       732.48 MB\n",
      "  MEMORY approach:      15.91 MB\n",
      "  Savings:             716.58 MB (97.8% reduction)\n",
      "\n",
      "Pandas:\n",
      "  TIME approach:       686.31 MB\n",
      "  MEMORY approach:       4.47 MB\n",
      "  Savings:             681.84 MB (99.3% reduction)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory Comparison: MEMORY-Optimized Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gc.collect()\n",
    "mem_before_polars_memory = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q3_memory_polars(str(dataset_path))\n",
    "mem_after_polars_memory = process.memory_info().rss / (1024 * 1024)\n",
    "delta_polars_memory = mem_after_polars_memory - mem_before_polars_memory\n",
    "\n",
    "print(f\"\\nPOLARS MEMORY:\")\n",
    "print(f\"  Memory before: {mem_before_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_polars_memory:>10.2f} MB\")\n",
    "\n",
    "gc.collect()\n",
    "mem_before_pandas_memory = process.memory_info().rss / (1024 * 1024)\n",
    "_ = q3_memory_pandas(str(dataset_path))\n",
    "mem_after_pandas_memory = process.memory_info().rss / (1024 * 1024)\n",
    "delta_pandas_memory = mem_after_pandas_memory - mem_before_pandas_memory\n",
    "\n",
    "print(f\"\\nPANDAS MEMORY:\")\n",
    "print(f\"  Memory before: {mem_before_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Memory after:  {mem_after_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Delta:         {delta_pandas_memory:>10.2f} MB\")\n",
    "\n",
    "print(f\"\\n{'RESULTS':<40}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Polars MEMORY delta:  {delta_polars_memory:>10.2f} MB\")\n",
    "print(f\"  Pandas MEMORY delta:  {delta_pandas_memory:>10.2f} MB\")\n",
    "print(f\"  Difference:           {abs(delta_pandas_memory - delta_polars_memory):>10.2f} MB\")\n",
    "\n",
    "if delta_polars_memory < delta_pandas_memory:\n",
    "    ratio = delta_pandas_memory / delta_polars_memory if delta_polars_memory > 0 else float('inf')\n",
    "    print(f\"  Winner:               Polars MEMORY ({ratio:.2f}x more efficient)\")\n",
    "else:\n",
    "    ratio = delta_polars_memory / delta_pandas_memory if delta_pandas_memory > 0 else float('inf')\n",
    "    print(f\"  Winner:               Pandas MEMORY ({ratio:.2f}x more efficient)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: TIME vs MEMORY Approaches\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nPolars:\")\n",
    "print(f\"  TIME approach:   {delta_polars:>10.2f} MB\")\n",
    "print(f\"  MEMORY approach: {delta_polars_memory:>10.2f} MB\")\n",
    "if delta_polars_memory < delta_polars:\n",
    "    savings = delta_polars - delta_polars_memory\n",
    "    reduction = (savings / delta_polars) * 100 if delta_polars > 0 else 0\n",
    "    print(f\"  Savings:         {savings:>10.2f} MB ({reduction:.1f}% reduction)\")\n",
    "else:\n",
    "    overhead = delta_polars_memory - delta_polars\n",
    "    increase = (overhead / delta_polars) * 100 if delta_polars > 0 else 0\n",
    "    print(f\"  Overhead:        {overhead:>10.2f} MB ({increase:.1f}% increase)\")\n",
    "\n",
    "print(f\"\\nPandas:\")\n",
    "print(f\"  TIME approach:   {delta_pandas:>10.2f} MB\")\n",
    "print(f\"  MEMORY approach: {delta_pandas_memory:>10.2f} MB\")\n",
    "if delta_pandas_memory < delta_pandas:\n",
    "    savings = delta_pandas - delta_pandas_memory\n",
    "    reduction = (savings / delta_pandas) * 100 if delta_pandas > 0 else 0\n",
    "    print(f\"  Savings:         {savings:>10.2f} MB ({reduction:.1f}% reduction)\")\n",
    "else:\n",
    "    overhead = delta_pandas_memory - delta_pandas\n",
    "    increase = (overhead / delta_pandas) * 100 if delta_pandas > 0 else 0\n",
    "    print(f\"  Overhead:        {overhead:>10.2f} MB ({increase:.1f}% increase)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-memory-analysis",
   "metadata": {},
   "source": "**Memory MEMORY:**\n\n**Pandas MEMORY gana en consumo** (4.47 MB vs 15.91 MB de Polars), siendo **3.56x m√°s eficiente**.\n\n**Comparaci√≥n TIME vs MEMORY:**\n- **Polars**: Reduce memoria **97.8%** (de 732 MB a 16 MB) - impresionante\n- **Pandas**: Reduce memoria **99.3%** (de 686 MB a 4.5 MB) - a√∫n mejor\n\n**Trade-off final:**\n- **Polars MEMORY**: √ìptimo balance (0.278s, 16 MB)\n- **Pandas MEMORY**: M√≠nima memoria (2.561s, 4.5 MB) pero 9x m√°s lento\n\n**Conclusi√≥n:** Para Q3, los enfoques MEMORY ofrecen ahorros dram√°ticos de RAM con penalizaci√≥n m√≠nima/nula de tiempo en Polars."
  },
  {
   "cell_type": "markdown",
   "id": "global-summary-header",
   "metadata": {},
   "source": "---\n\n## Resumen Global: Comparaci√≥n TIME vs MEMORY (Polars vs Pandas)\n\n### 1. Tiempo de Ejecuci√≥n\n\n| Enfoque | Biblioteca | Tiempo promedio | Speedup vs Pandas |\n|--------|-----------|-----------------|-------------------|\n| TIME | **Polars** | **0.318s** | 9.12x m√°s r√°pido |\n| TIME | Pandas | 2.898s | baseline |\n| MEMORY | **Polars** | **0.278s** | 9.23x m√°s r√°pido |\n| MEMORY | Pandas | 2.561s | baseline |\n\n**Observaciones:**\n- Polars mantiene velocidad consistente (~0.3s) en TIME y MEMORY\n- Pandas es consistentemente ~9x m√°s lento por overhead de Python\n- Polars MEMORY es ligeramente m√°s r√°pido que TIME (optimizador de queries)\n\n### 2. Uso de Memoria (Delta RSS)\n\n| Enfoque | Biblioteca | Delta de memoria | Reducci√≥n vs TIME |\n|--------|-----------|------------------|-------------------|\n| MEMORY | **Pandas** | **4.47 MB** | 99.3% menos |\n| MEMORY | Polars | 15.91 MB | 97.8% menos |\n| TIME | Pandas | 686.31 MB | baseline |\n| TIME | Polars | 732.48 MB | baseline |\n\n**Observaciones:**\n- Enfoques MEMORY reducen memoria dram√°ticamente (>97%)\n- Pandas MEMORY es el m√°s eficiente (4.5 MB) pero 9x m√°s lento\n- Polars ofrece el mejor balance: 0.278s con solo 16 MB\n\n### 3. Trade-offs Arquitecturales\n\n**Polars:**\n- Motor Rust + Arrow: ejecuci√≥n ultrarr√°pida (~0.3s) independiente del enfoque\n- Lazy evaluation no penaliza tiempo - optimizador elimina trabajo redundante\n- TIME y MEMORY convergen en performance\n- Uso de memoria TIME m√°s alto por buffers columnares\n\n**Pandas:**\n- Overhead de Python significativo: `.apply()`, `explode()`, dict access\n- Chunked processing efectivo para memoria (99.3% reducci√≥n)\n- Consistentemente ~9x m√°s lento que Polars\n- Mejor eficiencia de memoria en modo MEMORY (4.5 MB vs 16 MB de Polars)\n\n### 4. Escalabilidad Esperada\n\n**Dataset 10x m√°s grande (3.8 GB, 1.17M tweets):**\n\n| Enfoque | Tiempo estimado | Memoria estimada |\n|---------|----------------|------------------|\n| Polars TIME | ~3s | ~7.3 GB |\n| Polars MEMORY | ~3s | ~160 MB ‚úÖ |\n| Pandas TIME | ~29s | ~6.9 GB |\n| Pandas MEMORY | ~26s | ~45 MB ‚úÖ |\n\n**Conclusi√≥n:** Enfoques MEMORY escalan linealmente sin crecimiento de RAM. **Polars MEMORY es la opci√≥n √≥ptima** para datasets grandes.\n\n### 5. Recomendaci√≥n Final\n\n**üèÜ Ganador: Polars MEMORY**\n- Velocidad m√°xima (0.278s)\n- Memoria m√≠nima viable (16 MB)\n- Mejor escalabilidad\n\n**Alternativas:**\n- **Polars TIME**: Si RAM no es problema y quieres simplicidad de c√≥digo\n- **Pandas MEMORY**: Si memoria absoluta es cr√≠tica y toleras 9x m√°s lento\n- **Pandas TIME**: ‚ùå No recomendado (lento y consume mucha RAM)"
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": "---\n\n## Conclusiones Globales Q3 ‚Äì Comparaci√≥n TIME vs MEMORY (Polars vs Pandas)\n\nEste an√°lisis experimental evalu√≥ **4 enfoques** para extraer el top 10 de usuarios m√°s mencionados en tweets: Polars TIME, Pandas TIME, Polars MEMORY y Pandas MEMORY. Todos producen **resultados id√©nticos** (verificado), por lo que la evaluaci√≥n se centra en performance y consumo de RAM.\n\n### Hallazgos Principales\n\n#### 1. Polars domina en velocidad (~9x m√°s r√°pido)\n\nPolars ejecuta consistentemente en **~0.3 segundos** (TIME: 0.318s, MEMORY: 0.278s) vs ~2.5-2.9s de Pandas. La ventaja proviene de:\n- Motor Rust + Apache Arrow (columnar, paralelizado)\n- Manejo nativo de listas anidadas y structs\n- Lazy evaluation inteligente (optimizador de queries)\n\nPandas sufre de overhead masivo de Python: 1.08M llamadas vs 1.8k de Polars en profiling.\n\n#### 2. MEMORY no penaliza tiempo en Polars\n\n**Descubrimiento clave:** Polars MEMORY es **igual de r√°pido o m√°s** que Polars TIME (0.278s vs 0.318s). El optimizador de queries elimina materializaci√≥n innecesaria, ejecutando todo en una sola pasada optimizada.\n\nEn Pandas, MEMORY tiene overhead m√≠nimo (~10% m√°s lento) pero aceptable dado el dram√°tico ahorro de RAM.\n\n#### 3. Ahorros de memoria dram√°ticos con MEMORY\n\nLos enfoques MEMORY reducen consumo **>97%**:\n- **Polars**: 732 MB ‚Üí 16 MB (97.8% reducci√≥n)\n- **Pandas**: 686 MB ‚Üí 4.5 MB (99.3% reducci√≥n)\n\nPandas MEMORY es el campe√≥n absoluto de eficiencia de memoria (4.5 MB), pero a costa de ser 9x m√°s lento.\n\n#### 4. El problema real: procesamiento de listas anidadas\n\nQ3 requiere:\n1. Filtrar 32.4% de tweets con `mentionedUsers != null`\n2. Explotar 117k menciones de listas anidadas  \n3. Extraer campo `username` de estructuras/dicts\n4. Contar y ordenar\n\n**Polars brilla** porque maneja estos pasos de manera nativa en Rust. **Pandas sufre** porque cada paso requiere overhead de Python (`.apply()`, `.get()`, etc.).\n\n### Decisi√≥n Final\n\n**üèÜ Recomendaci√≥n: Polars MEMORY**\n\nEs el enfoque √≥ptimo para Q3 porque combina:\n- ‚úÖ Velocidad m√°xima (0.278s)\n- ‚úÖ Memoria m√≠nima viable (16 MB)\n- ‚úÖ Mejor escalabilidad (lineal en tiempo, constante en memoria)\n- ‚úÖ C√≥digo simple (mismo que TIME, solo cambia cu√°ndo se materializa)\n\n**Alternativas v√°lidas:**\n- **Polars TIME**: Si RAM no es restricci√≥n y priorizas simplicidad absoluta\n- **Pandas MEMORY**: Solo si requieres memoria <5 MB y toleras ser 9x m√°s lento\n\n**No recomendado:**\n- ‚ùå **Pandas TIME**: Lento (2.9s) + alto consumo (686 MB) - sin ventajas\n\n### Lecciones para Arquitectura de Datos\n\n1. **Polars es superior para datos estructurados complejos** (listas, structs, JSON anidado)\n2. **Lazy evaluation bien implementada no tiene costo** - Polars lo demuestra\n3. **Pandas sigue v√°lido para memoria cr√≠tica**, pero con penalizaci√≥n severa de tiempo\n4. **Para producci√≥n**: Polars MEMORY escala a datasets 10-100x m√°s grandes sin cambios\n\nEste an√°lisis confirma que **Polars debe ser la elecci√≥n predeterminada** para pipelines de datos modernos con JSON/nested data."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}